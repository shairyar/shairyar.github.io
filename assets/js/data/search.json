[ { "title": "Kubernetes Learning Path: Understanding Persistent Storage", "url": "/posts/kubernetes-learning-path-persistent-storage/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, persistent-storage, pvc, statefulset, postgresql, storage", "date": "2025-11-12 10:00:00 +0500", "content": "Kubernetes Learning Path: Understanding Persistent Storage When I first started deploying applications on Kubernetes, I quickly ran into a problem: my database data kept disappearing every time a pod restarted. Thatâ€™s when I realized I needed to understand persistent storage. In Kubernetes, pods are temporaryâ€”they can be created, destroyed, and recreated at any time. This is great for things like web servers that donâ€™t need to remember anything. But what about databases? They need to remember all your data even when the pod restarts. Thatâ€™s where persistent storage comes in. In this post, Iâ€™ll walk you through how to set up persistent storage in Kubernetes. Weâ€™ll use something called a PersistentVolumeClaim (or PVC for shortâ€”think of it as asking for storage) and a StatefulSet (which is like a special type of pod that can use storage). Weâ€™ll use a PostgreSQL database as our example. Why Persistent Storage Matters If you deploy a PostgreSQL database using a regular Deployment, every time the pod restarts or gets recreated, all your data disappears. The containerâ€™s filesystem is temporary. It only exists for the lifetime of the pod, and when that pod dies, so does everything inside it. Persistent storage fixes this. Your data survives pod restarts, which means databases and other stateful applications actually work the way theyâ€™re supposed to. You can also move data between nodes when needed. The Components: PVC and PV Before jumping into the configuration, hereâ€™s what you need to know: PersistentVolume (PV): This is the actual storage space in your cluster. Think of it like a real hard drive that exists somewhere. Someone (either an admin or Kubernetes itself) sets this up. PersistentVolumeClaim (PVC): This is you asking for storage. Itâ€™s like going to a restaurant and saying â€œI need a table for 4 people.â€ Youâ€™re making a request. You tell Kubernetes â€œI need 5 gigabytes of storageâ€ and Kubernetes finds or creates the actual storage (the PV) for you. Hereâ€™s how it works: You create a PVC (this is the configuration you provide). Kubernetes reads your PVC configuration and uses it to automatically create the actual storage (the PV). You donâ€™t manually create PVsâ€”you just provide the PVC configuration, and Kubernetes handles creating the PV for you. But remember: if you donâ€™t provide a PVC configuration first, Kubernetes wonâ€™t create any storage. The PVC configuration is what tells Kubernetes what storage to create. Setting Up Persistent Storage for PostgreSQL Iâ€™ll walk through a complete PostgreSQL setup with persistent storage. Iâ€™ll go through each configuration file and explain whatâ€™s happening. 1. PersistentVolumeClaim (PVC) First up, we need to request storage. Hereâ€™s the PVC: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: postgres-pvc # Name of this PVC resource - we'll use this exact name in the StatefulSet below to connect them namespace: default spec: accessModes: - ReadWriteOnce # Volume can be mounted read-write by a single node resources: requests: storage: 5Gi # Requesting 5 gigabytes of storage The important parts here: name: postgres-pvc: This is the name weâ€™re giving to this storage resource. The StatefulSet (which contains the PostgreSQL database pod) will use this exact name to connect to this storage. Think of the PVC as the hard drive, and the StatefulSet as the computer that needs to use that hard drive. Weâ€™ll reference this name in the StatefulSet configuration below. accessModes: ReadWriteOnce: This means only one computer (node) can use this storage at a time, and it can both read and write. This is perfect for databases. There are other options (like letting multiple computers use it), but for databases, you almost always want ReadWriteOnce. storage: 5Gi: Weâ€™re asking for 5 gigabytes. Change this to whatever you actually need. 2. StatefulSet for PostgreSQL For databases and other apps that need to remember things, you need a StatefulSet, not a regular Deployment. The main difference? StatefulSets can use persistent storage, and they give each pod a stable name that doesnâ€™t change. This is important for databases. Hereâ€™s the StatefulSet: apiVersion: apps/v1 kind: StatefulSet metadata: name: postgres namespace: default spec: serviceName: postgres # This name must match the Service name we'll create below (connects StatefulSet to Service) replicas: 1 selector: matchLabels: app: postgres # This label must match the label in the pod template below (in this same file) template: metadata: labels: app: postgres # This label must match the selector above (in this same file) and the Service selector below spec: containers: - name: postgres image: postgres:15-alpine ports: - containerPort: 5432 name: postgres envFrom: - secretRef: name: postgres-secret # References the Secret created earlier (loads all keys as env vars) volumeMounts: - name: postgres-storage # This name must match the volume name in the volumes section below (in this same file) mountPath: /var/lib/postgresql/data # Where PostgreSQL stores its data inside the container subPath: postgres # Creates a subdirectory to prevent permission issues resources: requests: memory: \"256Mi\" cpu: \"250m\" limits: memory: \"512Mi\" cpu: \"500m\" livenessProbe: exec: command: - pg_isready - -U - $(POSTGRES_USER) # Uses environment variable from postgres-secret - -d - $(POSTGRES_DB) # Uses environment variable from postgres-secret initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: exec: command: - pg_isready - -U - $(POSTGRES_USER) # Uses environment variable from postgres-secret - -d - $(POSTGRES_DB) # Uses environment variable from postgres-secret initialDelaySeconds: 5 periodSeconds: 5 volumes: - name: postgres-storage # This name must match the volumeMounts.name above (in this same file) - connects the volume to the mount persistentVolumeClaim: claimName: postgres-pvc # This is the name from the PVC we created earlier - connects this StatefulSet to that storage A few things to note: serviceName: postgres: This tells the StatefulSet to use a headless service (weâ€™ll create that next) for stable network identity. volumeMounts: This is where we connect the storage to the container. The mountPath is just a folder path inside the container where PostgreSQL will save its data (/var/lib/postgresql/data is PostgreSQLâ€™s default data folder). The subPath: postgres part creates a subfolder inside the storageâ€”this prevents permission problems. I learned this the hard way when PostgreSQL couldnâ€™t write to the storage because of permission issues. volumes: This is where we tell the StatefulSet which storage to use. We reference the PVC name (postgres-pvc) that we created earlier. This connects the storage to the container. Health checks: These check if PostgreSQL is actually working, not just if the container is running. They use pg_isready which is a PostgreSQL command. Notice weâ€™re using $(POSTGRES_USER) and $(POSTGRES_DB)â€”these are environment variables that come from the Secret. This way, if we change the Secret, the health checks automatically use the new values. 3. Headless Service StatefulSets need a special type of service called a â€œheadless service.â€ A normal service gives pods a single IP address to share, but a headless service doesnâ€™tâ€”it lets each pod have its own address. This is important for StatefulSets because each pod needs its own identity. Hereâ€™s what it looks like: apiVersion: v1 kind: Service metadata: name: postgres # This name must match the StatefulSet spec.serviceName above (connects Service to StatefulSet) namespace: default spec: selector: app: postgres # This label must match the label in StatefulSet template.metadata.labels above (connects Service to pods) ports: - port: 5432 # Port exposed by the service targetPort: 5432 # Port on the container (matches containerPort in StatefulSet) clusterIP: None # Headless service - required for StatefulSets The clusterIP: None is what makes it â€œheadlessâ€â€”it means thereâ€™s no shared IP address. Instead, each pod gets its own address, which is what StatefulSets need. 4. Secret for Database Credentials Weâ€™ll store the database credentials in a Secret (I covered this in Part 2): apiVersion: v1 kind: Secret metadata: name: postgres-secret # This name is referenced in StatefulSet envFrom.secretRef namespace: default type: Opaque stringData: # PostgreSQL initialization variables (used by postgres image) POSTGRES_USER: rails_user POSTGRES_PASSWORD: secure_password_change_me POSTGRES_DB: myapp_production How These Pieces Fit Together Before we deploy, let me explain how these three components connect to each other. Think of it like this: PersistentVolumeClaim (PVC) = Your storage (like a hard drive) StatefulSet = Your PostgreSQL database pod (the computer that needs the hard drive) Service = The way other pods can find and talk to your database Hereâ€™s how they link together: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Service â”‚ â† Gives your database a stable address â”‚ (postgres) â”‚ so other pods can find it â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ (finds pods by label: app=postgres) â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ StatefulSet â”‚ â† Your PostgreSQL database pod â”‚ (postgres) â”‚ - Uses Service name: \"postgres\" (connects to Service above) â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ - Has label: app=postgres (Service uses this to find it) â”‚ â”‚ (uses PVC name: \"postgres-pvc\") â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ PVC â”‚ â† Your storage (where database saves data) â”‚ (postgres-pvc) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ In simple terms: StatefulSet connects to PVC: The StatefulSet says â€œI need storageâ€ and uses the PVC name postgres-pvc to get it. This is like plugging a hard drive into your computer. Service connects to StatefulSet: The Service helps other pods find your database. It looks for pods with the label app=postgres (which is what the StatefulSet creates). Also, the Service name must be postgres because thatâ€™s what the StatefulSet is looking for. Everything stays in sync: When the StatefulSet creates a pod, that pod has the label app=postgres, so the Service automatically finds it. The pod also gets the storage from the PVC, so your data persists. Itâ€™s like a chain: Service â†’ finds â†’ StatefulSet pods â†’ uses â†’ PVC storage. Deploying Everything With all the pieces ready, time to deploy: # Apply all configurations kubectl apply -f postgres-secret.yaml kubectl apply -f postgres-pvc.yaml kubectl apply -f postgres-service.yaml kubectl apply -f postgres-statefulset.yaml Or if all files are in the same directory: kubectl apply -f postgres-secret.yaml postgres-pvc.yaml postgres-service.yaml postgres-statefulset.yaml Verifying the Setup Check that everything is working: # Check the PVC status kubectl get pvc # Output should show: # NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE # postgres-pvc Bound pvc-xxx 5Gi RWO local-path 10s You should see STATUS as Boundâ€”that means your request for storage was successful! Kubernetes found or created the actual storage and connected it to your PVC. If it says â€œPending,â€ something went wrong. # Check the StatefulSet and pods kubectl get statefulset kubectl get pods -l app=postgres # Check pod details to see the volume mount kubectl describe pod postgres-0 In the pod description, look for the volume mount under Mounts:. If you see it listed there, that means the storage is actually connected to your pod and ready to use. Testing Persistence The real test is whether your data actually survives a pod restart. Hereâ€™s how to verify: # Connect to the database and create some test data kubectl exec -it postgres-0 -- psql -U rails_user -d myapp_production # Inside psql: CREATE TABLE test_persistence (id SERIAL PRIMARY KEY, message TEXT); INSERT INTO test_persistence (message) VALUES ('This should persist!'); SELECT * FROM test_persistence; \\q Now delete the pod and watch Kubernetes recreate it: # Delete the pod kubectl delete pod postgres-0 # Watch it get recreated kubectl get pods -l app=postgres -w Once the new pod is running, check if the data is still there: # Connect again and check the data kubectl exec -it postgres-0 -- psql -U rails_user -d myapp_production -c \"SELECT * FROM test_persistence;\" If you see your test data, youâ€™re all set! Persistent storage is working. Common Issues and Solutions Permission errors: Make sure youâ€™re using subPath in your volume mount. This is usually the culprit. Storage not persisting: Check that the PVC status is Bound with kubectl get pvc (if it says â€œPending,â€ the storage wasnâ€™t created) Verify the volume is actually mounted: kubectl describe pod &lt;pod-name&gt; and look for the mount in the output Make sure youâ€™re using a StatefulSet, not a Deployment. Regular Deployments can lose data when pods restart, even if you attach storage to them. StatefulSets are designed to work with persistent storage. Conclusion Persistent storage is essential for running databases and other apps that need to remember things in Kubernetes. It adds a bit of complexity compared to simple web apps, but once you understand how PVCs (your storage request), PVs (the actual storage), and StatefulSets (the pods that use the storage) work together, itâ€™s pretty straightforward. Whatâ€™s Next? Iâ€™m planning to cover backup and restore strategies for persistent volumes, and how to set up multi-replica databases with persistent storage. Stay tuned! Series Navigation Part 1: Deploy Your First App Part 2: ConfigMaps and Secrets Part 3: Understanding Namespaces Part 4: Understanding Port Mapping in k3d Part 5: Setting Up k3s on Raspberry Pi Part 6: Understanding Persistent Storage â† You just finished this! " }, { "title": "Remote Access to Your k3s Homelab with Tailscale", "url": "/posts/remote-access-to-k3s-homelab-with-tailscale/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3s, tailscale, vpn, networking, homelab, raspberry-pi", "date": "2025-11-09 10:00:00 +0500", "content": "Remote Access to Your k3s Homelab with Tailscale While working on my k3s homelab cluster, I ran into an interesting challenge: how could I access my cluster when I wasnâ€™t home? I wanted to run kubectl commands from anywhere, manage my deployments remotely, and check on my applicationsâ€”but I didnâ€™t have a dedicated public IP address from my ISP. Thatâ€™s when I discovered Tailscale. Tailscale is a modern VPN built on WireGuard that creates a secure, peer-to-peer network between your devices. The best part? It just works. No port forwarding, no complex router configuration, no messing with firewall rules. You install it on your devices, authenticate, and suddenly they can all talk to each other securelyâ€”no matter where they are. In this post, Iâ€™ll show you exactly how I set up Tailscale on my k3s master node to enable remote cluster access. By the end, youâ€™ll be able to manage your homelab cluster from anywhereâ€”your office, a coffee shop, or even from another countryâ€”as long as youâ€™re connected to Tailscale. The Problem: No Dedicated IP Address Most home internet connections use dynamic IP addresses that change periodically. Even if your IP stayed constant, exposing your Kubernetes API server directly to the internet felt scary. There are solutions like ngrok or Cloudflare Tunnel, but while reading about them, I realized theyâ€™re meant to expose your applications to the internet. I plan to look into them in the future when Iâ€™m ready to expose my deployed applications publicly. For now, I just wanted an easy way to connect to my cluster. The Solution: Tailscale VPN Iâ€™ve never worked with a VPN before, so I was a little nervous about trying it out. But it turned out to be fun, and Iâ€™m glad I gave it a shot! Tailscale creates a secure mesh network between your devices. Once connected: Your k3s master node gets a Tailscale IP (in the 100.x.x.x range) Your laptop also gets a Tailscale IP These IPs are stable and donâ€™t change (I hope! ğŸ˜„) All traffic between them is encrypted via WireGuard No router configuration needed The beauty is that kubectl doesnâ€™t know the difference. It just connects to an IP addressâ€”whether thatâ€™s your local network (192.168.x.x) or Tailscale network (100.x.x.x). Prerequisites Before we start, make sure you have: A running k3s cluster (Iâ€™m using a Raspberry Pi 4) SSH access to your k3s master node A Tailscale account (free tier supports up to 20 devices) Tailscale installed on your laptop or workstation If you need help setting up k3s, check out my post on Setting Up k3s on Raspberry Pi. Step 1: Install Tailscale on Your k3s Master Node First, letâ€™s get Tailscale installed on the k3s master node. SSH into your master node: ssh pi@k3s-master # or use the IP address ssh pi@192.168.18.49 Install Tailscale using the official installation script: curl -fsSL https://tailscale.com/install.sh | sh This script detects your OS and installs the appropriate Tailscale package. On my Raspberry Pi running Ubuntu, it installed via apt. Verify the installation: tailscale version Output: 1.56.1 Step 2: Authenticate Tailscale Now we need to connect this node to your Tailscale network. Start Tailscale: sudo tailscale up This command will output a URL: To authenticate, visit: https://login.tailscale.com/a/xxxxxxxxxxxxx Copy that URL and open it in your browser. Youâ€™ll be asked to: Log in to your Tailscale account Authorize the device to join your network Give the device a name (I used â€œk3s-masterâ€) Once authorized, youâ€™ll see a success message. Back in your terminal, verify the connection: tailscale status Output: 100.77.77.77 k3s-master pi@ linux - Great! Your k3s master node is now part of your Tailscale network. Step 3: Get Your Tailscale IP Address Your master node now has a Tailscale IP in the 100.x.x.x range. Letâ€™s get it: tailscale ip Output: 100.77.77.77 Write this down! Youâ€™ll need it for kubectl configuration later. You can verify Tailscale is running: sudo systemctl status tailscaled Important note: The service is named tailscaled (with a â€˜dâ€™ at the end), not tailscale. This tripped me up at first when I was trying to check the service status. Step 4: Enable IP Forwarding For Tailscale to route traffic properly, we need to enable IP forwarding on the master node. Check if itâ€™s already enabled: sysctl net.ipv4.ip_forward If it shows net.ipv4.ip_forward = 1, youâ€™re good! If it shows 0, enable it: # Enable temporarily (until reboot) sudo sysctl -w net.ipv4.ip_forward=1 # Make it persistent across reboots echo 'net.ipv4.ip_forward = 1' | sudo tee -a /etc/sysctl.conf Step 5: Configure k3s Certificate to Include Tailscale IP Hereâ€™s something I learned the hard way: by default, the k3s API server certificate only includes your local IP addresses and hostnames. When you try to connect via Tailscale IP, youâ€™ll get TLS certificate errors. The solution is to add your Tailscale IP to the k3s certificateâ€™s Subject Alternative Names (SANs). On your k3s master node, check if the k3s config file exists: ls /etc/rancher/k3s/config.yaml If the file doesnâ€™t exist, create it: sudo mkdir -p /etc/rancher/k3s sudo nano /etc/rancher/k3s/config.yaml Add your Tailscale IP to the tls-san list: tls-san: - 100.77.77.77 # Replace with YOUR actual Tailscale IP - k3s-master # Optional: add hostname too If you already have a config file with other settings, just add the tls-san section. Donâ€™t remove your existing configuration. Restart k3s to regenerate the certificates: sudo systemctl restart k3s Wait about a minute for k3s to restart and regenerate certificates: # Check if k3s is running sudo systemctl status k3s # Wait a bit for certificates to regenerate sleep 60 This step is crucial. Without it, youâ€™ll get TLS certificate errors and youâ€™ll be tempted to use kubectl --insecure-skip-tls-verify, which isnâ€™t ideal. Step 6: Configure kubectl for Remote Access Now comes the fun partâ€”configuring kubectl on your laptop to connect via Tailscale. Copy the kubeconfig File The kubeconfig file is located at /etc/rancher/k3s/k3s.yaml on the master node, but itâ€™s owned by root. Weâ€™ll copy it to our laptop: # On your laptop, run: ssh pi@k3s-master \"sudo cat /etc/rancher/k3s/k3s.yaml\" &gt; ~/.kube/k3s-remote-config Replace pi with your actual SSH user and k3s-master with your master nodeâ€™s hostname or IP. Update the Server URL Open the copied config file: nano ~/.kube/k3s-remote-config Find the server: line (should be near the top) and change it from: server: https://127.0.0.1:6443 To your Tailscale IP: server: https://100.77.77.77:6443 # Replace with YOUR Tailscale IP Save and exit (Ctrl+X, then Y, then Enter). Test Remote Access Make sure Tailscale is running on your laptop: tailscale status You should see both your laptop and k3s-master listed. Now test kubectl access: kubectl --kubeconfig=~/.kube/k3s-remote-config get nodes Output: NAME STATUS ROLES AGE VERSION k3s-master Ready control-plane,master 30d v1.28.2+k3s1 k3s-worker1 Ready &lt;none&gt; 30d v1.28.2+k3s1 k3s-worker2 Ready &lt;none&gt; 30d v1.28.2+k3s1 Success! Youâ€™re now accessing your k3s cluster remotely via Tailscale! Managing Multiple Contexts Right now, you probably have two ways to access your cluster: Local access (when at home): https://192.168.18.49:6443 Remote access (via Tailscale): https://100.77.77.77:6443 Let me show you how to easily switch between them by merging both configs into one kubeconfig file with different contexts: # Backup your current config cp ~/.kube/config ~/.kube/config.backup # Merge configs KUBECONFIG=~/.kube/config:~/.kube/k3s-remote-config kubectl config view --flatten &gt; ~/.kube/merged-config # Replace your config with merged version mv ~/.kube/merged-config ~/.kube/config Then switch contexts: # View available contexts kubectl config get-contexts # Switch to local context kubectl config use-context default # Switch to remote context kubectl config use-context k3s-remote Troubleshooting Here are some issues that I ran into and how I fixed them. Issue 1: â€œconnection refusedâ€ Error Problem: kubectl returns â€œconnection refusedâ€ when trying to connect via Tailscale IP. Diagnosis: # From your laptop, test connectivity ping 100.77.77.77 # Use your Tailscale IP # Test if port 6443 is accessible curl -k https://100.77.77.77:6443 Solutions: Make sure Tailscale is running on both master node and laptop: tailscale status Verify IP forwarding is enabled on master: sysctl net.ipv4.ip_forward Check if k3s API server is listening: sudo netstat -tlnp | grep 6443 Issue 2: Tailscale Service Not Found Problem: sudo systemctl status tailscale returns â€œUnit tailscale.service could not be foundâ€ Solution: The service is named tailscaled (with a â€˜dâ€™): sudo systemctl status tailscaled sudo systemctl restart tailscaled Issue 3: Can Access Locally but Not Remotely Problem: kubectl works fine on home network but times out via Tailscale. Diagnosis: # Check Tailscale connectivity ping 100.77.77.77 # Your Tailscale IP # Check Tailscale routes tailscale status Solutions: Verify both devices show as connected in tailscale status Check if youâ€™re actually connected to Tailscale on your laptop Try disconnecting and reconnecting: sudo tailscale down &amp;&amp; sudo tailscale up Conclusion Setting up Tailscale for remote k3s access has been one of my favorite homelab improvements. The setup took less than an hour, and now I can manage my cluster from anywhere without worrying about dynamic IPs, port forwarding, or complex VPN configurations. The ability to run kubectl commands from my laptop at a coffee shop feels magical. Itâ€™s like my homelab is right there with me, even though itâ€™s sitting on my desk at home. If youâ€™re running a homelab cluster and want secure remote access without the headaches, I highly recommend giving Tailscale a try. The free tier is more than enough to get started, and the peace of mind knowing your cluster isnâ€™t exposed directly to the internet is worth it alone. Whatâ€™s Next? This was just Phase 1 of my remote access setup. Iâ€™m also exploring: Cloudflare Tunnel: For exposing web services publicly without a dedicated IP Stay tuned for more homelab adventures! Additional Resources Tailscale Documentation Tailscale Quick Start k3s Documentation Found this helpful? Have questions? Feel free to open an issue here. " }, { "title": "Kubernetes Learning Path: Setting Up Ingress Controller for External Access", "url": "/posts/kubernetes-learning-path-ingress-controller/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3s, traefik, ingress, nginx-ingress, networking", "date": "2025-11-06 09:00:00 +0500", "content": "Kubernetes Learning Path: Setting Up Ingress Controller for External Access After deploying my Rails application with PostgreSQL and SolidQueue workers in the previous post, I had a fully functional application running on my Raspberry Pi k3s cluster. But there was one problem: I could only access it using kubectl port-forward. Port-forwarding is fine for testing and debugging, but itâ€™s not a real solution. It requires keeping a terminal open, it only works from the machine running kubectl, and itâ€™s definitely not how youâ€™d let other people access your application. I needed a proper way to expose my Rails app to the outside world. Thatâ€™s where Ingress controllers come in. What is an Ingress Controller? Think of an Ingress controller as the front desk receptionist at a hotel. When visitors arrive, they donâ€™t go directly to individual roomsâ€”they check in at the front desk, and the receptionist routes them to the right place based on their reservation. In Kubernetes: Your Services are like hotel rooms (internal addresses for your apps) The Ingress controller is the front desk (accepts external HTTP/HTTPS traffic) Ingress resources are the routing rules (which requests go to which services) Without an Ingress controller, your services are only accessible within the cluster or through workarounds like port-forwarding. With an Ingress controller, you can access apps via actual domain names (like myapp.local or api.example.com), route multiple apps through a single IP address, and handle SSL/TLS termination. You can even do URL-based routingâ€”sending /api requests to your API service and /admin requests to your admin panel. Using Traefik Ingress Controller When I started looking into Ingress controllers, I discovered that k3s comes with Traefik pre-installed. Perfect! One less thing to install. Traefik is modern, has a beautiful web dashboard, and includes built-in Letâ€™s Encrypt support for automatic SSL certificates. Since itâ€™s already running in our k3s cluster, we just need to create Ingress resources to tell Traefik how to route traffic. Step 1: Verify Traefik is Running Letâ€™s confirm that Traefik is running in your k3s cluster: kubectl get pods -n kube-system | grep traefik Output: traefik-7cd4fcff68-9xk2p 1/1 Running 0 3d Check the Traefik service: kubectl get svc -n kube-system traefik Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik LoadBalancer 10.43.45.123 192.168.18.49 80:30080/TCP,443:30443/TCP 3d See that EXTERNAL-IP? Thatâ€™s the IP address of your k3s master node (in my case, 192.168.18.49). This is where Traefik is listening for incoming HTTP and HTTPS requests. Perfect! Traefik is ready to go. Now letâ€™s create an Ingress resource to route traffic to our Rails application. Step 2: Create an Ingress Resource An Ingress resource is just a set of rules that tells Traefik where to send incoming requests. Create a file called rails-ingress.yaml: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: rails-ingress annotations: # Tells Traefik to use the \"web\" entrypoint (HTTP port 80) traefik.ingress.kubernetes.io/router.entrypoints: web spec: rules: - host: rails.home # â† Domain name: requests to \"rails.home\" will match this rule http: paths: - path: / # â† Match all paths starting with \"/\" (everything) pathType: Prefix # â† \"/\" matches \"/\", \"/tasks\", \"/admin\", etc. backend: service: name: rails-service # â† Routes to the \"rails-service\" we created in Part 6 # (from rails-service.yaml in the previous post) port: number: 80 # â† Connects to port 80 of rails-service # rails-service then forwards to Rails pods on port 3000 Apply it: kubectl apply -f rails-ingress.yaml Output: ingress.networking.k8s.io/rails-ingress created Verify the Ingress kubectl get ingress rails-ingress Output: NAME CLASS HOSTS ADDRESS PORTS AGE rails-ingress &lt;none&gt; rails.home 192.168.18.49 80 30s You should see your host (rails.home) and the IP address where itâ€™s accessible. Perfect! Now we just need to configure DNS. Step 3: Configure DNS Resolution Right now, your Ingress is configured to route traffic for rails.home, but your computer doesnâ€™t know what rails.home means or where to find it. We need to tell your computer that rails.home should point to your k3s cluster IP. Option 1: Edit /etc/hosts (Quick and Easy) The simplest solution is to add an entry to your /etc/hosts file. This works immediately and is perfect for local development. # Find your k3s cluster IP first kubectl get svc -n kube-system traefik # Look for the EXTERNAL-IP column # Add to /etc/hosts (replace with your actual IP) echo \"192.168.18.49 rails.home\" | sudo tee -a /etc/hosts Option 2: Router DNS Configuration (Network-Wide) If you want everyone on your network to access rails.home without editing their hosts files, youâ€™ll need to configure DNS at the router level or set up a local DNS server. The challenge: Many ISP-provided routers donâ€™t allow you to customize DNS entries. My router definitely didnâ€™t give me this option. The workaround: Set up dnsmasq on your Raspberry Pi as a local DNS server. Install dnsmasq on Raspberry Pi # SSH to your k3s master node ssh pi@192.168.18.49 # Install dnsmasq sudo apt update sudo apt install -y dnsmasq # Edit configuration sudo nano /etc/dnsmasq.conf Add these lines at the end: # Listen on localhost and your Pi's IP listen-address=127.0.0.1,192.168.18.49 # DNS entry for Rails app address=/rails.home/192.168.18.49 # Forward other DNS queries to your router server=192.168.18.1 Save and exit (Ctrl+X, then Y, then Enter). Restart dnsmasq: sudo systemctl restart dnsmasq sudo systemctl enable dnsmasq Verify itâ€™s working: # Test DNS resolution from the Pi itself nslookup rails.home 127.0.0.1 Output: Server:\t\t127.0.0.1 Address:\t127.0.0.1#53 Name:\trails.home Address: 192.168.18.49 Configure Your Router (If Supported) Now you need to tell your router to use the Pi as the DNS server: Log into your router admin panel (usually 192.168.1.1 or 192.168.0.1) Look for DHCP settings Find â€œPrimary DNS Serverâ€ or â€œDNS Serverâ€ Change it to your Piâ€™s IP: 192.168.18.49 Save and reboot the router Important: Many ISP routers donâ€™t let you change the DHCP DNS server. If yours doesnâ€™t, youâ€™ll need to either manually configure DNS on each device or just stick with the /etc/hosts method. Manually Configure DNS on Individual Devices If router configuration doesnâ€™t work, you can manually configure DNS on your Linux machine: # Edit network connection (replace \"eth0\" with your interface) sudo nmcli connection modify eth0 ipv4.dns \"192.168.18.49\" sudo nmcli connection down eth0 &amp;&amp; sudo nmcli connection up eth0 Step 4: Test Your Ingress Test DNS Resolution First, verify that your DNS is resolving correctly: ping rails.home Output: PING rails.home (192.168.18.49): 56 data bytes 64 bytes from 192.168.18.49: icmp_seq=0 ttl=64 time=2.3 ms 64 bytes from 192.168.18.49: icmp_seq=1 ttl=64 time=2.1 ms If you see ping responses from your k3s IP address, DNS is working! If you get â€œName or service not knownâ€ or â€œcannot resolve rails.homeâ€, your DNS isnâ€™t configured correctly. Go back to Step 3. Test HTTP Access Now try accessing your Rails app via the Ingress: curl http://rails.home You should see your Rails applicationâ€™s HTML response! Or open your browser and visit: http://rails.home You should see your Rails app loading in the browserâ€”no port-forwarding required! Test the Host Header (Troubleshooting) If youâ€™re getting a 404 error when accessing via domain name but you know the service is running, the Ingress might be working but the host routing isnâ€™t matching. Test with the Host header explicitly: curl -H \"Host: rails.home\" http://192.168.18.49 If this works but http://rails.home doesnâ€™t, the issue is DNS resolution, not your Ingress configuration. Step 5: Access the Traefik Dashboard (Bonus) One of Traefikâ€™s best features is its built-in dashboard that shows all your routes, services, and middleware in real-time. Let me show you how to set it up, because I ran into some interesting issues along the way. Why Use IngressRoute Instead of Regular Ingress? When I first tried to access the Traefik dashboard, I created a regular Kubernetes Ingress resource pointing to port 9000. That didnâ€™t work. After some debugging and reading the Traefik docs, I learned that: Traefik v2 uses IngressRoute CRDs (Custom Resource Definitions) for advanced routing The dashboard is exposed via Traefikâ€™s internal service called api@internal This internal service requires an IngressRoute, not a standard Kubernetes Ingress Create the Traefik Dashboard IngressRoute Create a file called traefik-dashboard-ingress.yaml: apiVersion: traefik.io/v1alpha1 # â† Traefik-specific CRD (not standard k8s Ingress) kind: IngressRoute # â† IngressRoute is Traefik's advanced routing resource metadata: name: traefik-dashboard namespace: kube-system # â† Must be in kube-system where Traefik is running spec: entryPoints: - web # â† Uses HTTP entrypoint (port 80) routes: - match: Host(`traefik.home`) # â† Match requests to \"traefik.home\" kind: Rule # â† This is a routing rule services: - name: api@internal # â† Special Traefik internal service for dashboard # This is NOT a Kubernetes service - it's Traefik's internal API kind: TraefikService # â† Tells Traefik this is a Traefik-internal service A couple of important things: make sure you use traefik.io/v1alpha1 (not traefik.containo.us/v1alpha1â€”thatâ€™s the old API version). Also, Iâ€™m using .home instead of .local for the domain. I learned the hard way that .local has issues with mDNS, which Iâ€™ll explain in the troubleshooting section. Apply it: kubectl apply -f traefik-dashboard-ingress.yaml Output: ingressroute.traefik.io/traefik-dashboard created Configure DNS Add to /etc/hosts: # Add this line (use your master node IP, not 127.0.0.1) echo \"192.168.18.49 traefik.home\" | sudo tee -a /etc/hosts Important: Use your master nodeâ€™s actual IP address here, not 127.0.0.1, especially if youâ€™re accessing from a different machine. Access the Dashboard # Test with curl curl http://traefik.home/dashboard/ # Or open in your browser: # http://traefik.home/dashboard/ You should see the beautiful Traefik dashboard showing all your routes, including the rails.home route we created earlier! Quick Reference List all IngressRoutes: kubectl get ingressroute -A Check Traefik logs: kubectl logs -n kube-system -l app.kubernetes.io/name=traefik --tail=20 Port-forward alternative (if IngressRoute doesnâ€™t work): kubectl port-forward -n kube-system svc/traefik 9000:9000 # Then access: http://localhost:9000/dashboard/ Now you have both your Rails app and the Traefik dashboard accessible through clean hostnames on the same cluster IP! Common Issues I Ran Into Setting Up Rails Ingress (rails.home) Issue 1: â€œSite canâ€™t be reachedâ€ Error Problem: Accessing rails.home in the browser shows â€œThis site canâ€™t be reachedâ€. Diagnosis: # Test DNS resolution ping rails.home # If this fails with \"Name or service not known\", it's a DNS issue Solution: Your DNS isnâ€™t resolving. Double-check your /etc/hosts file or dnsmasq configuration. Make sure you didnâ€™t typo the hostname or IP address. Issue 2: 404 Not Found Problem: rails.home loads, but shows a 404 error. Diagnosis: # Check if Ingress is configured correctly kubectl describe ingress rails-ingress # Check if service exists and has endpoints kubectl get endpoints rails-service Solution: If endpoints are empty, your service selector doesnâ€™t match any pods If the Ingress rules look wrong, check your YAML for typos in the host or service name Make sure rails-service actually exists: kubectl get svc rails-service Issue 3: Mixed Content Warnings (HTTP vs HTTPS) Problem: Your Rails app loads but some resources fail to load with mixed content warnings. Solution: This happens when Rails thinks itâ€™s running on HTTPS but Ingress is serving HTTP. We covered this in the previous postâ€”make sure you have these settings in rails-configmap.yaml: RAILS_ASSUME_SSL: \"false\" RAILS_FORCE_SSL: \"false\" Weâ€™ll cover proper SSL/TLS setup with cert-manager in a future post. Setting Up Traefik Dashboard (traefik.home) Issue 1: â€œService port not foundâ€ Error Problem: You tried to create a regular Ingress for the Traefik dashboard and got â€œservice port not foundâ€ errors in the Traefik logs. Solution: This means you tried to use a regular Ingress pointing to port 9000. The Traefik service in k3s doesnâ€™t expose port 9000 by default. Use an IngressRoute with api@internal service instead (as shown in Step 5). Issue 2: API Version Errors Problem: You get an error like â€œno matches for kind IngressRoute in version traefik.containo.us/v1alpha1â€. Solution: Make sure youâ€™re using traefik.io/v1alpha1 not the old traefik.containo.us/v1alpha1. Check the correct API version: kubectl get crd ingressroutes.traefik.io -o jsonpath='{.spec.group}' &amp;&amp; echo Issue 3: The .local Domain Problem Problem: Initially, I used traefik.local as the hostname. I added it to /etc/hosts, but DNS resolution completely failed. rails.home worked fine, but traefik.local didnâ€™t resolve at all. Why this happens: The .local TLD is reserved for mDNS (multicast DNS/Bonjour). On systems with systemd-resolved or avahi, .local domains are intercepted by mDNS before checking /etc/hosts. You can verify this by checking your /etc/nsswitch.conf: cat /etc/nsswitch.conf | grep hosts If you see mdns_minimal [NOTFOUND=return] before files, mDNS will intercept .local domains. Solution: Use .home, .lan, or .internal TLD instead of .local. Much simpler than reconfiguring your systemâ€™s DNS resolution order. Conclusion Setting up an Ingress controller was a game-changer for my k3s cluster. No more fumbling with kubectl port-forward commands or keeping terminals open. Now my Rails app is accessible via a clean hostname (rails.home), and I can easily add more applications without worrying about port conflicts or IP addresses. The /etc/hosts method is perfect for getting started quickly on your development machine. For more complex setups where you want network-wide access, dnsmasq on the Raspberry Pi works greatâ€”though Iâ€™ll admit configuring it properly took some trial and error. What I really appreciate is how Ingress decouples external access from your application. Your Rails pods can be completely internal (ClusterIP service), and the Ingress controller handles all the external routing. When you scale up or redeploy, the Ingress keeps working without any reconfiguration. Thatâ€™s exactly the kind of flexibility Kubernetes is known for. Next up: Iâ€™ll be exploring persistent storage for file uploads, because right now any files uploaded to my Rails app would be lost if the pod restarts. Whatâ€™s Next? In future posts, Iâ€™ll be exploring: Persistent Storage for Uploads - NFS, Longhorn, and ReadWriteMany volumes SSL/TLS with cert-manager - Automatic Letâ€™s Encrypt certificates for HTTPS Mission Control Jobs - Web UI for monitoring SolidQueue background jobs Database Backups - Automated PostgreSQL backups with CronJobs Monitoring with Prometheus and Grafana - Cluster-wide metrics and dashboards Stay tuned! Series Navigation Part 1: Deploy Your First App Part 2: ConfigMaps and Secrets Part 3: Understanding Namespaces Part 4: Understanding Port Mapping in k3d Part 5: Setting Up k3s on Raspberry Pi Part 6: Deploying Rails 8 with SolidQueue on k3s Part 7: Setting Up Ingress Controller â† You just finished this! Part 8: Persistent Storage (Coming soon) Found a mistake or have questions? Feel free to open an issue here. " }, { "title": "Kubernetes Learning Path: Deploying Rails 8 with SolidQueue on Raspberry Pi k3s", "url": "/posts/kubernetes-learning-path-rails-solidqueue-on-k3s/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3s, raspberry-pi, ruby-on-rails, solidqueue, postgresql, deployment", "date": "2025-10-31 14:00:00 +0500", "content": "Kubernetes Learning Path: Deploying Rails 8 with SolidQueue on Raspberry Pi k3s After setting up my 3-node Raspberry Pi k3s cluster in the previous post, I wanted to deploy something realâ€”not just nginx demos, but an actual production-grade application. So I decided to deploy a Ruby on Rails 8 application with all the modern bells and whistles: PostgreSQL, background jobs with SolidQueue, database-backed caching with SolidCache, and proper health checks. This wasnâ€™t a weekend afternoon project. It took me a few days of trial and error, reading documentation, debugging architecture mismatches, and learning a lot about how Rails 8â€™s new features work in a Kubernetes environment. But once everything clicked into place and I saw my Rails app running across multiple pods, processing background jobs on separate workers, all on my little Pi clusterâ€”it felt incredible. Let me walk you through what I built, what went wrong, and what I learned. Why Rails 8? Iâ€™ll be honestâ€”I chose Rails because Iâ€™m already familiar with it. The goal here was to deploy a complete, production-grade application stack on Kubernetes, not to learn a new framework at the same time. Trying to learn Kubernetes and a new language/framework simultaneously would have been overwhelming. Rails was the obvious choice because I know how it works, I understand its conventions, and I can focus on the Kubernetes side of things without getting lost in framework documentation. The Pleasant Surprise: Rails 8â€™s Built-in Features What I didnâ€™t expect was how well Rails 8 fits into Kubernetes. It comes with SolidQueue and SolidCache built-inâ€”database-backed solutions for background jobs and caching. Before Rails 8, youâ€™d typically need Redis for Sidekiq (background jobs) and Redis or Memcached for caching. With Rails 8, everything uses PostgreSQL: Application data â†’ PostgreSQL Background job queue â†’ PostgreSQL (via SolidQueue) Cache storage â†’ PostgreSQL (via SolidCache) The Architecture Weâ€™re Building Hereâ€™s what I deployed in this phase: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ k3s Cluster (3 Raspberry Pis) â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Rails Web â”‚ â”‚ Rails Web â”‚ (Pods) â”‚ â”‚ â”‚ Pod x3 â”‚ â”‚ Pod x3 â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Rails Service â”‚ â”‚ â”‚ â”‚ (ClusterIP) â”‚ â”‚ â”‚ â”‚ Internal Only â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ SolidQueue Workers (x2) â”‚ â”‚ â”‚ â”‚ (Background Job Processors) â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ PostgreSQL 17 â”‚ â”‚ â”‚ â”‚ (StatefulSet) â”‚ â”‚ â”‚ â”‚ + Persistent â”‚ â”‚ â”‚ â”‚ Storage â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Access via: kubectl port-forward (for now) What I deployed: 3 Rails web server pods (Deployment) 2 SolidQueue worker pods (Deployment) 1 PostgreSQL pod (StatefulSet with 5GB persistent storage) 2 Services: rails-service (ClusterIP) and postgres (Headless) Whatâ€™s next (future posts): Traefik Ingress for external access Persistent volumes for file uploads Monitoring with Mission Control Jobs SSL/TLS with cert-manager What Youâ€™ll Need Before starting, make sure you have: A working k3s cluster on Raspberry Pi (from Part 5) Docker installed on your laptop for building images A Docker Hub account (free tier works fine) kubectl configured to access your cluster Basic understanding of Rails (helpful but not required) The Big Challenge: Cross-Architecture Builds This was my first major roadblock. I develop on an x86_64 laptop (Intel/AMD architecture), but Raspberry Pi uses ARM64 architecture. If you build a Docker image on your laptop without specifying the platform, it wonâ€™t run on the Pi. The symptom? Pods that immediately fail with cryptic errors like exec format error or exec /bin/sh: exec format error. Docker Hub Setup (One-Time) Before building and pushing images, you need a Docker Hub account and to log in: # Create a free account at https://hub.docker.com if you don't have one # Free tier includes unlimited public repos # Login to Docker Hub (do this once on each machine you build from) docker login # Enter your Docker Hub username and password Once logged in, you can push images to your Docker Hub account. Solution 1: Build on the Raspberry Pi directly I found out this to be the most reliable method: # SSH to your master node ssh pi@192.168.18.49 # If you haven't logged in on the Pi yet: docker login # Build the image directly on ARM64 hardware docker build -t your-dockerhub-username/my-rails-app:v1 . docker push your-dockerhub-username/my-rails-app:v1 Replace your-dockerhub-username with your actual Docker Hub username. Building on the Pi takes about 15-20 minutes because, well, itâ€™s a Raspberry Pi. But youâ€™ll get a native ARM64 image that works perfectly. Solution 2: Cross-compile on your laptop with QEMU You can also cross-compile on your laptop if you donâ€™t want to SSH to the Pi. This requires QEMU emulation: # On Arch Linux (my setup) sudo pacman -S qemu-user-static qemu-user-static-binfmt sudo systemctl start systemd-binfmt.service # Verify QEMU is working ls /proc/sys/fs/binfmt_misc/ | grep qemu # Should show: qemu-aarch64, qemu-arm, etc. # Now build for ARM64 from your laptop docker buildx build --platform linux/arm64 \\ -t your-dockerhub-username/my-rails-app:v1 \\ --push . Note: In my testing, building directly on the Pi was actually faster than cross-compiling with QEMU emulation. QEMU adds overhead, and the Piâ€™s native ARM64 build was more efficient. The cross-compile option is mainly useful if you canâ€™t SSH to the Pi or want to automate builds from your laptop. Lesson learned: Always specify --platform linux/arm64 when building for Raspberry Pi. I wasted several hours debugging â€œexec format errorâ€ before realizing my x86_64 images wouldnâ€™t run on ARM64. Step 1: Prepare the Rails Application I created a Rails 8.1.1 application with all the modern defaults. If you want to follow along, hereâ€™s the quick setup: # Create new Rails 8 app rails new my-rails-app --database=postgresql # Rails 8 automatically includes: # âœ… Solid Queue (background jobs) # âœ… Solid Cache (caching) # âœ… Dockerfile (production-ready, multi-stage) # âœ… Health check endpoint (/up) The generated Dockerfile is already optimized for production. Rails 8 does a great job here. Important Configuration Changes Rails 8 production mode assumes SSL by default, which caused CSRF errors when I tried to access the app via kubectl port-forward (which uses HTTP, not HTTPS). Make SSL configurable in config/environments/production.rb: # Allow disabling SSL for testing (keep enabled in real production) config.assume_ssl = ENV.fetch(\"RAILS_ASSUME_SSL\", \"true\") == \"true\" config.force_ssl = ENV.fetch(\"RAILS_FORCE_SSL\", \"true\") == \"true\" Fix database connection in config/database.yml: Rails defaults to Unix socket connections for PostgreSQL, but in Kubernetes we need TCP connections: production: &lt;&lt;: *default database: myapp_production username: &lt;%= ENV.fetch(\"DATABASE_USERNAME\") { \"rails_user\" } %&gt; password: &lt;%= ENV[\"DATABASE_PASSWORD\"] %&gt; host: &lt;%= ENV.fetch(\"DATABASE_HOST\") { \"localhost\" } %&gt; port: &lt;%= ENV.fetch(\"DATABASE_PORT\") { \"5432\" } %&gt; The key here is host: reading from DATABASE_HOST environment variable, which will point to our PostgreSQL service. Step 2: Build and Push the Docker Image I built the image on my Raspberry Pi master node for guaranteed compatibility: # SSH to the Pi ssh pi@192.168.18.49 # Clone your app or copy it over # Then build: cd my-rails-app docker build -t your-username/my-rails-app:v1 . docker push your-username/my-rails-app:v1 Building took about 18 minutes on my Pi 4. Grab some coffee. Important: This single image is reused for: Rails web server pods (default CMD) SolidQueue worker pods (override CMD) Database migration init container (runs before web pods start) One image, multiple purposes. Thatâ€™s the beauty of the Rails 8 Dockerfile. Step 3: Deploy PostgreSQL to Kubernetes Before Rails can start, we need the database running. I created four YAML files for PostgreSQL deployment. PostgreSQL Secret First, we need to store database credentials securely. Why we need this: PostgreSQL needs a username, password, and database name to initialize. Kubernetes Secrets store this sensitive data (base64-encoded, can be encrypted at rest). Where itâ€™s used: The PostgreSQL StatefulSet reads these values as environment variables to create the database and user. postgres-secret.yaml: apiVersion: v1 kind: Secret metadata: name: postgres-secret type: Opaque stringData: POSTGRES_USER: rails_user POSTGRES_PASSWORD: change_this_password_123 POSTGRES_DB: myapp_production âš ï¸ Important: Change that password! Never use default passwords in production. Apply it: kubectl apply -f postgres-secret.yaml Persistent Storage Databases need storage that survives pod restarts. Why we need this: Without persistent storage, all database data would be lost when the PostgreSQL pod restarts or crashes. PersistentVolumeClaims (PVCs) request storage from Kubernetes that persists independently of pods. Where itâ€™s used: The PostgreSQL StatefulSet mounts this volume to /var/lib/postgresql/data where PostgreSQL stores its data files. postgres-pvc.yaml: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: postgres-pvc spec: accessModes: - ReadWriteOnce # Single node can mount for read/write resources: requests: storage: 5Gi # Request 5GB of storage Apply it: kubectl apply -f postgres-pvc.yaml # Wait for it to bind kubectl get pvc -w # Press Ctrl+C when STATUS shows \"Bound\" k3s automatically provisions local storage on one of your Pi nodes. PostgreSQL StatefulSet Now for the actual database pod. Why we need this: We use a StatefulSet (not a Deployment) because databases need: Stable pod names (always postgres-0, not random names) Ordered startup and shutdown Stable persistent storage that follows the pod Where itâ€™s used: This creates the PostgreSQL pod that Rails and SolidQueue will connect to for all database operations. postgres-statefulset.yaml: apiVersion: apps/v1 kind: StatefulSet metadata: name: postgres spec: serviceName: postgres # â† Links to postgres-service below replicas: 1 selector: matchLabels: app: postgres # â† StatefulSet manages pods with this label template: metadata: labels: app: postgres # â† Each pod gets this label # postgres-service selector matches this to route traffic spec: containers: - name: postgres image: postgres:17-alpine ports: - containerPort: 5432 # â† Service targetPort forwards here envFrom: - secretRef: name: postgres-secret # â† Injects POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB volumeMounts: - name: postgres-storage mountPath: /var/lib/postgresql/data subPath: postgres volumes: - name: postgres-storage persistentVolumeClaim: claimName: postgres-pvc # â† Mounts storage from postgres-pvc.yaml above Apply it: kubectl apply -f postgres-statefulset.yaml # Watch PostgreSQL start kubectl get pods -l app=postgres -w # Wait for STATUS: Running, READY: 1/1 PostgreSQL Service Now letâ€™s create a stable DNS name for the database. Why we need this: Pod IP addresses change when they restart. A Service gives us a stable DNS name (postgres) that Rails can use to connect. Even if the PostgreSQL pod restarts and gets a new IP, the service name stays the same. Where itâ€™s used: Railsâ€™ database.yml will use host: postgres to connect. Kubernetes DNS automatically resolves this to the PostgreSQL podâ€™s IP. postgres-service.yaml: apiVersion: v1 kind: Service metadata: name: postgres # â† DNS name: pods can connect via \"postgres:5432\" spec: selector: app: postgres # â† Finds pods with label \"app: postgres\" # (matches labels in postgres-statefulset.yaml above) ports: - port: 5432 # â† Service listens on this port targetPort: 5432 # â† Forwards to pod's containerPort 5432 clusterIP: None # â† Headless service for StatefulSet Apply it: kubectl apply -f postgres-service.yaml Now any pod can connect to PostgreSQL using the hostname postgres on port 5432. This is what we configured in database.yml. Verify PostgreSQL is Running # Check pod status kubectl get pods -l app=postgres # Test connection kubectl exec -it postgres-0 -- psql -U rails_user -d myapp_production -c \"SELECT version();\" You should see PostgreSQL version info. Database is ready! Step 4: Configure Rails for Kubernetes Rails needs configuration and secrets to run in Kubernetes. I split these into ConfigMaps (non-sensitive config) and Secrets (sensitive stuff like passwords). Rails ConfigMap Why we need this: Rails needs various environment variables to run (database host, Rails environment, logging settings, etc.). ConfigMaps store non-sensitive configuration that can be shared across all Rails and worker pods. Where itâ€™s used: Both the Rails web deployment and SolidQueue worker deployment will inject these environment variables into their pods. rails-configmap.yaml: apiVersion: v1 kind: ConfigMap metadata: name: rails-config # â† Referenced by rails-deployment and solid-queue-deployment data: RAILS_ENV: \"production\" RAILS_LOG_TO_STDOUT: \"true\" DATABASE_HOST: \"postgres\" # â† Kubernetes DNS resolves \"postgres\" to postgres-service # which routes to postgres pods DATABASE_PORT: \"5432\" RAILS_ASSUME_SSL: \"false\" # For testing without SSL RAILS_FORCE_SSL: \"false\" SOLID_QUEUE_DISPATCHERS_POLLING_INTERVAL: \"1\" SOLID_QUEUE_WORKERS_POLLING_INTERVAL: \"0.1\" Rails Secret Why we need this: Rails requires sensitive data like the secret key (for encrypting sessions/cookies) and database credentials. Unlike ConfigMaps, Secrets are base64-encoded and can be encrypted at rest for better security. Where itâ€™s used: The Rails web pods and SolidQueue workers need these secrets to connect to the database and encrypt user sessions. First, generate a secret key. You can do this from your laptop or from the Piâ€”doesnâ€™t matter: # Option 1: Using Rails (run this on your laptop) docker run --platform linux/arm64 --rm your-username/my-rails-app:v1 ./bin/rails secret # Option 2: Using OpenSSL (simpler - run this on your laptop) openssl rand -hex 64 Youâ€™ll get a long random string like: f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9 Copy this outputâ€”youâ€™ll paste it into the YAML file in the next step. Now create the secret file: rails-secret.yaml: apiVersion: v1 kind: Secret metadata: name: rails-secret # â† Referenced by rails-deployment and solid-queue-deployment type: Opaque stringData: SECRET_KEY_BASE: \"your-generated-secret-key-paste-here\" DATABASE_PASSWORD: \"change_this_password_123\" # â† Must match postgres-secret.yaml DATABASE_USERNAME: \"rails_user\" DATABASE_NAME: \"myapp_production\" âš ï¸ Critical: DATABASE_PASSWORD must exactly match what you set in postgres-secret.yaml! Apply both: kubectl apply -f rails-configmap.yaml kubectl apply -f rails-secret.yaml Step 5: Deploy Rails Web Application Now for the main eventâ€”deploying Rails itself. Rails Deployment with Init Container This is where it gets interesting. The init container runs database migrations before the main Rails container starts. Why we need this: This is the core web application. The Deployment creates 3 Rails pods running Puma (the web server) to handle HTTP requests. We use a Deployment (not StatefulSet) because web servers are statelessâ€”any pod can handle any request. The init container trick: Before the web server starts, an init container runs database migrations. This ensures the database schema is up-to-date before Rails starts serving requests. Where itâ€™s used: These pods handle all HTTP requests to your Rails application. The Service (next step) will load-balance traffic across all 3 pods. rails-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: rails-app spec: replicas: 3 # Three web server pods selector: matchLabels: app: rails # â† Deployment manages pods with this label template: metadata: labels: app: rails # â† Each pod gets this label # rails-service.yaml selector \"app: rails\" finds these pods spec: initContainers: - name: db-setup image: your-username/my-rails-app:v1 command: - sh - -c - | bundle exec rails db:create || echo \"Databases may exist\" bundle exec rails db:prepare bundle exec rails solid_queue:install:migrations bundle exec rails solid_cache:install:migrations bundle exec rails db:migrate envFrom: - configMapRef: name: rails-config # â† Injects DATABASE_HOST=\"postgres\" etc. - secretRef: name: rails-secret # â† Injects DATABASE_PASSWORD, SECRET_KEY_BASE containers: - name: web image: your-username/my-rails-app:v1 ports: - containerPort: 3000 # â† rails-service targetPort forwards here envFrom: - configMapRef: name: rails-config # â† Rails reads DATABASE_HOST to connect to postgres - secretRef: name: rails-secret # â† Rails reads DATABASE_PASSWORD to authenticate livenessProbe: httpGet: path: /up port: 3000 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /up port: 3000 initialDelaySeconds: 10 periodSeconds: 5 resources: requests: memory: \"256Mi\" cpu: \"100m\" limits: memory: \"512Mi\" cpu: \"500m\" Whatâ€™s happening here: Init container runs first: creates databases, installs Solid Queue/Cache migrations, runs migrations Only after init succeeds, main container starts: Rails web server (Puma) Health checks ensure Rails is responding before routing traffic Resource limits prevent any pod from hogging the Piâ€™s limited RAM Apply it: kubectl apply -f rails-deployment.yaml # Watch pods start (this takes longer due to init container) kubectl get pods -l app=rails -w Youâ€™ll see pods in Init:0/1 status while migrations run. Check init logs: # Get pod name kubectl get pods -l app=rails # Check init container logs kubectl logs &lt;pod-name&gt; -c db-setup You should see migration output. Once init completes, pods transition to Running status. A Nasty Bug I Hit: Multi-Database Setup Rails 8 uses separate databases for cache and queue by default: myapp_production (primary) myapp_production_cache (cache) myapp_production_queue (queue) myapp_production_cable (action cable) PostgreSQL only auto-creates myapp_production. The init container failed with â€œDatabase myapp_production_cache does not existâ€. Solution: Add db:create before db:prepare in the init container: bundle exec rails db:create || echo \"Databases may exist\" bundle exec rails db:prepare The || echo prevents failure if databases already exist. Took me an hour to figure this out. Rails Service Time to create a load balancer for the web pods. Why we need this: Just like with PostgreSQL, we need a stable way to access the Rails pods. This Service gives us a single DNS name (rails-service) and automatically distributes incoming requests across all healthy Rails pods. Where itâ€™s used: Weâ€™ll use kubectl port-forward service/rails-service to access the app from our laptop. Later, an Ingress controller would route external traffic to this service. rails-service.yaml: apiVersion: v1 kind: Service metadata: name: rails-service # â† DNS name for accessing Rails internally spec: type: ClusterIP # â† Internal only (use Ingress for external access) selector: app: rails # â† Finds pods with label \"app: rails\" # (matches labels in rails-deployment.yaml) ports: - port: 80 # â† Service listens on port 80 targetPort: 3000 # â† Forwards to pod's containerPort 3000 (Puma) Apply it: kubectl apply -f rails-service.yaml # Check endpoints (should list all 3 Rails pod IPs) kubectl get endpoints rails-service The service automatically distributes traffic across your 3 Rails pods. Step 6: Deploy SolidQueue Workers Note: This is where I stopped in my initial deployment. Steps 7-8 below cover testing and troubleshooting what Iâ€™ve deployed so far. Future work like Ingress and persistent storage for uploads will be covered in upcoming posts. Background jobs need dedicated worker processes. Why we need this: Background jobs (sending emails, processing uploads, etc.) shouldnâ€™t run on the web server podsâ€”theyâ€™d compete for resources and slow down HTTP responses. SolidQueue workers are dedicated pods that poll the database for jobs and process them. The clever part: We use the exact same Docker image (your-username/my-rails-app:v1) for both web servers and workers. The only difference is what command we tell the container to run: Web pods: Run the default command from the Dockerfile â†’ Puma web server starts (rails server) Worker pods: Override the command in the deployment YAML â†’ SolidQueue starts (bundle exec rake solid_queue:start) This means you only need to build and maintain one Docker image for your entire Rails stack. Same codebase, different processes. Where itâ€™s used: These workers continuously poll the solid_queue_jobs table in PostgreSQL, pick up new jobs, process them, and mark them complete. solid-queue-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: solid-queue-worker spec: replicas: 2 selector: matchLabels: app: solid-queue-worker # â† Deployment manages pods with this label template: metadata: labels: app: solid-queue-worker # â† Each worker pod gets this label spec: containers: - name: worker image: your-username/my-rails-app:v1 # â† Same image as rails-deployment command: # â† Different command: runs SolidQueue instead of web server - bundle - exec - rake - solid_queue:start envFrom: - configMapRef: name: rails-config # â† Workers use same DATABASE_HOST=\"postgres\" to connect - secretRef: name: rails-secret # â† Workers use same DATABASE_PASSWORD to authenticate resources: requests: memory: \"256Mi\" cpu: \"100m\" limits: memory: \"512Mi\" cpu: \"500m\" Apply it: kubectl apply -f solid-queue-deployment.yaml # Watch workers start kubectl get pods -l app=solid-queue-worker -w Check worker logs: kubectl logs -f deployment/solid-queue-worker You should see SolidQueue starting up and polling for jobs. Understanding the Command Difference Now that youâ€™ve deployed both the Rails web servers and SolidQueue workers, letâ€™s see exactly how Kubernetes runs different commands from the same Docker image. In the Rails deployment (rails-deployment.yaml), we donâ€™t specify a command: field, so Kubernetes uses the default from the Dockerfile: containers: - name: web image: your-username/my-rails-app:v1 # No command: specified = uses Dockerfile default (Puma web server) In the worker deployment (solid-queue-deployment.yaml), we override the command: containers: - name: worker image: your-username/my-rails-app:v1 # Same image! command: # This overrides the Dockerfile default - bundle - exec - rake - solid_queue:start This is a powerful Kubernetes pattern: the command: field in your deployment YAML can override whatever CMD or ENTRYPOINT is defined in the Dockerfile. One image, multiple uses! Step 7: Test the Full Stack Time to see if everything works together. Access Rails Application Use port-forwarding to access the app: kubectl port-forward service/rails-service 3000:80 Open http://localhost:3000 in your browser. You should see your Rails app! Test Background Jobs Open a Rails console: kubectl exec -it deployment/rails-app -- rails console In the console, queue a test job: # Create a simple job class if you don't have one class TestJob &lt; ApplicationJob def perform(message) puts \"Processing: #{message}\" end end # Queue the job TestJob.perform_later(\"Hello from Kubernetes!\") # Check job count SolidQueue::Job.count # =&gt; 1 # Exit console exit Now check the worker logs: kubectl logs -f deployment/solid-queue-worker You should see the worker picking up and processing the job! If you see â€œProcessing: Hello from Kubernetes!â€ in the logs, your entire stack is working. Step 8: Challenges I Hit and How I Solved Them This deployment wasnâ€™t without its struggles. Here are the main issues I encountered and how I fixed them: 1. Cross-Architecture Exec Format Error Problem: Built image on x86_64 laptop, pods failed with â€œexec /bin/sh: exec format errorâ€ on ARM64 Pi. Solution: Build with --platform linux/arm64 on laptop with QEMU, or build directly on Pi. Lesson: Always specify target platform for cross-compilation. 2. PostgreSQL Socket Connection Error Problem: Init container failed: â€œconnection to socket â€˜/var/run/postgresql/.s.PGSQL.5432â€™ failedâ€ Solution: Rails was trying Unix socket. Fixed by ensuring database.yml reads DATABASE_HOST from env var (TCP connection). Lesson: Kubernetes requires TCP connections between pods, not Unix sockets. 3. Multi-Database Creation Problem: Init failed: â€œDatabase myapp_production_cache does not existâ€ Solution: Added rails db:create before db:prepare in init container to create all databases. Lesson: Rails 8 multi-database setup requires explicit database creation. 4. CSRF Errors with Port-Forward Problem: Got â€œHTTP Origin header didnâ€™t match request.base_urlâ€ when accessing via port-forward. Solution: Made force_ssl and assume_ssl configurable via env vars, disabled for local testing. Lesson: Production Rails expects HTTPS. For local testing without SSL, make it configurable. 5. Image Caching Issues Problem: Pushed updated image but pods still ran old cached version. Solution: Used new tag (v2, v3) instead of reusing v1. Kubernetes caches by tag. Lesson: Use unique tags for each build, or set imagePullPolicy: Always. Monitoring Your Application Check Pod Status # All pods kubectl get pods # Just Rails pods kubectl get pods -l app=rails # Just workers kubectl get pods -l app=solid-queue-worker # Detailed pod info kubectl describe pod &lt;pod-name&gt; View Logs # Rails web logs kubectl logs -f deployment/rails-app # Worker logs kubectl logs -f deployment/solid-queue-worker # PostgreSQL logs kubectl logs postgres-0 # Logs from specific pod kubectl logs &lt;pod-name&gt; # Previous crashed container logs kubectl logs &lt;pod-name&gt; --previous Rails Console Access # Open console in a web pod kubectl exec -it deployment/rails-app -- rails console # Check database connection &gt; ActiveRecord::Base.connection.execute('SELECT 1') # Check job queue &gt; SolidQueue::Job.count &gt; SolidQueue::Job.pending.count &gt; SolidQueue::Job.failed.count # Exit &gt; exit What You Learned âœ… How to deploy a production Rails 8 application on Kubernetes âœ… StatefulSets vs Deployments (databases vs stateless apps) âœ… Persistent storage with PersistentVolumeClaims âœ… Init containers for database migrations âœ… ConfigMaps and Secrets for configuration management âœ… Services for load balancing and service discovery âœ… Health checks with liveness and readiness probes âœ… Cross-architecture Docker builds (x86_64 â†’ ARM64) âœ… SolidQueue for background jobs in Kubernetes âœ… Resource limits and requests on Raspberry Pi Conclusion Deploying Rails 8 with SolidQueue on my Raspberry Pi k3s cluster was one of the most educational projects Iâ€™ve done. Itâ€™s one thing to deploy nginx demos; itâ€™s another to deploy a full production application stack with database, background jobs, and health checksâ€”all running on $35 computers in a distributed cluster. The challenges were real: architecture mismatches, database connection errors, multi-database setup, SSL configuration. But each problem taught me something valuable about how Kubernetes works and how modern Rails is designed to run in containerized environments. What really impressed me was how well Rails 8 works in Kubernetes. The SolidQueue and SolidCache features reduce infrastructure complexity significantlyâ€”no Redis to manage means fewer moving parts, simpler networking, and lower resource usage on the Pi cluster. And honestly? Seeing background jobs process on dedicated worker pods while the web server handles HTTP requests, all automatically distributed by Kubernetes services, with health checks ensuring everything stays healthyâ€”thatâ€™s pretty cool. This is just the beginning. Right now Iâ€™m using kubectl port-forward to access the app, which isnâ€™t practical for long-term use. In upcoming posts, Iâ€™ll be adding Ingress for proper external access, persistent storage for file uploads, monitoring with Mission Control Jobs, and possibly SSL/TLS with cert-manager. But the foundation is solid, and I have a working Rails cluster to build on. Whatâ€™s Next? I have a working Rails cluster, but thereâ€™s more to do. In the next post, Iâ€™ll be setting up an Ingress Controller for proper external HTTP accessâ€”no more port-forward! After that, Iâ€™ll be exploring: Persistent storage for uploads using NFS or Longhorn Mission Control Jobs for monitoring background jobs via web UI SSL/TLS with cert-manager and Letâ€™s Encrypt Monitoring with Prometheus and Grafana on the Pi cluster CI/CD pipelines with GitHub Actions deploying to k3s Database backups and disaster recovery The foundation is solid. Now itâ€™s time to make it production-ready. Stay tuned! Series Navigation Part 1: Deploy Your First App Part 2: ConfigMaps and Secrets Part 3: Understanding Namespaces Part 4: Understanding Port Mapping in k3d Part 5: Setting Up k3s on Raspberry Pi Part 6: Deploying Rails 8 with SolidQueue on k3s â† You just finished this! Part 7: Setting Up Ingress Controller Part 8: Persistent Storage (Coming soon) Found a mistake or have questions? Feel free to open an issue here. " }, { "title": "Kubernetes Learning Path: Setting Up k3s on Raspberry Pi", "url": "/posts/kubernetes-learning-path-k3s-on-raspberry-pi/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3s, raspberry-pi, deployment", "date": "2025-10-29 10:00:00 +0500", "content": "Kubernetes Learning Path: Setting Up k3s on Raspberry Pi After going through the first four parts of this series with k3d, I felt ready to take things to the next level. k3d is great for learning and local development, but thereâ€™s something different about running Kubernetes on actual hardware. You deal with real networking, real resource constraints, and real high availability scenarios that you just canâ€™t simulate properly on a single machine. So I decided to set up a 3-node Kubernetes cluster on Raspberry Pi devices using k3s. This would give me a production-like environment at home where I could practice everything I learned with k3d, but on real distributed hardware. Iâ€™ll be honestâ€”this took me a couple of hours and involved some head-scratching moments. But once everything clicked into place and I saw all three nodes showing â€œReadyâ€ status, it felt amazing. Let me walk you through what I did, what went wrong, and what I learned. Why Raspberry Pi and k3s? Before we dive in, you might be wondering why I chose this setup. I went with Raspberry Pi because theyâ€™re affordable, donâ€™t cost much to run 24/7, and honestly itâ€™s just fun to build things with them. Plus, dealing with ARM architecture and limited resources teaches you a lot about how Kubernetes manages resourcesâ€”you canâ€™t just throw CPU and RAM at problems like you might on a beefy server. As for k3s, itâ€™s basically Kubernetes but lightweight enough to actually run on a Pi without choking it. The best part? It has the same API as full Kubernetes, so everything you learn transfers directly. I wasnâ€™t interested in running some toy version of Kubernetesâ€”I wanted the real thing, just smaller. What Youâ€™ll Need Hereâ€™s what I used for my setup: Hardware: 3Ã— Raspberry Pi 4 (Iâ€™d recommend 4GB+ RAM each) 3Ã— SSD drives (You can try using an SD card, but I did not want to use it) Ethernet cables for all Pis A network switch or router with enough ports Power supplies for each Pi Software: Ubuntu Server 24.04 LTS on all devices SSH access to all Pis I went with Ubuntu Server because Iâ€™m already familiar with it. You could probably use Raspberry Pi OS too, but I stuck with what I know. The Big Picture Before we start, hereâ€™s what weâ€™re building: My Network (192.168.18.0/24) â”‚ â”œâ”€â”€ k3s-master (192.168.18.49) [Control Plane] â”œâ”€â”€ k3s-worker-1 (192.168.18.51) [Worker Node] â””â”€â”€ k3s-worker-2 (192.168.18.52) [Worker Node] The master node runs the Kubernetes control plane (API server, scheduler, etc.), and the worker nodes run your actual application workloads. All three nodes are connected via ethernet for stability. Step 1: Network Setupâ€”The Foundation That Matters This was the most important step, and I learned it the hard way. If your master nodeâ€™s IP address changes after youâ€™ve set up the cluster, the whole thing breaks. Worker nodes connect to the master using its IP, and that IP is hardcoded during the join process. So the very first thing you need to do is set up static IP addresses on all your Raspberry Pis. Configure Static IPs with Netplan On each Raspberry Pi, I configured a static IP using netplan. Hereâ€™s what I did on the master node: sudo nano /etc/netplan/50-cloud-init.yaml I replaced the contents with this configuration: network: version: 2 renderer: networkd ethernets: eth0: dhcp4: no addresses: - 192.168.18.49/24 # Master node IP routes: - to: 0.0.0.0/0 via: 192.168.18.1 # Your router IP metric: 100 nameservers: addresses: - 8.8.8.8 - 1.1.1.1 Important things to note: Replace 192.168.18.49 with your desired static IP Replace 192.168.18.1 with your routerâ€™s IP (usually 192.168.1.1 or 192.168.0.1) Use different IPs for each node (.49 for master, .51 for worker-1, .52 for worker-2) Apply the configuration: sudo netplan apply Verify the IP address: ip addr show eth0 You should see your static IP address listed. If you donâ€™t have network access after applying, double-check that your router IP and subnet are correct. Pro tip: I initially had both WiFi and ethernet active on my Pis, which caused routing conflicts. I disabled WiFi completely and used only ethernet. Much more stable. Set Up Hostname Resolution To make life easier, I set up /etc/hosts on all nodes so I could use hostnames instead of IP addresses: sudo nano /etc/hosts Add these lines on all three nodes: 192.168.18.49 k3s-master 192.168.18.51 k3s-worker-1 192.168.18.52 k3s-worker-2 Now you can ping by hostname: ping k3s-master This makes troubleshooting and managing the cluster much easier. Step 2: Preparing All Nodes Before installing k3s, each Raspberry Pi needs some preparation. I had to do these steps on all three nodes. System Updates Always start with updates: sudo apt update &amp;&amp; sudo apt upgrade -y sudo apt install -y curl vim git This took about 10-15 minutes per Pi on my internet connection. Set Hostnames On each Pi, set the appropriate hostname: # On master node: sudo hostnamectl set-hostname k3s-master # On worker-1: sudo hostnamectl set-hostname k3s-worker-1 # On worker-2: sudo hostnamectl set-hostname k3s-worker-2 Verify: hostname Enable Legacy iptables This step confused me at first. Modern Ubuntu uses something called nftables by default, but k3s needs the older iptables-legacy for its networking to work. I didnâ€™t really understand why until I forgot to do it on one node and spent an hour debugging why pods couldnâ€™t talk to each other. Run this on all nodes: sudo update-alternatives --set iptables /usr/sbin/iptables-legacy sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy Lesson learned: donâ€™t skip this step. Enable cgroup Memory Kubernetes needs cgroup support for resource management (CPU and memory limits). This requires editing the boot parameters: sudo nano /boot/firmware/cmdline.txt Youâ€™ll see a long line of parameters. Add these to the end of the existing line (donâ€™t create a new line): cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory The entire line might look something like this: console=serial0,115200 console=tty1 root=PARTUUID=12345678-02 rootfstype=ext4 elevator=deadline rootwait cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory Warning: These parameters must be on the same line as the existing boot parameters. If you put them on a new line, they wonâ€™t work. Save the file and reboot: sudo reboot After the reboot, verify cgroup memory is enabled: cat /proc/cgroups | grep memory You should see output showing memory cgroup support. Step 3: Installing k3s on the Master Node Okay, now for the fun partâ€”actually installing k3s. After all that prep work, the installation itself is almost anticlimactic. On the master node: curl -sfL https://get.k3s.io | sh - Thatâ€™s it. The script downloads and installs k3s automatically. It took about 2-3 minutes on my Pi. Verify the Master Node Check that k3s is running: sudo systemctl status k3s You should see output showing k3s is â€œactive (running)â€. Check the node status: sudo k3s kubectl get nodes Output: NAME STATUS ROLES AGE VERSION k3s-master Ready control-plane,master 30s v1.27.7+k3s1 Seeing â€œReadyâ€ status means the master node is operational. Get the Join Token Worker nodes need a token to join the cluster. Get it from the master: sudo cat /var/lib/rancher/k3s/server/node-token Youâ€™ll see a long token like: K107f8a9b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0z::server:a1b2c3d4e5f6g7h8i9j0 Copy this tokenâ€”youâ€™ll need it in the next step. I saved it to a text file for easy reference. Set Up kubectl Access By default, you need sudo to run kubectl commands. Letâ€™s fix that: mkdir ~/.kube sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config sudo chown $USER:$USER ~/.kube/config chmod 600 ~/.kube/config Now you can run kubectl without sudo: kubectl get nodes Much better. Access the Cluster from Your Laptop Honestly, SSHing into the master node every time you want to run a kubectl command gets old fast. Letâ€™s set it up so you can manage the cluster directly from your laptop. First, make sure you have kubectl installed on your local machine. If you donâ€™t: # macOS brew install kubectl # Linux curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Now, copy the kubeconfig from the master node to your laptop. On your local machine, run: # Create .kube directory if it doesn't exist mkdir -p ~/.kube # Copy the config from master node scp user@192.168.18.49:~/.kube/config ~/.kube/k3s-config Replace user with your actual username on the Pi, and adjust the IP if needed. Hereâ€™s the important part: the config file references 127.0.0.1 (localhost), which works when youâ€™re on the master node but not from your laptop. You need to change it to the master nodeâ€™s actual IP. Edit the file: nano ~/.kube/k3s-config Find this line: server: https://127.0.0.1:6443 Change it to: server: https://192.168.18.49:6443 Save and exit. Now tell kubectl to use this config: export KUBECONFIG=~/.kube/k3s-config Test it: kubectl get nodes Output: NAME STATUS ROLES AGE VERSION k3s-master Ready control-plane,master 10m v1.27.7+k3s1 It works! Youâ€™re now managing the cluster from your laptop without SSH. Pro tip: If you want this to persist across terminal sessions, add export KUBECONFIG=~/.kube/k3s-config to your ~/.bashrc or ~/.zshrc file. Or, if you want to merge it with your default kubeconfig, you can copy the content into ~/.kube/config and use context switching. This makes the whole experience way more convenient. I can be coding on my laptop, deploy to the cluster with kubectl apply, and check logsâ€”all without leaving my editor or opening another terminal to SSH. Step 4: Joining Worker Nodes Now letâ€™s add the worker nodes to the cluster. On each worker node, run this command (replace the token with your actual token from the master): curl -sfL https://get.k3s.io | K3S_URL=https://192.168.18.49:6443 \\ K3S_TOKEN=K107f8a9b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0z::server:a1b2c3d4e5f6g7h8i9j0 sh - What this does: K3S_URL tells the worker where the master node is K3S_TOKEN authenticates the worker with the master The script installs k3s in agent mode (worker mode) Installation takes a couple of minutes per worker. Verify Worker Nodes On each worker, check that the k3s agent is running: sudo systemctl status k3s-agent You should see â€œactive (running)â€. Now go back to the master node and check all nodes: kubectl get nodes Output: NAME STATUS ROLES AGE VERSION k3s-master Ready control-plane,master 5m v1.27.7+k3s1 k3s-worker-1 Ready &lt;none&gt; 2m v1.27.7+k3s1 k3s-worker-2 Ready &lt;none&gt; 1m v1.27.7+k3s1 All three nodes showing â€œReadyâ€! This is the moment where all the preparation pays off. You now have a working Kubernetes cluster running on real hardware. Step 5: Testing the Cluster Letâ€™s deploy something to make sure everything actually works. Iâ€™ll deploy a simple nginx application with 3 replicas, which should spread across all the worker nodes. Create a file called nginx-test.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort selector: app: nginx ports: - port: 80 targetPort: 80 nodePort: 30080 Deploy it: kubectl apply -f nginx-test.yaml Output: deployment.apps/nginx-test created service/nginx-service created Watch the pods come up: kubectl get pods -o wide -w Output: NAME READY STATUS RESTARTS AGE NODE nginx-test-7d4c9d8c9b-4xk2p 1/1 Running 0 10s k3s-worker-1 nginx-test-7d4c9d8c9b-7m8qz 1/1 Running 0 10s k3s-worker-2 nginx-test-7d4c9d8c9b-9n2kx 1/1 Running 0 10s k3s-worker-1 Notice how the pods are distributed across the worker nodes automatically! Kubernetes is doing its job of spreading the workload. Access the Application Since we used NodePort, we can access the application through any nodeâ€™s IP on port 30080: curl http://192.168.18.49:30080 Or from your laptopâ€™s browser (if youâ€™re on the same network): http://192.168.18.49:30080 You should see the nginx welcome page. Try accessing it through the other nodes too: curl http://192.168.18.51:30080 curl http://192.168.18.52:30080 All three IPs work! NodePort makes the service available on all nodes, regardless of where the pods are actually running. Challenges I Ran Into Let me share the issues I encountered so you can avoid them: WiFi and Ethernet Conflicts Initially, I had both WiFi and ethernet enabled on my Pis. This caused weird routing issues where the cluster would work sometimes and fail other times. The solution was to disable WiFi completely in netplan: network: version: 2 renderer: networkd # Wifi disabled - Only eth0 cgroup Parameters on Wrong Line My first attempt at adding cgroup parameters, I put them on a new line in cmdline.txt. This doesnâ€™t workâ€”boot parameters must be on a single line. The fix was to add them to the end of the existing line. Forgetting iptables on One Node I accidentally skipped the iptables-legacy configuration on one worker node. Pods on that node couldnâ€™t communicate with pods on other nodes. The symptom was that some HTTP requests would work and others would timeout, depending on which pod handled the request. Once I set up iptables-legacy on all nodes and rebooted, everything worked. What I Practiced Now that I have a real cluster, Iâ€™ve been practicing all the concepts from the previous posts: From the k3d tutorials: âœ… Multi-replica deployments that actually run on different physical machines âœ… ConfigMaps and Secrets for configuration management âœ… Namespaces for environment separation âœ… Services with NodePort for external access New things with real hardware: âœ… Real high availability: I powered off one worker node and watched Kubernetes automatically reschedule the pods to the remaining nodes. This is the kind of behavior you canâ€™t properly test on k3d. âœ… Resource distribution: Seeing how Kubernetes spreads pods across nodes based on available resources The experience of seeing pods automatically move from a failed node to healthy ones was really cool. This is what Kubernetes is designed for. Things I Wish Iâ€™d Known Before Starting Looking back, hereâ€™s what would have saved me time: Get the networking sorted out first. Seriously. Make sure those static IPs are stable before you even think about installing k3s. I canâ€™t stress this enoughâ€”if your master nodeâ€™s IP changes later, youâ€™re basically starting over. Skip WiFi, use ethernet. I tried having both enabled at first and it caused all sorts of weird routing problems. Just use ethernet and call it a day. Write everything down. IP addresses, hostnames, that join tokenâ€”put it all in a text file. Future you will thank present you when something breaks at 11 PM and you canâ€™t remember which node is which. Test as you go. Donâ€™t configure all three Pis and then try to figure out what went wrong. Do one, make sure it works, then move to the next. Pis are slow, thatâ€™s okay. If a pod takes 45 seconds to start, thatâ€™s just how it is with Raspberry Pi. Donâ€™t panic thinking somethingâ€™s broken. What You Learned âœ… How to set up static networking on Raspberry Pi âœ… How to install and configure k3s on multiple nodes âœ… How to create a multi-node Kubernetes cluster on real hardware âœ… The importance of proper preparation (iptables, cgroups, networking) âœ… How to troubleshoot common cluster setup issues âœ… The difference between local development (k3d) and real clusters Conclusion Setting up k3s on Raspberry Pi was one of the most rewarding things Iâ€™ve done in my Kubernetes learning journey. Yeah, it took a few hours and I definitely had moments where I questioned my life choices (looking at you, networking issues), but seeing all three nodes report â€œReadyâ€ made it all worth it. The best part? Now I have a production-like environment sitting on my desk where I can break things without worrying about cloud bills. Everything I learned with k3d applies here, except now when I mess up, itâ€™s on real hardware with actual network cables and blinking LEDs. Thereâ€™s something satisfying about that. If youâ€™ve been learning Kubernetes with k3d or minikube, Iâ€™d say take the jump to real hardware when you feel ready. The lessons you get from dealing with actual networking problems and watching a node physically fail (or accidentally unplugging one) are different from simulations. Plus, having your own cluster just feels cool. Whatâ€™s Next? In future posts, Iâ€™ll be documenting what I build on this cluster: Setting up Traefik Ingress with custom domains Deploying a full Rails application with PostgreSQL Configuring persistent storage with NFS Implementing monitoring and logging CI/CD pipelines targeting the cluster Stay tuned! Series Navigation Part 1: Deploy Your First App Part 2: ConfigMaps and Secrets Part 3: Understanding Namespaces Part 4: Understanding Port Mapping in k3d Part 5: Setting Up k3s on Raspberry Pi â† You just finished this! Part 6: Persistent Storage (Coming soon) " }, { "title": "Kubernetes Learning Path: Understanding Port Mapping in k3d", "url": "/posts/kubernetes-learning-path-port-mapping/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3d, port-mapping, nodeport, loadbalancer", "date": "2025-10-23 10:00:00 +0500", "content": "Kubernetes Learning Path: Understanding Port Mapping in k3d In the previous posts, weâ€™ve been using kubectl port-forward to access our applications. That works fine for testing, but itâ€™s manual and you can only forward one service at a time. What if you want to run multiple services and access them all through different ports on your localhost? When I first started with k3d, I thought I could just create a cluster, deploy some apps, and hit localhost:8080 or localhost:3000 to see them. Nope. Services running inside the cluster arenâ€™t automatically accessible from your host machine. You need to set up port mapping when you create the cluster. Hereâ€™s where it gets trickyâ€”I spent way too long trying to map multiple services using LoadBalancer, only to get 404 errors. It took me a while to figure out that LoadBalancer in k3d requires Ingress configuration to actually route traffic. Once I discovered NodePort as a simpler alternative for local development, everything clicked. In this post, Iâ€™ll show you what I learned about port mapping in k3d, including the LoadBalancer mistake I made and the NodePort solution that actually worked. The Problem: Services Are Inside the Cluster Your k3d cluster runs inside Docker containers. When you create a service, it gets an IP address inside the Docker network, not on your host machine. So when you try to access localhost:8080, nothing happensâ€”thereâ€™s no service listening there. Think of your cluster like a virtual network inside your computer. To access services in that network, you need a way to route traffic from your localhost into the clusterâ€”thatâ€™s what port mapping does. Understanding Port Mapping Port mapping tells k3d: â€œwhen traffic hits localhost:8080 on my machine, forward it to a specific port inside the cluster.â€ You set this up when you create the cluster. The basic syntax looks like this: --port \"8080:30080@server:0\" This breaks down to: 8080 - Port on your localhost 30080 - Port inside the cluster @server:0 - Target the first server node But hereâ€™s where it gets confusingâ€”what port do you map to? LoadBalancer port 80? A NodePort? I made the mistake of using LoadBalancer, and that didnâ€™t work the way I expected. My First Attempt: Using LoadBalancer I thought I could create a cluster with multiple LoadBalancer port mappings, like this: k3d cluster create learning \\ --port \"8080:80@loadbalancer\" \\ --port \"3000:80@loadbalancer\" Output: INFO[0000] Prep: Network INFO[0000] Created network 'k3d-learning' INFO[0000] Created image volume k3d-learning-images INFO[0001] Creating node 'k3d-learning-server-0' INFO[0002] Creating LoadBalancer 'k3d-learning-serverlb' INFO[0003] Cluster 'learning' created successfully! Then I deployed two different nginx applications with LoadBalancer services, making sure to give them custom content so I could tell them apart. Let me show you exactly what I did so you can see where it goes wrong. First, create ConfigMaps with different content for each app: kubectl create configmap app1-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;APP 1&lt;/h1&gt;&lt;p&gt;This should only appear on port 8080&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' kubectl create configmap app2-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;APP 2&lt;/h1&gt;&lt;p&gt;This should only appear on port 3000&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' Output: configmap/app1-html created configmap/app2-html created Create the first deployment and service. Create a file called app1-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: app1 spec: replicas: 1 selector: matchLabels: app: app1 template: metadata: labels: app: app1 spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: html # References the volume defined below mountPath: /usr/share/nginx/html # Where nginx looks for HTML files volumes: - name: html # Volume name referenced above configMap: name: app1-html # References the ConfigMap we created --- apiVersion: v1 kind: Service metadata: name: app1-service spec: type: LoadBalancer # Using LoadBalancer type selector: app: app1 # Routes traffic to pods with label app=app1 ports: - port: 80 # Port the service listens on inside the cluster targetPort: 80 # Port on the pod containers Apply it: kubectl apply -f app1-deployment.yaml Output: deployment.apps/app1 created service/app1-service created How ConfigMap keys become files: When you mount a ConfigMap as a volume, Kubernetes automatically converts each key-value pair into a file. The ConfigMap key (index.html) becomes the filename, and the value (the HTML content) becomes the file contents. So when we mount this ConfigMap at /usr/share/nginx/html, Kubernetes will create /usr/share/nginx/html/index.html inside the container with our HTML content. This is why nginx can find and serve the fileâ€”itâ€™s automatically created in the directory where nginx looks for HTML files. Create the second deployment and service. Create app2-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: app2 spec: replicas: 1 selector: matchLabels: app: app2 template: metadata: labels: app: app2 spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: html # References the volume defined below mountPath: /usr/share/nginx/html # Where nginx looks for HTML files volumes: - name: html # Volume name referenced above configMap: name: app2-html # References app2's ConfigMap --- apiVersion: v1 kind: Service metadata: name: app2-service spec: type: LoadBalancer # Using LoadBalancer type selector: app: app2 # Routes traffic to pods with label app=app2 ports: - port: 80 # Port the service listens on inside the cluster targetPort: 80 # Port on the pod containers Apply it: kubectl apply -f app2-deployment.yaml Output: deployment.apps/app2 created service/app2-service created Check your services: kubectl get services Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app1-service LoadBalancer 10.43.123.45 &lt;pending&gt; 80:31234/TCP 1m app2-service LoadBalancer 10.43.234.56 &lt;pending&gt; 80:31567/TCP 30s kubernetes ClusterIP 10.43.0.1 &lt;none&gt; 443/TCP 5m Both services are created! I opened my browser and tried: http://localhost:8080 - Shows â€œ404 page not foundâ€ http://localhost:3000 - Showsâ€¦ â€œ404 page not foundâ€ Wait, what? Iâ€™m getting 404 errors from both ports. The services are running (I can see them in kubectl get services), but I canâ€™t access them. Whatâ€™s going on? What I Figured Out About LoadBalancer Mapping After some testing and digging, hereâ€™s what I learned: k3d uses Traefik as its built-in load balancer. When you map ports to @loadbalancer, those ports forward traffic to Traefik. But hereâ€™s the catchâ€”Traefik needs Ingress resources to know how to route traffic to your services. Without Ingress configuration, Traefik has no routing rules and just returns 404. â€œBut wait, didnâ€™t port-forward work in the previous tutorials?â€ Yes! And that confused me at first too. Hereâ€™s the key difference: kubectl port-forward creates a direct tunnel from your machine to the pod through the Kubernetes API server. It completely bypasses all cluster networking, load balancers, and ingress controllers. Itâ€™s purely a development tool that works immediately with zero configuration. LoadBalancer with port mapping routes traffic through k3dâ€™s Traefik load balancer, which expects proper Ingress resources with routing rules (hostnames, paths, etc.) to know which service should handle the traffic. So to make LoadBalancer work in k3d, youâ€™d need to: Create Ingress resources for each service Configure host-based routing (like app1.localhost and app2.localhost) Set up DNS or host file entries Possibly configure TLS certificates For local development, that seemed like way too much work just to access my apps. I needed something simpler, and thatâ€™s when I discovered NodePort. (Weâ€™ll explore Ingress and Traefik in a future postâ€”theyâ€™re powerful for production setups, but overkill for basic local development.) What Worked for Me: Using NodePort What ended up working was using NodePort instead of LoadBalancer, and mapping my localhost ports directly to specific NodePorts. This way, each localhost port goes straight to a specific service without going through a shared load balancer. First, letâ€™s clean up the old cluster: k3d cluster delete learning Output: INFO[0000] Deleting cluster 'learning' INFO[0001] Cluster 'learning' deleted successfully! Now create a new cluster with NodePort mappings: k3d cluster create learning \\ --port \"8080:30080@server:0\" \\ --port \"3000:30081@server:0\" Output: INFO[0000] Prep: Network INFO[0000] Created network 'k3d-learning' INFO[0000] Created image volume k3d-learning-images INFO[0001] Creating node 'k3d-learning-server-0' INFO[0002] Creating LoadBalancer 'k3d-learning-serverlb' INFO[0003] Cluster 'learning' created successfully! The port mapping syntax means: Traffic to localhost:8080 â†’ forwards to port 30080 inside the cluster Traffic to localhost:3000 â†’ forwards to port 30081 inside the cluster @server:0 targets the first server node (the control plane) NodePort services listen on ports between 30000-32767 by default, so weâ€™re using 30080 and 30081. Deploy Apps with NodePort Services Now letâ€™s deploy two apps with NodePort services that use specific ports. Weâ€™ll also customize the content so we can tell them apart. Step 1: Create App 1 with Custom Content First, create a ConfigMap with custom HTML for app1: kubectl create configmap app1-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 1 on localhost:8080&lt;/h1&gt;&lt;p&gt;NodePort: 30080&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' Output: configmap/app1-html created Create a file called app1-nodeport.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: app1 spec: replicas: 1 selector: matchLabels: app: app1 template: metadata: labels: app: app1 spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: html # References the volume defined below mountPath: /usr/share/nginx/html # Where nginx looks for HTML files volumes: - name: html # Volume name referenced above configMap: name: app1-html # References the ConfigMap we created --- apiVersion: v1 kind: Service metadata: name: app1-service spec: type: NodePort # Using NodePort instead of LoadBalancer selector: app: app1 # Routes traffic to pods with label app=app1 ports: - port: 80 # Port the service listens on inside the cluster targetPort: 80 # Port on the pod containers nodePort: 30080 # Specific NodePort (must match cluster port mapping) The key parts: The ConfigMap contains our custom HTML The volume makes the ConfigMap available to the pod The volumeMount puts that HTML where nginx expects to find it (/usr/share/nginx/html) The NodePort is set to 30080, which matches our cluster port mapping Apply it: kubectl apply -f app1-nodeport.yaml Output: deployment.apps/app1 created service/app1-service created Step 2: Create App 2 with Different Content Create a ConfigMap for app2: kubectl create configmap app2-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 2 on localhost:3000&lt;/h1&gt;&lt;p&gt;NodePort: 30081&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' Output: configmap/app2-html created Create app2-nodeport.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: app2 spec: replicas: 1 selector: matchLabels: app: app2 template: metadata: labels: app: app2 spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: html # References the volume defined below mountPath: /usr/share/nginx/html # Where nginx looks for HTML files volumes: - name: html # Volume name referenced above configMap: name: app2-html # References app2's ConfigMap --- apiVersion: v1 kind: Service metadata: name: app2-service spec: type: NodePort # Using NodePort instead of LoadBalancer selector: app: app2 # Routes traffic to pods with label app=app2 ports: - port: 80 # Port the service listens on inside the cluster targetPort: 80 # Port on the pod containers nodePort: 30081 # Different NodePort for app2 Apply it: kubectl apply -f app2-nodeport.yaml Output: deployment.apps/app2 created service/app2-service created Step 3: Verify the Deployments Check that everything is running: kubectl get all Output: NAME READY STATUS RESTARTS AGE pod/app1-7d8c9f5b4-x9k2p 1/1 Running 0 1m pod/app2-6n4m8r3c2-y7p5q 1/1 Running 0 45s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/app1-service NodePort 10.43.123.45 &lt;none&gt; 80:30080/TCP 1m service/app2-service NodePort 10.43.234.56 &lt;none&gt; 80:30081/TCP 45s service/kubernetes ClusterIP 10.43.0.1 &lt;none&gt; 443/TCP 5m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/app1 1/1 1 1 1m deployment.apps/app2 1/1 1 1 45s Notice the PORT(S) column shows 80:30080/TCP and 80:30081/TCPâ€”these are the specific NodePorts we configured. Step 4: Access Your Applications Now open your browser and visit: http://localhost:8080 You should see: This is App 1 on localhost:8080 NodePort: 30080 http://localhost:3000 You should see: This is App 2 on localhost:3000 NodePort: 30081 Perfect! Each port shows different content. The traffic flow is: Browser hits localhost:8080 k3d forwards to NodePort 30080 on the cluster node NodePort 30080 routes to app1-service Service forwards to an app1 pod Pod serves the custom HTML from the ConfigMap And the same happens for localhost:3000 â†’ NodePort 30081 â†’ app2-service â†’ app2 pod. Why I Prefer This Approach For my local k3d setup, I found NodePort to be more straightforward: Each localhost port maps to a specific NodePort Each NodePort routes to a specific service No shared load balancer to cause confusion Simple and predictable for local development The trade-off is that you need to plan your port mappings when creating the cluster. If you want to add a third app later, youâ€™d need to recreate the cluster with an additional port mapping. But for local development, I found that to be a reasonable compromise. Quick Experiments Add a Third Application Letâ€™s add one more app to see how easy it is once you understand the pattern. First, recreate the cluster with an additional port mapping: k3d cluster delete learning k3d cluster create learning \\ --port \"8080:30080@server:0\" \\ --port \"3000:30081@server:0\" \\ --port \"8081:30082@server:0\" Output: INFO[0000] Deleting cluster 'learning' INFO[0001] Cluster 'learning' deleted successfully! INFO[0000] Prep: Network INFO[0000] Created network 'k3d-learning' INFO[0003] Cluster 'learning' created successfully! Create a ConfigMap for app3: kubectl create configmap app3-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 3 on localhost:8081&lt;/h1&gt;&lt;p&gt;NodePort: 30082&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' Output: configmap/app3-html created Create app3-nodeport.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: app3 spec: replicas: 1 selector: matchLabels: app: app3 template: metadata: labels: app: app3 spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: html mountPath: /usr/share/nginx/html volumes: - name: html configMap: name: app3-html --- apiVersion: v1 kind: Service metadata: name: app3-service spec: type: NodePort selector: app: app3 ports: - port: 80 targetPort: 80 nodePort: 30082 Apply it: kubectl apply -f app3-nodeport.yaml Output: deployment.apps/app3 created service/app3-service created Redeploy app1 and app2 (since we recreated the cluster): kubectl create configmap app1-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 1 on localhost:8080&lt;/h1&gt;&lt;p&gt;NodePort: 30080&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' kubectl create configmap app2-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 2 on localhost:3000&lt;/h1&gt;&lt;p&gt;NodePort: 30081&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' kubectl apply -f app1-nodeport.yaml kubectl apply -f app2-nodeport.yaml Output: configmap/app1-html created configmap/app2-html created deployment.apps/app1 created service/app1-service created deployment.apps/app2 created service/app2-service created Now you have three apps accessible at: http://localhost:8080 (app1) http://localhost:3000 (app2) http://localhost:8081 (app3) Check Service Details You can see all the NodePort mappings: kubectl get services Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app1-service NodePort 10.43.123.45 &lt;none&gt; 80:30080/TCP 2m app2-service NodePort 10.43.234.56 &lt;none&gt; 80:30081/TCP 2m app3-service NodePort 10.43.111.22 &lt;none&gt; 80:30082/TCP 1m kubernetes ClusterIP 10.43.0.1 &lt;none&gt; 443/TCP 5m You can see the NodePort for each service in the PORT(S) columnâ€”80:30080, 80:30081, and 80:30082. Test with curl Instead of using a browser, you can test with curl: curl http://localhost:8080 Output: &lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 1 on localhost:8080&lt;/h1&gt;&lt;p&gt;NodePort: 30080&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; curl http://localhost:3000 Output: &lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 2 on localhost:3000&lt;/h1&gt;&lt;p&gt;NodePort: 30081&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; curl http://localhost:8081 Output: &lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 3 on localhost:8081&lt;/h1&gt;&lt;p&gt;NodePort: 30082&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; Perfect! Each port serves different content. What Happens If You Skip the Port Mapping? Letâ€™s see what happens if you try to access a NodePort that wasnâ€™t mapped when creating the cluster. Create a service with NodePort 30083 (which we didnâ€™t map): kubectl create deployment app4 --image=nginx:alpine kubectl expose deployment app4 --type=NodePort --port=80 --name=app4-service --target-port=80 Output: deployment.apps/app4 created service/app4-service exposed Check what NodePort it got: kubectl get service app4-service Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app4-service NodePort 10.43.98.77 &lt;none&gt; 80:31847/TCP 10s It got assigned NodePort 31847 (a random port since we didnâ€™t specify one). Now try to access it: curl http://localhost:31847 Output: curl: (7) Failed to connect to localhost port 31847 after 0 ms: Couldn't connect to server It doesnâ€™t work because we didnâ€™t map that port when creating the cluster. You can only access NodePorts that you explicitly mapped during cluster creation. This is why planning your port mappings ahead of time is important with k3d. When to Use LoadBalancer vs NodePort Based on what Iâ€™ve learned so far, hereâ€™s when Iâ€™d use each: I use NodePort when: Iâ€™m running k3d locally for development I want multiple services accessible on different localhost ports I want a direct, simple mapping without extra configuration I want something that works immediately without Ingress setup Iâ€™d use LoadBalancer when: Iâ€™m ready to set up Ingress resources with proper routing rules (weâ€™ll cover this in a future post) Iâ€™m deploying to a real cloud environment (AWS, GCP, Azure) where LoadBalancer actually provisions external IPs I want a single entry point with host-based or path-based routing Iâ€™m building a production-ready setup with proper domain names For my local k3d development with multiple services, NodePort with port mapping has been working really well. Itâ€™s direct, predictable, and doesnâ€™t require any additional configuration beyond the cluster creation. But Iâ€™m sure there are other ways to handle this that I havenâ€™t discovered yet. Clean Up Delete all the deployments and services: kubectl delete deployment app1 app2 app3 app4 kubectl delete service app1-service app2-service app3-service app4-service kubectl delete configmap app1-html app2-html app3-html (Note: If youâ€™re cleaning up after the LoadBalancer example earlier, youâ€™ll only have app1, app2, and their ConfigMaps.) Output: deployment.apps \"app1\" deleted deployment.apps \"app2\" deleted deployment.apps \"app3\" deleted deployment.apps \"app4\" deleted service \"app1-service\" deleted service \"app2-service\" deleted service \"app3-service\" deleted service \"app4-service\" deleted configmap \"app1-html\" deleted configmap \"app2-html\" deleted configmap \"app3-html\" deleted Remove the YAML files: rm -f app1-deployment.yaml app2-deployment.yaml app1-nodeport.yaml app2-nodeport.yaml app3-nodeport.yaml If you want to keep the cluster for future experiments, leave it running. Otherwise, delete it: k3d cluster delete learning Output: INFO[0000] Deleting cluster 'learning' INFO[0001] Cluster 'learning' deleted successfully! What You Learned âœ… Port mapping in k3d isnâ€™t automaticâ€”you set it up when creating the cluster âœ… LoadBalancer in k3d requires Ingress resources to route traffic (weâ€™ll cover this in a future post) âœ… kubectl port-forward is different from LoadBalancerâ€”it creates a direct tunnel bypassing all cluster networking âœ… NodePort gives you direct port-to-service mappings without needing Ingress configuration âœ… You need to plan your port mappings ahead of time with k3d âœ… ConfigMaps can serve custom HTML content from volumes âœ… Multiple services can run on different localhost ports with the right setup What I found is that for local k3d development, NodePort with explicit port mappings gave me the control and predictability I needed. I know exactly which localhost port goes to which service, with no surprises and no extra configuration. Your mileage may vary depending on your setup and requirements. Whatâ€™s Next? In Part 5 of this series, weâ€™ll explore Persistent Storage in Kubernetes. Youâ€™ll learn how to: Use volumes to persist data beyond pod lifecycles Work with Persistent Volumes and Persistent Volume Claims Deploy stateful applications that need to keep data Stay tuned! Series Navigation Part 1: Deploy Your First App Part 2: ConfigMaps and Secrets Part 3: Understanding Namespaces Part 4: Understanding Port Mapping in k3d â† You just finished this! Part 5: Persistent Storage (Coming soon) Found a mistake or have questions? Feel free to open an issue here. " }, { "title": "Kubernetes Learning Path: Understanding Namespaces", "url": "/posts/kubernetes-learning-path-namespaces/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3d, namespaces, configmap", "date": "2025-10-20 10:00:00 +0500", "content": "Kubernetes Learning Path: Understanding Namespaces In the previous posts, weâ€™ve deployed apps and managed configuration. But what happens when you want to run multiple environments on the same cluster? Maybe youâ€™re running dev, staging, and prod all together, or different teams need to share the cluster without accidentally deleting each otherâ€™s stuff. Thatâ€™s where namespaces come in. Theyâ€™re basically Kubernetesâ€™s way of keeping things organized. What Are Namespaces? Think of namespaces like separate folders on your computer. You could put all your files in one big folder, but that gets messy fast. Instead, you organize them into different foldersâ€”work stuff here, personal stuff there, projects over there. Namespaces do the same thing for your Kubernetes resources. They give you: Isolation - Resources in one namespace donâ€™t interfere with resources in another Organization - Group related resources together (dev, staging, prod) Access Control - Different teams can have permissions to different namespaces Resource Limits - You can set quotas per namespace But hereâ€™s the thingâ€”namespaces arenâ€™t completely isolated. Pods in different namespaces can still talk to each other (unless you set up network policies). Think of namespaces more like organizational boundaries than security walls. See Your Current Namespaces Kubernetes creates a few namespaces by default: kubectl get namespaces Output: NAME STATUS AGE default Active 2d kube-node-lease Active 2d kube-public Active 2d kube-system Active 2d Hereâ€™s what theyâ€™re for: default - Where your resources go if you donâ€™t specify a namespace kube-system - System components like DNS and dashboard live here kube-public - Public resources readable by everyone (rarely used) kube-node-lease - Heartbeat information from nodes (you wonâ€™t need to touch this) When youâ€™ve been running kubectl get pods in the previous tutorials, youâ€™ve actually been looking at the default namespace this whole time. You just didnâ€™t know it. Creating Namespaces Letâ€™s create two namespaces for development and production environments: kubectl create namespace dev kubectl create namespace prod Output: namespace/dev created namespace/prod created Check your namespaces again: kubectl get namespaces Output: NAME STATUS AGE default Active 2d dev Active 5s kube-node-lease Active 2d kube-public Active 2d kube-system Active 2d prod Active 5s Your new namespaces are ready to use. Real Example: Deploy App to Multiple Namespaces Letâ€™s deploy a simple app to both dev and prod namespaces. Weâ€™ll use different ConfigMaps for each environment to show how namespaces keep things separate. Step 1: Create ConfigMaps for Each Environment First, create a ConfigMap for dev with development-specific settings: kubectl create configmap app-config \\ --from-literal=ENVIRONMENT=development \\ --from-literal=DEBUG_MODE=true \\ --from-literal=API_URL=http://dev-api.example.com \\ --namespace=dev Output: configmap/app-config created Now create a different ConfigMap for prod with production settings: kubectl create configmap app-config \\ --from-literal=ENVIRONMENT=production \\ --from-literal=DEBUG_MODE=false \\ --from-literal=API_URL=http://api.example.com \\ --namespace=prod Output: configmap/app-config created Notice we used the same name app-config in both namespaces. You might think this would cause a conflict, but it doesnâ€™tâ€”resources with the same name can exist in different namespaces without any issues. Theyâ€™re completely separate. View the dev ConfigMap: kubectl get configmap app-config -n dev -o yaml Output: apiVersion: v1 data: API_URL: http://dev-api.example.com DEBUG_MODE: \"true\" ENVIRONMENT: development kind: ConfigMap metadata: name: app-config namespace: dev The -n flag is shorthand for --namespace. Youâ€™ll use it a lot. Step 2: Create the Application Deployment Create a file called app-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: webapp spec: replicas: 2 selector: matchLabels: app: webapp template: metadata: labels: app: webapp spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 env: # Environment variables pulled from ConfigMap # Each env variable references a key from the ConfigMap in the SAME namespace - name: ENVIRONMENT # Name of the environment variable inside the container valueFrom: configMapKeyRef: name: app-config # ConfigMap name (must exist in the same namespace) key: ENVIRONMENT # Key from the ConfigMap to get the value from - name: DEBUG_MODE # Another environment variable in the container valueFrom: configMapKeyRef: name: app-config # Same ConfigMap as above key: DEBUG_MODE # Different key from the same ConfigMap - name: API_URL # Third environment variable valueFrom: configMapKeyRef: name: app-config # Still the same ConfigMap key: API_URL # Another key from the ConfigMap This is a simple nginx deployment that pulls configuration from a ConfigMap. The cool part? We can use this exact same YAML file in both namespaces, and each deployment will automatically grab its own namespace-specific ConfigMap. Same file, different configs. Deploy to dev: kubectl apply -f app-deployment.yaml --namespace=dev Output: deployment.apps/webapp created Deploy to prod (using the same file): kubectl apply -f app-deployment.yaml --namespace=prod Output: deployment.apps/webapp created Step 3: Verify the Deployments Check pods in the dev namespace: kubectl get pods -n dev Output: NAME READY STATUS RESTARTS AGE webapp-7d8c9f5b4-2x9k7 1/1 Running 0 15s webapp-7d8c9f5b4-5m2p9 1/1 Running 0 15s Check pods in the prod namespace: kubectl get pods -n prod Output: NAME READY STATUS RESTARTS AGE webapp-7d8c9f5b4-6n3q8 1/1 Running 0 10s webapp-7d8c9f5b4-8p4r2 1/1 Running 0 10s Same deployment name, same pod namesâ€”but theyâ€™re in different namespaces, so thereâ€™s no conflict. Step 4: Verify Different Configurations Letâ€™s check that each deployment is actually using its own ConfigMap. Check the dev environment variables: kubectl exec -n dev deployment/webapp -- sh -c 'echo \"ENV: $ENVIRONMENT\" &amp;&amp; echo \"DEBUG: $DEBUG_MODE\" &amp;&amp; echo \"API: $API_URL\"' Output: ENV: development DEBUG: true API: http://dev-api.example.com Now check prod: kubectl exec -n prod deployment/webapp -- sh -c 'echo \"ENV: $ENVIRONMENT\" &amp;&amp; echo \"DEBUG: $DEBUG_MODE\" &amp;&amp; echo \"API: $API_URL\"' Output: ENV: production DEBUG: false API: http://api.example.com There we go! Each deployment is using its own ConfigMap values. The dev environment has debug mode enabled and points to a dev API, while prod has debug mode disabled and points to the production API. Same deployment file, totally different behavior. Working with Namespaces View Resources in a Specific Namespace # Get all pods in dev namespace kubectl get pods -n dev # Get all resources in prod namespace kubectl get all -n prod Output: NAME READY STATUS RESTARTS AGE pod/webapp-7d8c9f5b4-6n3q8 1/1 Running 0 5m pod/webapp-7d8c9f5b4-8p4r2 1/1 Running 0 5m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/webapp 2/2 2 2 5m View Resources Across All Namespaces kubectl get pods --all-namespaces Or use the shorthand: kubectl get pods -A Output: NAMESPACE NAME READY STATUS RESTARTS AGE dev webapp-7d8c9f5b4-2x9k7 1/1 Running 0 10m dev webapp-7d8c9f5b4-5m2p9 1/1 Running 0 10m prod webapp-7d8c9f5b4-6n3q8 1/1 Running 0 8m prod webapp-7d8c9f5b4-8p4r2 1/1 Running 0 8m kube-system coredns-59b4f5bbd5-8xk2p 1/1 Running 0 2d Set a Default Namespace Typing -n dev or -n prod every single time gets annoying real quick. You can set a default namespace for your current context so you donâ€™t have to: kubectl config set-context --current --namespace=dev Output: Context \"k3d-learning\" modified. Now all your commands will default to the dev namespace: kubectl get pods Output (without needing -n dev): NAME READY STATUS RESTARTS AGE webapp-7d8c9f5b4-2x9k7 1/1 Running 0 15m webapp-7d8c9f5b4-5m2p9 1/1 Running 0 15m Switch back to default when youâ€™re done: kubectl config set-context --current --namespace=default Describe a Namespace kubectl describe namespace dev Output: Name: dev Labels: kubernetes.io/metadata.name=dev Annotations: &lt;none&gt; Status: Active No resource quota. No LimitRange resource. This shows you any resource quotas or limits applied to the namespace (none in our case). Namespace Communication Hereâ€™s something that caught me by surpriseâ€”pods in different namespaces can still talk to each other by default. Namespaces donâ€™t block network traffic, they just change how DNS works. Letâ€™s say you have a service called api-service in the prod namespace. Pods can reach it using: api-service - Only works from within the same namespace api-service.prod - Works from any namespace api-service.prod.svc.cluster.local - Full DNS name (also works from anywhere) So namespaces arenâ€™t security wallsâ€”theyâ€™re more like organizational folders. If you need actual network isolation, youâ€™d have to set up Network Policies, but thatâ€™s beyond the scope of this post. Practical Tips Specify Namespace in YAML Files Instead of using -n flags all the time, you can specify the namespace in your YAML files: apiVersion: v1 kind: ConfigMap metadata: name: app-config namespace: dev # &lt;-- Specify namespace here data: ENVIRONMENT: development This is clearer and less error-prone, especially when managing multiple resources. Use Namespaces for Environments A common pattern is to use namespaces for different environments on the same cluster: dev namespace for development staging namespace for staging prod namespace for production But rememberâ€”if you have the resources, separate clusters for production are often better. Namespaces provide logical separation, not physical isolation. Donâ€™t Overdo It Iâ€™ve seen people create a namespace for every little thing, and honestly, it just makes life harder. Start simple: One or two namespaces for different environments Maybe separate namespaces for completely different applications Donâ€™t create namespaces just because you can If you find yourself constantly switching between 10+ namespaces, youâ€™ve probably gone too far. Keep it simple. Clean Up Delete the deployments from both namespaces: kubectl delete deployment webapp -n dev kubectl delete deployment webapp -n prod Output: deployment.apps \"webapp\" deleted deployment.apps \"webapp\" deleted Delete the ConfigMaps: kubectl delete configmap app-config -n dev kubectl delete configmap app-config -n prod Output: configmap \"app-config\" deleted configmap \"app-config\" deleted Delete the namespaces themselves (this also deletes everything inside them): kubectl delete namespace dev kubectl delete namespace prod Output: namespace \"dev\" deleted namespace \"prod\" deleted When you delete a namespace, Kubernetes automatically deletes everything inside itâ€”all pods, deployments, services, everything. Super handy for cleanup, but also scary if you accidentally delete the wrong namespace. Double-check before you hit enter on that delete command. What You Learned âœ… What namespaces are and why theyâ€™re useful âœ… How to create and manage namespaces âœ… How to deploy the same application to different namespaces âœ… How to use namespace-specific ConfigMaps âœ… How to work with resources across namespaces Thatâ€™s pretty much it for namespaces. Theyâ€™re not complicatedâ€”just think of them as folders for organizing your cluster. Once you start using them, youâ€™ll wonder how you managed without them. Whatâ€™s Next? In Part 4 of this series, weâ€™ll explore Persistent Storage in Kubernetes. Youâ€™ll learn how to: Use volumes to persist data beyond pod lifecycles Work with Persistent Volumes and Persistent Volume Claims Deploy stateful applications like databases Stay tuned! Series Navigation Part 1: Deploy Your First App Part 2: ConfigMaps and Secrets Part 3: Understanding Namespaces â† You just finished this! Part 4: Persistent Storage (Coming soon) Part 5: Ingress and Load Balancing (Coming soon) Found a mistake or have questions? Feel free to open an issue here. " }, { "title": "Kubernetes Learning Path: ConfigMaps and Secrets", "url": "/posts/kubernetes-learning-path-configmaps-and-secrets/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3d, configmap, secrets", "date": "2025-10-19 10:00:00 +0500", "content": "Kubernetes Learning Path: ConfigMaps and Secrets In the previous post, we deployed our first application to Kubernetes. But hereâ€™s the thingâ€”real applications need configuration. Database URLs, API keys, feature flags, and other settings that change between environments. You definitely donâ€™t want to hardcode these values into your container images. That would mean rebuilding your entire image just to change a database URL or update an API key. Thatâ€™s where ConfigMaps and Secrets come in. What Are ConfigMaps and Secrets? Think of ConfigMaps and Secrets like configuration files that live inside Kubernetes. Instead of baking configuration into your container images, you store it separately and inject it into your pods at runtime. ConfigMaps are for non-sensitive configuration dataâ€”things like application settings, feature flags, or database names. Secrets are for sensitive dataâ€”like passwords, API keys, or certificates. Theyâ€™re similar to ConfigMaps but have some extra protection (though theyâ€™re still just base64-encoded, not encrypted by default). Letâ€™s see how to use them with some practical examples. Working with ConfigMaps Create a ConfigMap from Literal Values The simplest way to create a ConfigMap is from literal values on the command line: kubectl create configmap app-config \\ --from-literal=APP_ENV=production \\ --from-literal=LOG_LEVEL=info Output: configmap/app-config created Check what you just created: kubectl get configmap app-config -o yaml Output: apiVersion: v1 data: APP_ENV: production LOG_LEVEL: info kind: ConfigMap metadata: name: app-config namespace: default See how your key-value pairs are stored? Now letâ€™s create one from a file. Create a ConfigMap from a File First, create a simple config file: cat &gt; app.properties &lt;&lt; EOF database.name=myapp database.pool.size=20 cache.enabled=true EOF Now create a ConfigMap from this file: kubectl create configmap app-config-file --from-file=app.properties Output: configmap/app-config-file created View it: kubectl describe configmap app-config-file Output: Name: app-config-file Namespace: default Labels: &lt;none&gt; Annotations: &lt;none&gt; Data ==== app.properties: ---- database.name=myapp database.pool.size=20 cache.enabled=true The entire file content is stored as a single key called app.properties. Use ConfigMap as Environment Variables Now letâ€™s use our ConfigMap in a pod. Create a file called pod-with-configmap.yaml: apiVersion: v1 kind: Pod metadata: name: config-demo-pod spec: containers: - name: demo image: alpine:latest command: [\"sh\", \"-c\", \"echo APP_ENV=$APP_ENV &amp;&amp; echo LOG_LEVEL=$LOG_LEVEL &amp;&amp; sleep 3600\"] env: - name: APP_ENV # Environment variable name inside the container valueFrom: configMapKeyRef: name: app-config # References the ConfigMap we created earlier key: APP_ENV # Pulls the value of \"APP_ENV\" key from that ConfigMap - name: LOG_LEVEL valueFrom: configMapKeyRef: name: app-config # Same ConfigMap as above key: LOG_LEVEL # Pulls the value of \"LOG_LEVEL\" key Apply it: kubectl apply -f pod-with-configmap.yaml Output: pod/config-demo-pod created Check the logs to see if the environment variables were injected: kubectl logs config-demo-pod Output: APP_ENV=production LOG_LEVEL=info Great! The values from your ConfigMap are now available as environment variables inside the container. Use ConfigMap as a Volume Sometimes you want configuration files, not just environment variables. You can mount a ConfigMap as a volume. Create pod-with-config-volume.yaml: apiVersion: v1 kind: Pod metadata: name: config-volume-pod spec: containers: - name: demo image: alpine:latest command: [\"sh\", \"-c\", \"cat /config/app.properties &amp;&amp; sleep 3600\"] volumeMounts: - name: config-volume # References the volume defined below mountPath: /config # Where to mount the files inside the container volumes: - name: config-volume # Volume name (must match volumeMounts above) configMap: name: app-config-file # References the ConfigMap created from app.properties file Apply it: kubectl apply -f pod-with-config-volume.yaml Output: pod/config-volume-pod created Check the logs: kubectl logs config-volume-pod Output: database.name=myapp database.pool.size=20 cache.enabled=true The file from your ConfigMap is now mounted inside the container at /config/app.properties. This is handy when your app expects actual config files instead of environment variables. Working with Secrets Secrets work almost the same way as ConfigMaps, but theyâ€™re meant for sensitive data. Create a Secret from Literal Values kubectl create secret generic db-secret \\ --from-literal=username=admin \\ --from-literal=password=super-secret-password Output: secret/db-secret created View it: kubectl get secret db-secret -o yaml Output: apiVersion: v1 data: password: c3VwZXItc2VjcmV0LXBhc3N3b3Jk username: YWRtaW4= kind: Secret metadata: name: db-secret namespace: default type: Opaque Notice the values are base64-encoded. Thatâ€™s not encryptionâ€”itâ€™s just encoding. Anyone with access to your cluster can decode them: echo \"YWRtaW4=\" | base64 -d Output: admin So Secrets arenâ€™t super secure by default, but theyâ€™re better than ConfigMaps because Kubernetes handles them differentlyâ€”theyâ€™re not written to disk on nodes unless needed, and you can enable encryption at rest if you want. Create a Secret from a File Create a file with sensitive data: echo \"my-super-secret-api-key\" &gt; api-key.txt Create a Secret from it: kubectl create secret generic api-secret --from-file=api-key.txt Output: secret/api-secret created Use Secret as Environment Variables Create pod-with-secret.yaml: apiVersion: v1 kind: Pod metadata: name: secret-demo-pod spec: containers: - name: demo image: alpine:latest command: [\"sh\", \"-c\", \"echo DB_USER=$DB_USER &amp;&amp; echo DB_PASS=$DB_PASS &amp;&amp; sleep 3600\"] env: - name: DB_USER # Environment variable name inside the container valueFrom: secretKeyRef: name: db-secret # References the Secret we created earlier key: username # Pulls the value of \"username\" key from that Secret - name: DB_PASS valueFrom: secretKeyRef: name: db-secret # Same Secret as above key: password # Pulls the value of \"password\" key Apply it: kubectl apply -f pod-with-secret.yaml Output: pod/secret-demo-pod created Check the logs: kubectl logs secret-demo-pod Output: DB_USER=admin DB_PASS=super-secret-password The secret values are automatically decoded and injected as plain text into your environment variables. Use Secret as a Volume Just like ConfigMaps, you can mount Secrets as volumes. Create pod-with-secret-volume.yaml: apiVersion: v1 kind: Pod metadata: name: secret-volume-pod spec: containers: - name: demo image: alpine:latest command: [\"sh\", \"-c\", \"cat /secrets/api-key.txt &amp;&amp; sleep 3600\"] volumeMounts: - name: secret-volume # References the volume defined below mountPath: /secrets # Where to mount the secret files inside the container readOnly: true # Good security practice for secrets volumes: - name: secret-volume # Volume name (must match volumeMounts above) secret: secretName: api-secret # References the Secret created from api-key.txt file Apply it: kubectl apply -f pod-with-secret-volume.yaml Output: pod/secret-volume-pod created Check the logs: kubectl logs secret-volume-pod Output: my-super-secret-api-key The secret file is mounted and readable inside the container. Notice we set readOnly: trueâ€”thatâ€™s a good practice for security. Practical Example: nginx with Custom Config Letâ€™s put it all together. Weâ€™ll deploy nginx with a custom config file from a ConfigMap, and also inject both ConfigMap and Secret values as environment variables. This shows how you can use multiple ConfigMaps and Secrets in a single deployment. First, create a simple custom nginx config file: cat &gt; default.conf &lt;&lt; EOF server { listen 80; location / { return 200 'Hello from Kubernetes!\\nThis config came from a ConfigMap.\\n'; add_header Content-Type text/plain; } } EOF Create a ConfigMap from it: kubectl create configmap nginx-config --from-file=default.conf Output: configmap/nginx-config created Now create a deployment that uses both ConfigMap and Secret. Create nginx-deployment-with-config.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-with-config spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 env: # Environment variable from ConfigMap - name: APP_ENV # Variable name in the container valueFrom: configMapKeyRef: name: app-config # References ConfigMap created with --from-literal key: APP_ENV # Gets the \"APP_ENV\" key value # Environment variable from Secret - name: API_KEY # Variable name in the container valueFrom: secretKeyRef: name: api-secret # References Secret created from api-key.txt key: api-key.txt # Gets the file content as the value volumeMounts: - name: nginx-config # Must match volume name below mountPath: /etc/nginx/conf.d/default.conf # Exact file path in container subPath: default.conf # Mounts only this file, not entire directory volumes: - name: nginx-config # Volume name referenced above configMap: name: nginx-config # References ConfigMap created from default.conf Notice the subPath: default.confâ€”this mounts just the single file instead of the entire directory, which prevents nginx from having issues with its default config. Apply it: kubectl apply -f nginx-deployment-with-config.yaml Output: deployment.apps/nginx-with-config created Wait for the pod to be ready: kubectl get pods -l app=nginx Output: NAME READY STATUS RESTARTS AGE nginx-with-config-7d8c9f5b4-x9k2p 1/1 Running 0 15s Test it with port-forwarding: kubectl port-forward deployment/nginx-with-config 8080:80 In another terminal: curl http://localhost:8080 Output: Hello from Kubernetes! This config came from a ConfigMap. Perfect! Now letâ€™s verify the environment variables are available inside the container: kubectl exec deployment/nginx-with-config -- sh -c 'echo \"APP_ENV=$APP_ENV\" &amp;&amp; echo \"API_KEY=$API_KEY\"' Output: APP_ENV=production API_KEY=my-super-secret-api-key Your nginx is now running with configuration from a ConfigMap and has access to both the ConfigMap and Secret values through environment variables. Quick Experiments Update a ConfigMap You can update a ConfigMap and see how it affects running pods. Note that environment variables wonâ€™t update automaticallyâ€”you need to restart the pod. But volume-mounted configs can update automatically (though it might take a minute). kubectl edit configmap app-config This opens an editor where you can change the values. Change APP_ENV from production to staging and save. For the changes to take effect in pods using environment variables, you need to restart them: kubectl rollout restart deployment/nginx-with-config Output: deployment.apps/nginx-with-config restarted View Secret Values You already saw that secrets are just base64-encoded. Letâ€™s decode one: kubectl get secret db-secret -o jsonpath='{.data.password}' | base64 -d Output: super-secret-password This is why you still need to protect access to your Kubernetes clusterâ€”secrets arenâ€™t truly encrypted without additional setup. Clean Up Delete all the resources we created: kubectl delete pod config-demo-pod config-volume-pod secret-demo-pod secret-volume-pod kubectl delete deployment nginx-with-config kubectl delete configmap app-config app-config-file nginx-config kubectl delete secret db-secret api-secret rm -f app.properties api-key.txt default.conf Output: pod \"config-demo-pod\" deleted pod \"config-volume-pod\" deleted pod \"secret-demo-pod\" deleted pod \"secret-volume-pod\" deleted deployment.apps \"nginx-with-config\" deleted configmap \"app-config\" deleted configmap \"app-config-file\" deleted configmap \"nginx-config\" deleted secret \"db-secret\" deleted secret \"api-secret\" deleted What You Learned âœ… How to create ConfigMaps from literal values and files âœ… How to inject ConfigMaps as environment variables and volumes âœ… How to create and use Secrets for sensitive data âœ… How to mount Secrets in pods âœ… The difference between ConfigMaps and Secrets Thatâ€™s it! You can now keep configuration separate from your container images, which makes your apps way more flexible when moving between environments. Whatâ€™s Next? In Part 3 of this series, weâ€™ll explore Understanding Namespaces in Kubernetes. Youâ€™ll learn how to: Organize resources using namespaces Deploy apps to different environments Work with namespace-specific configurations Stay tuned! Series Navigation Part 1: Deploy Your First App Part 2: ConfigMaps and Secrets â† You just finished this! Part 3: Understanding Namespaces (Coming soon) Part 4: Persistent Storage (Coming soon) Part 5: Ingress and Load Balancing (Coming soon) Found a mistake or have questions? Feel free to open an issue here. " }, { "title": "Kubernetes Learning Path: Deploy Your First App", "url": "/posts/kubernetes-learning-path-deploy-your-first-app/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3d, deployment, docker", "date": "2025-10-17 16:00:00 +0500", "content": "Kubernetes Learning Path: Deploy Your First App I have been wanting to learn Kubernetes for a while now, and I came across a tool called k3d. It is a great tool that makes it easy to run Kubernetes clusters locally inside Docker containers, making it perfect for learning and development without needing complex infrastructure setup. In this post, Iâ€™ll walk you through deploying your first application to Kubernetes using k3d. Weâ€™ll start from scratch and work our way up to a running nginx server. Quick Setup: Install k3d First, make sure you have Docker running on your machine. Then install k3d: # macOS (using Homebrew) brew install k3d # Or using curl curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash Verify the installation: k3d version Create Your First Cluster A cluster is a group of machines (nodes) working together to run your applicationsâ€”think of it like a team of servers that can share the workload. Creating a cluster is simple: # Create a cluster named \"learning\" k3d cluster create learning Output: INFO[0000] Prep: Network INFO[0000] Created network 'k3d-learning' INFO[0000] Created image volume k3d-learning-images INFO[0000] Starting new tools node... INFO[0001] Creating node 'k3d-learning-server-0' INFO[0001] Pulling image 'ghcr.io/k3d-io/k3d-tools:5.6.0' INFO[0002] Creating LoadBalancer 'k3d-learning-serverlb' INFO[0003] Cluster 'learning' created successfully! INFO[0003] You can now use it like this: kubectl cluster-info This takes just a few seconds! Now check that your cluster is running: k3d cluster list Output: NAME SERVERS AGENTS LOADBALANCER learning 1/1 0/0 true Check the cluster nodes: kubectl get nodes Output: NAME STATUS ROLES AGE VERSION k3d-learning-server-0 Ready control-plane,master 30s v1.27.4+k3s1 You should see one node in â€œReadyâ€ state. Now youâ€™re ready to deploy. Deploy Your First App So youâ€™ve got a local Kubernetes cluster running with k3d? Great! Letâ€™s deploy something to it. Weâ€™ll use nginx as our first applicationâ€”itâ€™s simple, reliable, and perfect for understanding how Kubernetes deployments work. I know that in real life youâ€™ll be deploying much more complex applications than just a static nginx server, but we all have to start somewhere. Once you understand the basics with nginx, the same patterns apply to any containerized application. Weâ€™ll work our way up to more complex deployments in future posts. What Youâ€™ll Deploy Weâ€™re going to create two things: A Deployment that runs 2 nginx containers A Service that makes nginx accessible from your browser Step 1: Create the Deployment Create a file called nginx-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-demo labels: app: nginx # &lt;--- Label for the Deployment itself (for organizing Deployments) spec: replicas: 2 selector: matchLabels: app: nginx # &lt;--- Deployment manages pods with this label template: metadata: labels: app: nginx # &lt;--- These labels will be stamped on each pod spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 What does this do? Creates 2 identical nginx pods (containers running nginx) Uses the lightweight nginx:alpine image Exposes port 80 in each container Understanding the Three Label Sections: When I first saw three different places with app: nginx, I was confused about why we needed so many labels. Hereâ€™s what each one does: metadata.labels - Labels for the Deployment resource itself (helps you organize Deployments with commands like kubectl get deployments -l app=nginx) spec.selector.matchLabels - Tells the Deployment which pods it should manage template.metadata.labels - Labels that get stamped on each pod created by this Deployment The selector and template labels must match, otherwise the Deployment wonâ€™t know which pods to manage. Weâ€™ll use these pod labels to connect the Service in Step 2. Deploy it: kubectl apply -f nginx-deployment.yaml # Watch your pods start kubectl get pods -w First, youâ€™ll see your 2 pods being created: NAME READY STATUS RESTARTS AGE nginx-demo-7d4c9d8c9b-4xk2p 0/1 ContainerCreating 0 2s nginx-demo-7d4c9d8c9b-7m8qz 0/1 ContainerCreating 0 2s The 0/1 means 0 out of 1 containers are ready yetâ€”theyâ€™re still starting up. Then, after a few seconds, both pods will be running: NAME READY STATUS RESTARTS AGE nginx-demo-7d4c9d8c9b-4xk2p 1/1 Running 0 5s nginx-demo-7d4c9d8c9b-7m8qz 1/1 Running 0 5s Now 1/1 shows both pods are fully ready and running. The -w flag watches for changes in real-time (similar to tail -f), so press Ctrl+C to stop watching and return to your command prompt. Step 2: Expose with a Service Your pods are running, but you canâ€™t access them yet. So why do we need a Service anyway? Why Services are Required Think of it like a hotel receptionist. Guests donâ€™t need to know which room each staff member is inâ€”they just ask at the front desk, and the receptionist routes them to whoever can help. In Kubernetes: Each pod gets its own IP address that changes when it restarts We have 2 nginx pods running, and more could be added or removed The Service gives you one stable address and automatically routes traffic to healthy pods When pods restart or scale up/down, the Service updates its routing automatically Without a Service, youâ€™d have to track pod IPs manually every time something changes. Create nginx-service.yaml: apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: LoadBalancer selector: app: nginx # &lt;--- This selector matches pods with label \"app: nginx\" ports: - port: 80 targetPort: 80 How the Service finds your Pods See the selector: app: nginx? The Service looks for all pods with the label app: nginx (the same labels we set in Step 1) and automatically sends traffic to them. When you add more pods with this label, the Service finds them. When you remove pods, the Service stops sending traffic to them. Itâ€™s all based on matching labels. Apply it: kubectl apply -f nginx-service.yaml # Check the service kubectl get service nginx-service You should see: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-service LoadBalancer 10.43.123.456 &lt;pending&gt; 80:32000/TCP 5s The service is now routing traffic to your pods. The EXTERNAL-IP shows &lt;pending&gt; in k3dâ€”donâ€™t worry about it, weâ€™ll use port-forwarding to access the app. Step 3: Access Your Application Use port-forwarding to access nginx: kubectl port-forward service/nginx-service 8080:80 Open your browser and visit http://localhost:8080. You should see the nginx welcome page. Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. Your application is now running on Kubernetes and accessible from your browser. Quick Experiments Scale Your Application # Scale to 5 pods kubectl scale deployment nginx-demo --replicas=5 Output: deployment.apps/nginx-demo scaled Now watch the new pods appear: kubectl get pods -w Youâ€™ll see 3 additional pods being created: NAME READY STATUS RESTARTS AGE nginx-demo-7d4c9d8c9b-4xk2p 1/1 Running 0 5m nginx-demo-7d4c9d8c9b-7m8qz 1/1 Running 0 5m nginx-demo-7d4c9d8c9b-9n2kx 0/1 ContainerCreating 0 2s nginx-demo-7d4c9d8c9b-6p4mw 0/1 ContainerCreating 0 2s nginx-demo-7d4c9d8c9b-8r5tn 0/1 ContainerCreating 0 2s Scale back to 2: kubectl scale deployment nginx-demo --replicas=2 Kubernetes will automatically terminate the extra pods. Check the Logs # Get pod name kubectl get pods Output: NAME READY STATUS RESTARTS AGE nginx-demo-7d4c9d8c9b-4xk2p 1/1 Running 0 10m nginx-demo-7d4c9d8c9b-7m8qz 1/1 Running 0 10m Now view logs from one of the pods: kubectl logs nginx-demo-7d4c9d8c9b-4xk2p Output (nginx access logs): /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Configuration complete; ready for start up 127.0.0.1 - - [17/Oct/2025:10:30:15 +0000] \"GET / HTTP/1.1\" 200 615 \"-\" \"Mozilla/5.0\" These are the nginx startup messages and any HTTP requests. Your logs might be different. Execute Commands # Check nginx version (replace with your actual pod name) kubectl exec nginx-demo-7d4c9d8c9b-4xk2p -- nginx -v Output: nginx version: nginx/1.25.3 Open a shell inside the pod: kubectl exec -it nginx-demo-7d4c9d8c9b-4xk2p -- /bin/sh Youâ€™ll get an interactive shell prompt: / # pwd / / # ls bin dev etc home lib media mnt opt proc root run sbin srv sys tmp usr var / # exit You can run any commands inside the container. Type exit to leave the shell. Clean Up When youâ€™re done experimenting, itâ€™s good practice to clean up the resources. This also helps you practice the delete commands: kubectl delete -f nginx-service.yaml kubectl delete -f nginx-deployment.yaml Output: service \"nginx-service\" deleted deployment.apps \"nginx-demo\" deleted Verify everything is gone: kubectl get all Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.43.0.1 &lt;none&gt; 443/TCP 30m Your nginx resources are gone! The only thing remaining is the default kubernetes service, which is a system service thatâ€™s always present. What You Learned âœ… How to create a Kubernetes Deployment âœ… How to expose an app with a Service âœ… How to scale applications up and down âœ… How to inspect pods (check logs and execute commands) Youâ€™ve successfully deployed your first application to Kubernetes. The same pattern works for any containerized applicationâ€”just swap the nginx image for your own. Whatâ€™s Next? In Part 2 of this series, weâ€™ll explore ConfigMaps and Secrets to manage configuration and sensitive data in your Kubernetes applications. Youâ€™ll learn how to: Store configuration separately from your code Manage environment variables Handle sensitive information securely Stay tuned! Series Navigation Part 1: Deploy Your First App â† You just finished this! Part 2: ConfigMaps and Secrets Part 3: Understanding Namespaces (Coming soon) Part 4: Persistent Storage (Coming soon) Part 5: Ingress and Load Balancing (Coming soon) Found a mistake or have questions? Feel free to open an issue here. " }, { "title": "Deploying Ruby on Rails on Ubuntu", "url": "/posts/deploy-ruby-on-rails-on-ubuntu/", "categories": "Ubuntu, Deployment", "tags": "ruby-on-rails", "date": "2022-07-12 13:54:50 +0500", "content": "Deploying Ruby on Rails on Ubuntu I always wanted to learn how the deployment and server provisioning is done. While setting up a homelab using Raspbery Pi 4, which uses an ARM based architecture and many deployment tools do not support that yet so I thought this is perfect oppertuity to finally learn. This is my first ever home lab setup: I use 4, Raspbery Pi 4 with 4Gig of RAM. The case I am using is from UCTRONICS. For operating system I am using Ubuntu server 22.04. While configuring the Raspberry Pi to host my Ruby on Rails sites, these are the steps I had to go through. Secure the remote server with SSH Install the copy utility Make sure ssh-copy-id is installed on Mac, it does not come in mac by default. brew install ssh-copy-id Copy command The following command will prompt for password ssh-copy-id -i ~/.ssh/id_rsa.pub user@remote-host-ip You should now be able to ssh into remote server without needing to provide your password. Disable password based login Inside the sshd_config file set the PasswordAuthentication to no sudo vim /etc/ssh/sshd_config Restart the SSH service sudo systemctl restart ssh Installing Ruby &amp; Rails dependencies Node.js Node version manager sudo apt install curl curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | bash Run the following command so nvm command is available source ~/.profile Install node version Install the LTS version nvm install --lts Install Yarn sudo apt install yarn Install Redis server sudo apt install redis-server sudo apt install redis-tools ### Make sure it is up and running sudo systemctl status redis-server Dependencies for compiiling Ruby along with Node.js and Yarn sudo apt install git-core curl zlib1g-dev build-essential libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 libxml2-dev libxslt1-dev libcurl4-openssl-dev software-properties-common libffi-dev dirmngr gnupg apt-transport-https ca-certificates Install Ruby rbenv Run each command separately git clone https://github.com/rbenv/rbenv.git ~/.rbenv echo 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' &gt;&gt; ~/.bashrc echo 'eval \"$(rbenv init -)\"' &gt;&gt; ~/.bashrc git clone https://github.com/rbenv/ruby-build.git ~/.rbenv/plugins/ruby-build echo 'export PATH=\"$HOME/.rbenv/plugins/ruby-build/bin:$PATH\"' &gt;&gt; ~/.bashrc git clone https://github.com/rbenv/rbenv-vars.git ~/.rbenv/plugins/rbenv-vars exec $SHELL Ruby ubuntu@ubuntu:~$ rbenv install 3.1.2 ubuntu@ubuntu:~$ rbenv global 3.1.2 ubuntu@ubuntu:~$ ruby -v Bundler gem install bundler Puma sudo apt -y install puma Nginx sudo apt -y install nginx Nginx site Remove defailt site sudo rm /etc/nginx/sites-enabled/default Enable your site sudo vim /etc/nginx/sites-enabled/app-name Add site configuration upstream puma_your_app_name { server unix:///home/ubuntu/freeapi/shared/tmp/sockets/puma.sock; } server { listen 80 default_server deferred; server_name freeapi.com www.freeapi.com; # Don't forget to update these, too root /home/ubuntu/your_app_directory/current/public; access_log /home/ubuntu/your_app_directory/current/log/nginx.access.log; error_log /home/ubuntu/your_app_directory/current/log/nginx.error.log info; location ^~ /assets/ { gzip_static on; expires max; add_header Cache-Control public; } try_files $uri/index.html $uri @puma_your_app_name; location @puma_your_app_name { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_redirect off; proxy_pass http://puma_your_app_name; } error_page 500 502 503 504 /500.html; client_max_body_size 10M; keepalive_timeout 10; } Restart Nginx service sudo service nginx restart If the service fails to start check the logs to see what the error is. The status command will also provide helpful information systemctl status nginx.service. At this stage the error might be related to nginx log files not being available, thats because we did not deploy the app yet. If you want you can comment the access_log and error_log line in the above confirutaion, restart the nginx service and then check if it is running successfully. Install PostgreSQL sudo apt install postgresql postgresql-contrib libpq-dev If you see a lock error then simply reboot and run the above command again. Create the PostgreSQL user and database sudo su - postgres createuser --pwprompt ubuntu createdb -O ubuntu your-app-db exit Deployment Capistrano Add the following gems to the Gemfile and run the command bundle install gem 'capistrano', require: false gem 'capistrano-rails', require: false gem 'capistrano-puma', require: false gem 'capistrano-rbenv', require: false gem 'capistrano-bundler', require: false Generate the deployment files cap install STAGES=production This will generate the following files Capfile config/deploy.rb config/deploy/production.rb and the following directory lib/capistrano/tasks Add the following in Capfile require \"capistrano/rails\" require \"capistrano/rbenv\" require \"capistrano/bundler\" require \"capistrano/rails/assets\" require \"capistrano/rails/migrations\" require \"capistrano/puma\" install_plugin Capistrano::Puma install_plugin Capistrano::Puma::Systemd set :rbenv_type, :user set :rbenv_ruby, '3.1.2' Add the following inside config/deploy.rb set :application, \"your-app-name\" set :repo_url, \"git@github.com:username/myapp.git\" set :deploy_to, \"/home/ubuntu/#{fetch :application}\" append :linked_dirs, 'log', 'tmp/pids', 'tmp/cache', 'tmp/sockets', 'vendor/bundle', '.bundle', 'public/system', 'public/uploads' Add the following inside config/deploy/production set :branch, \"main\" server \"your-ip-server\", user: \"ubuntu\", roles: %w[web app db] set :ssh_options, { keys: %w(/Users/your-user/.ssh/id_ed25519.pub), forward_agent: true, auth_methods: %w(publickey) } Make sure the branch you want to deploy is already pushed on GitHub. Setup env vars SSH into the remote server mkdir /home/ubuntu/your-app nano /home/ubuntu/your-app/.rbenv-vars Add the DB and Master key env vars # For Postgres DATABASE_URL=postgresql://user:PASSWORD@127.0.0.1/myapp RAILS_MASTER_KEY=xyz123 RACK_ENV=production RAILS_ENV=production Upload puma config cap production puma:config Upload puma service cap production puma:systemd:config puma:systemd:enable Finally deploy the app using the following command cap production deploy --trace Troubleshooting Removing the repository from Ubuntu This guide is for Ubuntu 22.04, if you end up adding the wrong repo which is not compatiable for example sudo add-apt-repository ppa:chris-lea/redis-server just remove it by using the --remove flag sudo add-apt-repository --remove ppa:chris-lea/redis-server Puma If you see the following error on deployment NameError: uninitialized constant Capistrano::Puma install_plugin Capistrano::Puma ^^^^^^ /Users/shairyar/Sites/fakeapi/Capfile:44:in `&lt;top (required)&gt;' Make sure you are using the gem capistrano3-puma and not capistrano-puma. GitHub permission issue Just make sure the new secure ssh key id_ed25519.pub is added to remote server and inside config/deploy/production key forward_agent is set to true set :ssh_options, { keys: %w(/Users/shairyar/.ssh/id_ed25519.pub), forward_agent: true, auth_methods: %w(publickey) } Yarn install issue Running the following command shold fix the problem sudo apt-get remove cmdinstall;sudo apt update;sudo apt-get install yarn Contribute Found a mistake? Or if you have any questions, please feel free to open an issue here. Found something that can be improved? Feel free to create a PR here. " } ]
