[ { "title": "Kubernetes Learning Path: Deploying Rails 8 with SolidQueue on Raspberry Pi k3s", "url": "/posts/kubernetes-learning-path-rails-solidqueue-on-k3s/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3s, raspberry-pi, ruby-on-rails, solidqueue, postgresql, deployment", "date": "2025-10-31 14:00:00 +0500", "content": "Kubernetes Learning Path: Deploying Rails 8 with SolidQueue on Raspberry Pi k3s After setting up my 3-node Raspberry Pi k3s cluster in the previous post, I wanted to deploy something real—not just nginx demos, but an actual production-grade application. So I decided to deploy a Ruby on Rails 8 application with all the modern bells and whistles: PostgreSQL, background jobs with SolidQueue, database-backed caching with SolidCache, and proper health checks. This wasn’t a weekend afternoon project. It took me a few days of trial and error, reading documentation, debugging architecture mismatches, and learning a lot about how Rails 8’s new features work in a Kubernetes environment. But once everything clicked into place and I saw my Rails app running across multiple pods, processing background jobs on separate workers, all on my little Pi cluster—it felt incredible. Let me walk you through what I built, what went wrong, and what I learned. Why Rails 8? I’ll be honest—I chose Rails because I’m already familiar with it. The goal here was to deploy a complete, production-grade application stack on Kubernetes, not to learn a new framework at the same time. Trying to learn Kubernetes and a new language/framework simultaneously would have been overwhelming. Rails was the obvious choice because I know how it works, I understand its conventions, and I can focus on the Kubernetes side of things without getting lost in framework documentation. The Pleasant Surprise: Rails 8’s Built-in Features What I didn’t expect was how well Rails 8 fits into Kubernetes. It comes with SolidQueue and SolidCache built-in—database-backed solutions for background jobs and caching. Before Rails 8, you’d typically need Redis for Sidekiq (background jobs) and Redis or Memcached for caching. With Rails 8, everything uses PostgreSQL: Application data → PostgreSQL Background job queue → PostgreSQL (via SolidQueue) Cache storage → PostgreSQL (via SolidCache) The Architecture We’re Building Here’s what I deployed in this phase: ┌─────────────────────────────────────────────┐ │ k3s Cluster (3 Raspberry Pis) │ │ │ │ ┌────────────┐ ┌────────────┐ │ │ │ Rails Web │ │ Rails Web │ (Pods) │ │ │ Pod x3 │ │ Pod x3 │ │ │ └─────┬──────┘ └─────┬──────┘ │ │ │ │ │ │ └────────┬───────┘ │ │ │ │ │ ┌────────▼────────┐ │ │ │ Rails Service │ │ │ │ (ClusterIP) │ │ │ │ Internal Only │ │ │ └────────┬────────┘ │ │ │ │ │ ┌──────────────▼──────────────┐ │ │ │ SolidQueue Workers (x2) │ │ │ │ (Background Job Processors) │ │ │ └──────────────┬──────────────┘ │ │ │ │ │ ┌────────▼────────┐ │ │ │ PostgreSQL 17 │ │ │ │ (StatefulSet) │ │ │ │ + Persistent │ │ │ │ Storage │ │ │ └─────────────────┘ │ │ │ └─────────────────────────────────────────────┘ Access via: kubectl port-forward (for now) What I deployed: 3 Rails web server pods (Deployment) 2 SolidQueue worker pods (Deployment) 1 PostgreSQL pod (StatefulSet with 5GB persistent storage) 2 Services: rails-service (ClusterIP) and postgres (Headless) What’s next (future posts): Traefik Ingress for external access Persistent volumes for file uploads Monitoring with Mission Control Jobs SSL/TLS with cert-manager What You’ll Need Before starting, make sure you have: A working k3s cluster on Raspberry Pi (from Part 5) Docker installed on your laptop for building images A Docker Hub account (free tier works fine) kubectl configured to access your cluster Basic understanding of Rails (helpful but not required) The Big Challenge: Cross-Architecture Builds This was my first major roadblock. I develop on an x86_64 laptop (Intel/AMD architecture), but Raspberry Pi uses ARM64 architecture. If you build a Docker image on your laptop without specifying the platform, it won’t run on the Pi. The symptom? Pods that immediately fail with cryptic errors like exec format error or exec /bin/sh: exec format error. Docker Hub Setup (One-Time) Before building and pushing images, you need a Docker Hub account and to log in: # Create a free account at https://hub.docker.com if you don't have one # Free tier includes unlimited public repos # Login to Docker Hub (do this once on each machine you build from) docker login # Enter your Docker Hub username and password Once logged in, you can push images to your Docker Hub account. Solution 1: Build on the Raspberry Pi directly I found out this to be the most reliable method: # SSH to your master node ssh pi@192.168.18.49 # If you haven't logged in on the Pi yet: docker login # Build the image directly on ARM64 hardware docker build -t your-dockerhub-username/my-rails-app:v1 . docker push your-dockerhub-username/my-rails-app:v1 Replace your-dockerhub-username with your actual Docker Hub username. Building on the Pi takes about 15-20 minutes because, well, it’s a Raspberry Pi. But you’ll get a native ARM64 image that works perfectly. Solution 2: Cross-compile on your laptop with QEMU You can also cross-compile on your laptop if you don’t want to SSH to the Pi. This requires QEMU emulation: # On Arch Linux (my setup) sudo pacman -S qemu-user-static qemu-user-static-binfmt sudo systemctl start systemd-binfmt.service # Verify QEMU is working ls /proc/sys/fs/binfmt_misc/ | grep qemu # Should show: qemu-aarch64, qemu-arm, etc. # Now build for ARM64 from your laptop docker buildx build --platform linux/arm64 \\ -t your-dockerhub-username/my-rails-app:v1 \\ --push . Note: In my testing, building directly on the Pi was actually faster than cross-compiling with QEMU emulation. QEMU adds overhead, and the Pi’s native ARM64 build was more efficient. The cross-compile option is mainly useful if you can’t SSH to the Pi or want to automate builds from your laptop. Lesson learned: Always specify --platform linux/arm64 when building for Raspberry Pi. I wasted several hours debugging “exec format error” before realizing my x86_64 images wouldn’t run on ARM64. Step 1: Prepare the Rails Application I created a Rails 8.1.1 application with all the modern defaults. If you want to follow along, here’s the quick setup: # Create new Rails 8 app rails new my-rails-app --database=postgresql # Rails 8 automatically includes: # ✅ Solid Queue (background jobs) # ✅ Solid Cache (caching) # ✅ Dockerfile (production-ready, multi-stage) # ✅ Health check endpoint (/up) The generated Dockerfile is already optimized for production. Rails 8 does a great job here. Important Configuration Changes Rails 8 production mode assumes SSL by default, which caused CSRF errors when I tried to access the app via kubectl port-forward (which uses HTTP, not HTTPS). Make SSL configurable in config/environments/production.rb: # Allow disabling SSL for testing (keep enabled in real production) config.assume_ssl = ENV.fetch(\"RAILS_ASSUME_SSL\", \"true\") == \"true\" config.force_ssl = ENV.fetch(\"RAILS_FORCE_SSL\", \"true\") == \"true\" Fix database connection in config/database.yml: Rails defaults to Unix socket connections for PostgreSQL, but in Kubernetes we need TCP connections: production: &lt;&lt;: *default database: myapp_production username: &lt;%= ENV.fetch(\"DATABASE_USERNAME\") { \"rails_user\" } %&gt; password: &lt;%= ENV[\"DATABASE_PASSWORD\"] %&gt; host: &lt;%= ENV.fetch(\"DATABASE_HOST\") { \"localhost\" } %&gt; port: &lt;%= ENV.fetch(\"DATABASE_PORT\") { \"5432\" } %&gt; The key here is host: reading from DATABASE_HOST environment variable, which will point to our PostgreSQL service. Step 2: Build and Push the Docker Image I built the image on my Raspberry Pi master node for guaranteed compatibility: # SSH to the Pi ssh pi@192.168.18.49 # Clone your app or copy it over # Then build: cd my-rails-app docker build -t your-username/my-rails-app:v1 . docker push your-username/my-rails-app:v1 Building took about 18 minutes on my Pi 4. Grab some coffee. Important: This single image is reused for: Rails web server pods (default CMD) SolidQueue worker pods (override CMD) Database migration init container (runs before web pods start) One image, multiple purposes. That’s the beauty of the Rails 8 Dockerfile. Step 3: Deploy PostgreSQL to Kubernetes Before Rails can start, we need the database running. I created four YAML files for PostgreSQL deployment. PostgreSQL Secret First, we need to store database credentials securely. Why we need this: PostgreSQL needs a username, password, and database name to initialize. Kubernetes Secrets store this sensitive data (base64-encoded, can be encrypted at rest). Where it’s used: The PostgreSQL StatefulSet reads these values as environment variables to create the database and user. postgres-secret.yaml: apiVersion: v1 kind: Secret metadata: name: postgres-secret type: Opaque stringData: POSTGRES_USER: rails_user POSTGRES_PASSWORD: change_this_password_123 POSTGRES_DB: myapp_production ⚠️ Important: Change that password! Never use default passwords in production. Apply it: kubectl apply -f postgres-secret.yaml Persistent Storage Databases need storage that survives pod restarts. Why we need this: Without persistent storage, all database data would be lost when the PostgreSQL pod restarts or crashes. PersistentVolumeClaims (PVCs) request storage from Kubernetes that persists independently of pods. Where it’s used: The PostgreSQL StatefulSet mounts this volume to /var/lib/postgresql/data where PostgreSQL stores its data files. postgres-pvc.yaml: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: postgres-pvc spec: accessModes: - ReadWriteOnce # Single node can mount for read/write resources: requests: storage: 5Gi # Request 5GB of storage Apply it: kubectl apply -f postgres-pvc.yaml # Wait for it to bind kubectl get pvc -w # Press Ctrl+C when STATUS shows \"Bound\" k3s automatically provisions local storage on one of your Pi nodes. PostgreSQL StatefulSet Now for the actual database pod. Why we need this: We use a StatefulSet (not a Deployment) because databases need: Stable pod names (always postgres-0, not random names) Ordered startup and shutdown Stable persistent storage that follows the pod Where it’s used: This creates the PostgreSQL pod that Rails and SolidQueue will connect to for all database operations. postgres-statefulset.yaml: apiVersion: apps/v1 kind: StatefulSet metadata: name: postgres spec: serviceName: postgres # ← Links to postgres-service below replicas: 1 selector: matchLabels: app: postgres # ← StatefulSet manages pods with this label template: metadata: labels: app: postgres # ← Each pod gets this label # postgres-service selector matches this to route traffic spec: containers: - name: postgres image: postgres:17-alpine ports: - containerPort: 5432 # ← Service targetPort forwards here envFrom: - secretRef: name: postgres-secret # ← Injects POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB volumeMounts: - name: postgres-storage mountPath: /var/lib/postgresql/data subPath: postgres volumes: - name: postgres-storage persistentVolumeClaim: claimName: postgres-pvc # ← Mounts storage from postgres-pvc.yaml above Apply it: kubectl apply -f postgres-statefulset.yaml # Watch PostgreSQL start kubectl get pods -l app=postgres -w # Wait for STATUS: Running, READY: 1/1 PostgreSQL Service Now let’s create a stable DNS name for the database. Why we need this: Pod IP addresses change when they restart. A Service gives us a stable DNS name (postgres) that Rails can use to connect. Even if the PostgreSQL pod restarts and gets a new IP, the service name stays the same. Where it’s used: Rails’ database.yml will use host: postgres to connect. Kubernetes DNS automatically resolves this to the PostgreSQL pod’s IP. postgres-service.yaml: apiVersion: v1 kind: Service metadata: name: postgres # ← DNS name: pods can connect via \"postgres:5432\" spec: selector: app: postgres # ← Finds pods with label \"app: postgres\" # (matches labels in postgres-statefulset.yaml above) ports: - port: 5432 # ← Service listens on this port targetPort: 5432 # ← Forwards to pod's containerPort 5432 clusterIP: None # ← Headless service for StatefulSet Apply it: kubectl apply -f postgres-service.yaml Now any pod can connect to PostgreSQL using the hostname postgres on port 5432. This is what we configured in database.yml. Verify PostgreSQL is Running # Check pod status kubectl get pods -l app=postgres # Test connection kubectl exec -it postgres-0 -- psql -U rails_user -d myapp_production -c \"SELECT version();\" You should see PostgreSQL version info. Database is ready! Step 4: Configure Rails for Kubernetes Rails needs configuration and secrets to run in Kubernetes. I split these into ConfigMaps (non-sensitive config) and Secrets (sensitive stuff like passwords). Rails ConfigMap Why we need this: Rails needs various environment variables to run (database host, Rails environment, logging settings, etc.). ConfigMaps store non-sensitive configuration that can be shared across all Rails and worker pods. Where it’s used: Both the Rails web deployment and SolidQueue worker deployment will inject these environment variables into their pods. rails-configmap.yaml: apiVersion: v1 kind: ConfigMap metadata: name: rails-config # ← Referenced by rails-deployment and solid-queue-deployment data: RAILS_ENV: \"production\" RAILS_LOG_TO_STDOUT: \"true\" DATABASE_HOST: \"postgres\" # ← Kubernetes DNS resolves \"postgres\" to postgres-service # which routes to postgres pods DATABASE_PORT: \"5432\" RAILS_ASSUME_SSL: \"false\" # For testing without SSL RAILS_FORCE_SSL: \"false\" SOLID_QUEUE_DISPATCHERS_POLLING_INTERVAL: \"1\" SOLID_QUEUE_WORKERS_POLLING_INTERVAL: \"0.1\" Rails Secret Why we need this: Rails requires sensitive data like the secret key (for encrypting sessions/cookies) and database credentials. Unlike ConfigMaps, Secrets are base64-encoded and can be encrypted at rest for better security. Where it’s used: The Rails web pods and SolidQueue workers need these secrets to connect to the database and encrypt user sessions. First, generate a secret key. You can do this from your laptop or from the Pi—doesn’t matter: # Option 1: Using Rails (run this on your laptop) docker run --platform linux/arm64 --rm your-username/my-rails-app:v1 ./bin/rails secret # Option 2: Using OpenSSL (simpler - run this on your laptop) openssl rand -hex 64 You’ll get a long random string like: f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9 Copy this output—you’ll paste it into the YAML file in the next step. Now create the secret file: rails-secret.yaml: apiVersion: v1 kind: Secret metadata: name: rails-secret # ← Referenced by rails-deployment and solid-queue-deployment type: Opaque stringData: SECRET_KEY_BASE: \"your-generated-secret-key-paste-here\" DATABASE_PASSWORD: \"change_this_password_123\" # ← Must match postgres-secret.yaml DATABASE_USERNAME: \"rails_user\" DATABASE_NAME: \"myapp_production\" ⚠️ Critical: DATABASE_PASSWORD must exactly match what you set in postgres-secret.yaml! Apply both: kubectl apply -f rails-configmap.yaml kubectl apply -f rails-secret.yaml Step 5: Deploy Rails Web Application Now for the main event—deploying Rails itself. Rails Deployment with Init Container This is where it gets interesting. The init container runs database migrations before the main Rails container starts. Why we need this: This is the core web application. The Deployment creates 3 Rails pods running Puma (the web server) to handle HTTP requests. We use a Deployment (not StatefulSet) because web servers are stateless—any pod can handle any request. The init container trick: Before the web server starts, an init container runs database migrations. This ensures the database schema is up-to-date before Rails starts serving requests. Where it’s used: These pods handle all HTTP requests to your Rails application. The Service (next step) will load-balance traffic across all 3 pods. rails-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: rails-app spec: replicas: 3 # Three web server pods selector: matchLabels: app: rails # ← Deployment manages pods with this label template: metadata: labels: app: rails # ← Each pod gets this label # rails-service.yaml selector \"app: rails\" finds these pods spec: initContainers: - name: db-setup image: your-username/my-rails-app:v1 command: - sh - -c - | bundle exec rails db:create || echo \"Databases may exist\" bundle exec rails db:prepare bundle exec rails solid_queue:install:migrations bundle exec rails solid_cache:install:migrations bundle exec rails db:migrate envFrom: - configMapRef: name: rails-config # ← Injects DATABASE_HOST=\"postgres\" etc. - secretRef: name: rails-secret # ← Injects DATABASE_PASSWORD, SECRET_KEY_BASE containers: - name: web image: your-username/my-rails-app:v1 ports: - containerPort: 3000 # ← rails-service targetPort forwards here envFrom: - configMapRef: name: rails-config # ← Rails reads DATABASE_HOST to connect to postgres - secretRef: name: rails-secret # ← Rails reads DATABASE_PASSWORD to authenticate livenessProbe: httpGet: path: /up port: 3000 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /up port: 3000 initialDelaySeconds: 10 periodSeconds: 5 resources: requests: memory: \"256Mi\" cpu: \"100m\" limits: memory: \"512Mi\" cpu: \"500m\" What’s happening here: Init container runs first: creates databases, installs Solid Queue/Cache migrations, runs migrations Only after init succeeds, main container starts: Rails web server (Puma) Health checks ensure Rails is responding before routing traffic Resource limits prevent any pod from hogging the Pi’s limited RAM Apply it: kubectl apply -f rails-deployment.yaml # Watch pods start (this takes longer due to init container) kubectl get pods -l app=rails -w You’ll see pods in Init:0/1 status while migrations run. Check init logs: # Get pod name kubectl get pods -l app=rails # Check init container logs kubectl logs &lt;pod-name&gt; -c db-setup You should see migration output. Once init completes, pods transition to Running status. A Nasty Bug I Hit: Multi-Database Setup Rails 8 uses separate databases for cache and queue by default: myapp_production (primary) myapp_production_cache (cache) myapp_production_queue (queue) myapp_production_cable (action cable) PostgreSQL only auto-creates myapp_production. The init container failed with “Database myapp_production_cache does not exist”. Solution: Add db:create before db:prepare in the init container: bundle exec rails db:create || echo \"Databases may exist\" bundle exec rails db:prepare The || echo prevents failure if databases already exist. Took me an hour to figure this out. Rails Service Time to create a load balancer for the web pods. Why we need this: Just like with PostgreSQL, we need a stable way to access the Rails pods. This Service gives us a single DNS name (rails-service) and automatically distributes incoming requests across all healthy Rails pods. Where it’s used: We’ll use kubectl port-forward service/rails-service to access the app from our laptop. Later, an Ingress controller would route external traffic to this service. rails-service.yaml: apiVersion: v1 kind: Service metadata: name: rails-service # ← DNS name for accessing Rails internally spec: type: ClusterIP # ← Internal only (use Ingress for external access) selector: app: rails # ← Finds pods with label \"app: rails\" # (matches labels in rails-deployment.yaml) ports: - port: 80 # ← Service listens on port 80 targetPort: 3000 # ← Forwards to pod's containerPort 3000 (Puma) Apply it: kubectl apply -f rails-service.yaml # Check endpoints (should list all 3 Rails pod IPs) kubectl get endpoints rails-service The service automatically distributes traffic across your 3 Rails pods. Step 6: Deploy SolidQueue Workers Note: This is where I stopped in my initial deployment. Steps 7-8 below cover testing and troubleshooting what I’ve deployed so far. Future work like Ingress and persistent storage for uploads will be covered in upcoming posts. Background jobs need dedicated worker processes. Why we need this: Background jobs (sending emails, processing uploads, etc.) shouldn’t run on the web server pods—they’d compete for resources and slow down HTTP responses. SolidQueue workers are dedicated pods that poll the database for jobs and process them. The clever part: We use the exact same Docker image (your-username/my-rails-app:v1) for both web servers and workers. The only difference is what command we tell the container to run: Web pods: Run the default command from the Dockerfile → Puma web server starts (rails server) Worker pods: Override the command in the deployment YAML → SolidQueue starts (bundle exec rake solid_queue:start) This means you only need to build and maintain one Docker image for your entire Rails stack. Same codebase, different processes. Where it’s used: These workers continuously poll the solid_queue_jobs table in PostgreSQL, pick up new jobs, process them, and mark them complete. solid-queue-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: solid-queue-worker spec: replicas: 2 selector: matchLabels: app: solid-queue-worker # ← Deployment manages pods with this label template: metadata: labels: app: solid-queue-worker # ← Each worker pod gets this label spec: containers: - name: worker image: your-username/my-rails-app:v1 # ← Same image as rails-deployment command: # ← Different command: runs SolidQueue instead of web server - bundle - exec - rake - solid_queue:start envFrom: - configMapRef: name: rails-config # ← Workers use same DATABASE_HOST=\"postgres\" to connect - secretRef: name: rails-secret # ← Workers use same DATABASE_PASSWORD to authenticate resources: requests: memory: \"256Mi\" cpu: \"100m\" limits: memory: \"512Mi\" cpu: \"500m\" Apply it: kubectl apply -f solid-queue-deployment.yaml # Watch workers start kubectl get pods -l app=solid-queue-worker -w Check worker logs: kubectl logs -f deployment/solid-queue-worker You should see SolidQueue starting up and polling for jobs. Understanding the Command Difference Now that you’ve deployed both the Rails web servers and SolidQueue workers, let’s see exactly how Kubernetes runs different commands from the same Docker image. In the Rails deployment (rails-deployment.yaml), we don’t specify a command: field, so Kubernetes uses the default from the Dockerfile: containers: - name: web image: your-username/my-rails-app:v1 # No command: specified = uses Dockerfile default (Puma web server) In the worker deployment (solid-queue-deployment.yaml), we override the command: containers: - name: worker image: your-username/my-rails-app:v1 # Same image! command: # This overrides the Dockerfile default - bundle - exec - rake - solid_queue:start This is a powerful Kubernetes pattern: the command: field in your deployment YAML can override whatever CMD or ENTRYPOINT is defined in the Dockerfile. One image, multiple uses! Step 7: Test the Full Stack Time to see if everything works together. Access Rails Application Use port-forwarding to access the app: kubectl port-forward service/rails-service 3000:80 Open http://localhost:3000 in your browser. You should see your Rails app! Test Background Jobs Open a Rails console: kubectl exec -it deployment/rails-app -- rails console In the console, queue a test job: # Create a simple job class if you don't have one class TestJob &lt; ApplicationJob def perform(message) puts \"Processing: #{message}\" end end # Queue the job TestJob.perform_later(\"Hello from Kubernetes!\") # Check job count SolidQueue::Job.count # =&gt; 1 # Exit console exit Now check the worker logs: kubectl logs -f deployment/solid-queue-worker You should see the worker picking up and processing the job! If you see “Processing: Hello from Kubernetes!” in the logs, your entire stack is working. Step 8: Challenges I Hit and How I Solved Them This deployment wasn’t without its struggles. Here are the main issues I encountered and how I fixed them: 1. Cross-Architecture Exec Format Error Problem: Built image on x86_64 laptop, pods failed with “exec /bin/sh: exec format error” on ARM64 Pi. Solution: Build with --platform linux/arm64 on laptop with QEMU, or build directly on Pi. Lesson: Always specify target platform for cross-compilation. 2. PostgreSQL Socket Connection Error Problem: Init container failed: “connection to socket ‘/var/run/postgresql/.s.PGSQL.5432’ failed” Solution: Rails was trying Unix socket. Fixed by ensuring database.yml reads DATABASE_HOST from env var (TCP connection). Lesson: Kubernetes requires TCP connections between pods, not Unix sockets. 3. Multi-Database Creation Problem: Init failed: “Database myapp_production_cache does not exist” Solution: Added rails db:create before db:prepare in init container to create all databases. Lesson: Rails 8 multi-database setup requires explicit database creation. 4. CSRF Errors with Port-Forward Problem: Got “HTTP Origin header didn’t match request.base_url” when accessing via port-forward. Solution: Made force_ssl and assume_ssl configurable via env vars, disabled for local testing. Lesson: Production Rails expects HTTPS. For local testing without SSL, make it configurable. 5. Image Caching Issues Problem: Pushed updated image but pods still ran old cached version. Solution: Used new tag (v2, v3) instead of reusing v1. Kubernetes caches by tag. Lesson: Use unique tags for each build, or set imagePullPolicy: Always. Monitoring Your Application Check Pod Status # All pods kubectl get pods # Just Rails pods kubectl get pods -l app=rails # Just workers kubectl get pods -l app=solid-queue-worker # Detailed pod info kubectl describe pod &lt;pod-name&gt; View Logs # Rails web logs kubectl logs -f deployment/rails-app # Worker logs kubectl logs -f deployment/solid-queue-worker # PostgreSQL logs kubectl logs postgres-0 # Logs from specific pod kubectl logs &lt;pod-name&gt; # Previous crashed container logs kubectl logs &lt;pod-name&gt; --previous Rails Console Access # Open console in a web pod kubectl exec -it deployment/rails-app -- rails console # Check database connection &gt; ActiveRecord::Base.connection.execute('SELECT 1') # Check job queue &gt; SolidQueue::Job.count &gt; SolidQueue::Job.pending.count &gt; SolidQueue::Job.failed.count # Exit &gt; exit What You Learned ✅ How to deploy a production Rails 8 application on Kubernetes ✅ StatefulSets vs Deployments (databases vs stateless apps) ✅ Persistent storage with PersistentVolumeClaims ✅ Init containers for database migrations ✅ ConfigMaps and Secrets for configuration management ✅ Services for load balancing and service discovery ✅ Health checks with liveness and readiness probes ✅ Cross-architecture Docker builds (x86_64 → ARM64) ✅ SolidQueue for background jobs in Kubernetes ✅ Resource limits and requests on Raspberry Pi Conclusion Deploying Rails 8 with SolidQueue on my Raspberry Pi k3s cluster was one of the most educational projects I’ve done. It’s one thing to deploy nginx demos; it’s another to deploy a full production application stack with database, background jobs, and health checks—all running on $35 computers in a distributed cluster. The challenges were real: architecture mismatches, database connection errors, multi-database setup, SSL configuration. But each problem taught me something valuable about how Kubernetes works and how modern Rails is designed to run in containerized environments. What really impressed me was how well Rails 8 works in Kubernetes. The SolidQueue and SolidCache features reduce infrastructure complexity significantly—no Redis to manage means fewer moving parts, simpler networking, and lower resource usage on the Pi cluster. And honestly? Seeing background jobs process on dedicated worker pods while the web server handles HTTP requests, all automatically distributed by Kubernetes services, with health checks ensuring everything stays healthy—that’s pretty cool. This is just the beginning. Right now I’m using kubectl port-forward to access the app, which isn’t practical for long-term use. In upcoming posts, I’ll be adding Ingress for proper external access, persistent storage for file uploads, monitoring with Mission Control Jobs, and possibly SSL/TLS with cert-manager. But the foundation is solid, and I have a working Rails cluster to build on. What’s Next? I have a working Rails cluster, but there’s more to do. In future posts, I’ll be exploring: Traefik Ingress for proper external HTTP access (no more port-forward!) Persistent storage for uploads using NFS or Longhorn Mission Control Jobs for monitoring background jobs via web UI SSL/TLS with cert-manager and Let’s Encrypt Monitoring with Prometheus and Grafana on the Pi cluster CI/CD pipelines with GitHub Actions deploying to k3s Database backups and disaster recovery The foundation is solid. Now it’s time to make it production-ready. Stay tuned! Series Navigation Part 1: Deploy Your First App Part 2: ConfigMaps and Secrets Part 3: Understanding Namespaces Part 4: Understanding Port Mapping in k3d Part 5: Setting Up k3s on Raspberry Pi Part 6: Deploying Rails 8 with SolidQueue on k3s ← You just finished this! Part 7: Persistent Storage (Coming soon) Found a mistake or have questions? Feel free to open an issue here. " }, { "title": "Kubernetes Learning Path: Setting Up k3s on Raspberry Pi", "url": "/posts/kubernetes-learning-path-k3s-on-raspberry-pi/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3s, raspberry-pi, deployment", "date": "2025-10-29 10:00:00 +0500", "content": "Kubernetes Learning Path: Setting Up k3s on Raspberry Pi After going through the first four parts of this series with k3d, I felt ready to take things to the next level. k3d is great for learning and local development, but there’s something different about running Kubernetes on actual hardware. You deal with real networking, real resource constraints, and real high availability scenarios that you just can’t simulate properly on a single machine. So I decided to set up a 3-node Kubernetes cluster on Raspberry Pi devices using k3s. This would give me a production-like environment at home where I could practice everything I learned with k3d, but on real distributed hardware. I’ll be honest—this took me a couple of hours and involved some head-scratching moments. But once everything clicked into place and I saw all three nodes showing “Ready” status, it felt amazing. Let me walk you through what I did, what went wrong, and what I learned. Why Raspberry Pi and k3s? Before we dive in, you might be wondering why I chose this setup. I went with Raspberry Pi because they’re affordable, don’t cost much to run 24/7, and honestly it’s just fun to build things with them. Plus, dealing with ARM architecture and limited resources teaches you a lot about how Kubernetes manages resources—you can’t just throw CPU and RAM at problems like you might on a beefy server. As for k3s, it’s basically Kubernetes but lightweight enough to actually run on a Pi without choking it. The best part? It has the same API as full Kubernetes, so everything you learn transfers directly. I wasn’t interested in running some toy version of Kubernetes—I wanted the real thing, just smaller. What You’ll Need Here’s what I used for my setup: Hardware: 3× Raspberry Pi 4 (I’d recommend 4GB+ RAM each) 3× SSD drives (You can try using an SD card, but I did not want to use it) Ethernet cables for all Pis A network switch or router with enough ports Power supplies for each Pi Software: Ubuntu Server 24.04 LTS on all devices SSH access to all Pis I went with Ubuntu Server because I’m already familiar with it. You could probably use Raspberry Pi OS too, but I stuck with what I know. The Big Picture Before we start, here’s what we’re building: My Network (192.168.18.0/24) │ ├── k3s-master (192.168.18.49) [Control Plane] ├── k3s-worker-1 (192.168.18.51) [Worker Node] └── k3s-worker-2 (192.168.18.52) [Worker Node] The master node runs the Kubernetes control plane (API server, scheduler, etc.), and the worker nodes run your actual application workloads. All three nodes are connected via ethernet for stability. Step 1: Network Setup—The Foundation That Matters This was the most important step, and I learned it the hard way. If your master node’s IP address changes after you’ve set up the cluster, the whole thing breaks. Worker nodes connect to the master using its IP, and that IP is hardcoded during the join process. So the very first thing you need to do is set up static IP addresses on all your Raspberry Pis. Configure Static IPs with Netplan On each Raspberry Pi, I configured a static IP using netplan. Here’s what I did on the master node: sudo nano /etc/netplan/50-cloud-init.yaml I replaced the contents with this configuration: network: version: 2 renderer: networkd ethernets: eth0: dhcp4: no addresses: - 192.168.18.49/24 # Master node IP routes: - to: 0.0.0.0/0 via: 192.168.18.1 # Your router IP metric: 100 nameservers: addresses: - 8.8.8.8 - 1.1.1.1 Important things to note: Replace 192.168.18.49 with your desired static IP Replace 192.168.18.1 with your router’s IP (usually 192.168.1.1 or 192.168.0.1) Use different IPs for each node (.49 for master, .51 for worker-1, .52 for worker-2) Apply the configuration: sudo netplan apply Verify the IP address: ip addr show eth0 You should see your static IP address listed. If you don’t have network access after applying, double-check that your router IP and subnet are correct. Pro tip: I initially had both WiFi and ethernet active on my Pis, which caused routing conflicts. I disabled WiFi completely and used only ethernet. Much more stable. Set Up Hostname Resolution To make life easier, I set up /etc/hosts on all nodes so I could use hostnames instead of IP addresses: sudo nano /etc/hosts Add these lines on all three nodes: 192.168.18.49 k3s-master 192.168.18.51 k3s-worker-1 192.168.18.52 k3s-worker-2 Now you can ping by hostname: ping k3s-master This makes troubleshooting and managing the cluster much easier. Step 2: Preparing All Nodes Before installing k3s, each Raspberry Pi needs some preparation. I had to do these steps on all three nodes. System Updates Always start with updates: sudo apt update &amp;&amp; sudo apt upgrade -y sudo apt install -y curl vim git This took about 10-15 minutes per Pi on my internet connection. Set Hostnames On each Pi, set the appropriate hostname: # On master node: sudo hostnamectl set-hostname k3s-master # On worker-1: sudo hostnamectl set-hostname k3s-worker-1 # On worker-2: sudo hostnamectl set-hostname k3s-worker-2 Verify: hostname Enable Legacy iptables This step confused me at first. Modern Ubuntu uses something called nftables by default, but k3s needs the older iptables-legacy for its networking to work. I didn’t really understand why until I forgot to do it on one node and spent an hour debugging why pods couldn’t talk to each other. Run this on all nodes: sudo update-alternatives --set iptables /usr/sbin/iptables-legacy sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy Lesson learned: don’t skip this step. Enable cgroup Memory Kubernetes needs cgroup support for resource management (CPU and memory limits). This requires editing the boot parameters: sudo nano /boot/firmware/cmdline.txt You’ll see a long line of parameters. Add these to the end of the existing line (don’t create a new line): cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory The entire line might look something like this: console=serial0,115200 console=tty1 root=PARTUUID=12345678-02 rootfstype=ext4 elevator=deadline rootwait cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory Warning: These parameters must be on the same line as the existing boot parameters. If you put them on a new line, they won’t work. Save the file and reboot: sudo reboot After the reboot, verify cgroup memory is enabled: cat /proc/cgroups | grep memory You should see output showing memory cgroup support. Step 3: Installing k3s on the Master Node Okay, now for the fun part—actually installing k3s. After all that prep work, the installation itself is almost anticlimactic. On the master node: curl -sfL https://get.k3s.io | sh - That’s it. The script downloads and installs k3s automatically. It took about 2-3 minutes on my Pi. Verify the Master Node Check that k3s is running: sudo systemctl status k3s You should see output showing k3s is “active (running)”. Check the node status: sudo k3s kubectl get nodes Output: NAME STATUS ROLES AGE VERSION k3s-master Ready control-plane,master 30s v1.27.7+k3s1 Seeing “Ready” status means the master node is operational. Get the Join Token Worker nodes need a token to join the cluster. Get it from the master: sudo cat /var/lib/rancher/k3s/server/node-token You’ll see a long token like: K107f8a9b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0z::server:a1b2c3d4e5f6g7h8i9j0 Copy this token—you’ll need it in the next step. I saved it to a text file for easy reference. Set Up kubectl Access By default, you need sudo to run kubectl commands. Let’s fix that: mkdir ~/.kube sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config sudo chown $USER:$USER ~/.kube/config chmod 600 ~/.kube/config Now you can run kubectl without sudo: kubectl get nodes Much better. Access the Cluster from Your Laptop Honestly, SSHing into the master node every time you want to run a kubectl command gets old fast. Let’s set it up so you can manage the cluster directly from your laptop. First, make sure you have kubectl installed on your local machine. If you don’t: # macOS brew install kubectl # Linux curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Now, copy the kubeconfig from the master node to your laptop. On your local machine, run: # Create .kube directory if it doesn't exist mkdir -p ~/.kube # Copy the config from master node scp user@192.168.18.49:~/.kube/config ~/.kube/k3s-config Replace user with your actual username on the Pi, and adjust the IP if needed. Here’s the important part: the config file references 127.0.0.1 (localhost), which works when you’re on the master node but not from your laptop. You need to change it to the master node’s actual IP. Edit the file: nano ~/.kube/k3s-config Find this line: server: https://127.0.0.1:6443 Change it to: server: https://192.168.18.49:6443 Save and exit. Now tell kubectl to use this config: export KUBECONFIG=~/.kube/k3s-config Test it: kubectl get nodes Output: NAME STATUS ROLES AGE VERSION k3s-master Ready control-plane,master 10m v1.27.7+k3s1 It works! You’re now managing the cluster from your laptop without SSH. Pro tip: If you want this to persist across terminal sessions, add export KUBECONFIG=~/.kube/k3s-config to your ~/.bashrc or ~/.zshrc file. Or, if you want to merge it with your default kubeconfig, you can copy the content into ~/.kube/config and use context switching. This makes the whole experience way more convenient. I can be coding on my laptop, deploy to the cluster with kubectl apply, and check logs—all without leaving my editor or opening another terminal to SSH. Step 4: Joining Worker Nodes Now let’s add the worker nodes to the cluster. On each worker node, run this command (replace the token with your actual token from the master): curl -sfL https://get.k3s.io | K3S_URL=https://192.168.18.49:6443 \\ K3S_TOKEN=K107f8a9b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0z::server:a1b2c3d4e5f6g7h8i9j0 sh - What this does: K3S_URL tells the worker where the master node is K3S_TOKEN authenticates the worker with the master The script installs k3s in agent mode (worker mode) Installation takes a couple of minutes per worker. Verify Worker Nodes On each worker, check that the k3s agent is running: sudo systemctl status k3s-agent You should see “active (running)”. Now go back to the master node and check all nodes: kubectl get nodes Output: NAME STATUS ROLES AGE VERSION k3s-master Ready control-plane,master 5m v1.27.7+k3s1 k3s-worker-1 Ready &lt;none&gt; 2m v1.27.7+k3s1 k3s-worker-2 Ready &lt;none&gt; 1m v1.27.7+k3s1 All three nodes showing “Ready”! This is the moment where all the preparation pays off. You now have a working Kubernetes cluster running on real hardware. Step 5: Testing the Cluster Let’s deploy something to make sure everything actually works. I’ll deploy a simple nginx application with 3 replicas, which should spread across all the worker nodes. Create a file called nginx-test.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort selector: app: nginx ports: - port: 80 targetPort: 80 nodePort: 30080 Deploy it: kubectl apply -f nginx-test.yaml Output: deployment.apps/nginx-test created service/nginx-service created Watch the pods come up: kubectl get pods -o wide -w Output: NAME READY STATUS RESTARTS AGE NODE nginx-test-7d4c9d8c9b-4xk2p 1/1 Running 0 10s k3s-worker-1 nginx-test-7d4c9d8c9b-7m8qz 1/1 Running 0 10s k3s-worker-2 nginx-test-7d4c9d8c9b-9n2kx 1/1 Running 0 10s k3s-worker-1 Notice how the pods are distributed across the worker nodes automatically! Kubernetes is doing its job of spreading the workload. Access the Application Since we used NodePort, we can access the application through any node’s IP on port 30080: curl http://192.168.18.49:30080 Or from your laptop’s browser (if you’re on the same network): http://192.168.18.49:30080 You should see the nginx welcome page. Try accessing it through the other nodes too: curl http://192.168.18.51:30080 curl http://192.168.18.52:30080 All three IPs work! NodePort makes the service available on all nodes, regardless of where the pods are actually running. Challenges I Ran Into Let me share the issues I encountered so you can avoid them: WiFi and Ethernet Conflicts Initially, I had both WiFi and ethernet enabled on my Pis. This caused weird routing issues where the cluster would work sometimes and fail other times. The solution was to disable WiFi completely in netplan: network: version: 2 renderer: networkd # Wifi disabled - Only eth0 cgroup Parameters on Wrong Line My first attempt at adding cgroup parameters, I put them on a new line in cmdline.txt. This doesn’t work—boot parameters must be on a single line. The fix was to add them to the end of the existing line. Forgetting iptables on One Node I accidentally skipped the iptables-legacy configuration on one worker node. Pods on that node couldn’t communicate with pods on other nodes. The symptom was that some HTTP requests would work and others would timeout, depending on which pod handled the request. Once I set up iptables-legacy on all nodes and rebooted, everything worked. What I Practiced Now that I have a real cluster, I’ve been practicing all the concepts from the previous posts: From the k3d tutorials: ✅ Multi-replica deployments that actually run on different physical machines ✅ ConfigMaps and Secrets for configuration management ✅ Namespaces for environment separation ✅ Services with NodePort for external access New things with real hardware: ✅ Real high availability: I powered off one worker node and watched Kubernetes automatically reschedule the pods to the remaining nodes. This is the kind of behavior you can’t properly test on k3d. ✅ Resource distribution: Seeing how Kubernetes spreads pods across nodes based on available resources The experience of seeing pods automatically move from a failed node to healthy ones was really cool. This is what Kubernetes is designed for. Things I Wish I’d Known Before Starting Looking back, here’s what would have saved me time: Get the networking sorted out first. Seriously. Make sure those static IPs are stable before you even think about installing k3s. I can’t stress this enough—if your master node’s IP changes later, you’re basically starting over. Skip WiFi, use ethernet. I tried having both enabled at first and it caused all sorts of weird routing problems. Just use ethernet and call it a day. Write everything down. IP addresses, hostnames, that join token—put it all in a text file. Future you will thank present you when something breaks at 11 PM and you can’t remember which node is which. Test as you go. Don’t configure all three Pis and then try to figure out what went wrong. Do one, make sure it works, then move to the next. Pis are slow, that’s okay. If a pod takes 45 seconds to start, that’s just how it is with Raspberry Pi. Don’t panic thinking something’s broken. What You Learned ✅ How to set up static networking on Raspberry Pi ✅ How to install and configure k3s on multiple nodes ✅ How to create a multi-node Kubernetes cluster on real hardware ✅ The importance of proper preparation (iptables, cgroups, networking) ✅ How to troubleshoot common cluster setup issues ✅ The difference between local development (k3d) and real clusters Conclusion Setting up k3s on Raspberry Pi was one of the most rewarding things I’ve done in my Kubernetes learning journey. Yeah, it took a few hours and I definitely had moments where I questioned my life choices (looking at you, networking issues), but seeing all three nodes report “Ready” made it all worth it. The best part? Now I have a production-like environment sitting on my desk where I can break things without worrying about cloud bills. Everything I learned with k3d applies here, except now when I mess up, it’s on real hardware with actual network cables and blinking LEDs. There’s something satisfying about that. If you’ve been learning Kubernetes with k3d or minikube, I’d say take the jump to real hardware when you feel ready. The lessons you get from dealing with actual networking problems and watching a node physically fail (or accidentally unplugging one) are different from simulations. Plus, having your own cluster just feels cool. What’s Next? In future posts, I’ll be documenting what I build on this cluster: Setting up Traefik Ingress with custom domains Deploying a full Rails application with PostgreSQL Configuring persistent storage with NFS Implementing monitoring and logging CI/CD pipelines targeting the cluster Stay tuned! Series Navigation Part 1: Deploy Your First App Part 2: ConfigMaps and Secrets Part 3: Understanding Namespaces Part 4: Understanding Port Mapping in k3d Part 5: Setting Up k3s on Raspberry Pi ← You just finished this! Part 6: Persistent Storage (Coming soon) " }, { "title": "Kubernetes Learning Path: Understanding Port Mapping in k3d", "url": "/posts/kubernetes-learning-path-port-mapping/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3d, port-mapping, nodeport, loadbalancer", "date": "2025-10-23 10:00:00 +0500", "content": "Kubernetes Learning Path: Understanding Port Mapping in k3d In the previous posts, we’ve been using kubectl port-forward to access our applications. That works fine for testing, but it’s manual and you can only forward one service at a time. What if you want to run multiple services and access them all through different ports on your localhost? When I first started with k3d, I thought I could just create a cluster, deploy some apps, and hit localhost:8080 or localhost:3000 to see them. Nope. Services running inside the cluster aren’t automatically accessible from your host machine. You need to set up port mapping when you create the cluster. Here’s where it gets tricky—I spent way too long trying to map multiple services using LoadBalancer, only to get 404 errors. It took me a while to figure out that LoadBalancer in k3d requires Ingress configuration to actually route traffic. Once I discovered NodePort as a simpler alternative for local development, everything clicked. In this post, I’ll show you what I learned about port mapping in k3d, including the LoadBalancer mistake I made and the NodePort solution that actually worked. The Problem: Services Are Inside the Cluster Your k3d cluster runs inside Docker containers. When you create a service, it gets an IP address inside the Docker network, not on your host machine. So when you try to access localhost:8080, nothing happens—there’s no service listening there. Think of your cluster like a virtual network inside your computer. To access services in that network, you need a way to route traffic from your localhost into the cluster—that’s what port mapping does. Understanding Port Mapping Port mapping tells k3d: “when traffic hits localhost:8080 on my machine, forward it to a specific port inside the cluster.” You set this up when you create the cluster. The basic syntax looks like this: --port \"8080:30080@server:0\" This breaks down to: 8080 - Port on your localhost 30080 - Port inside the cluster @server:0 - Target the first server node But here’s where it gets confusing—what port do you map to? LoadBalancer port 80? A NodePort? I made the mistake of using LoadBalancer, and that didn’t work the way I expected. My First Attempt: Using LoadBalancer I thought I could create a cluster with multiple LoadBalancer port mappings, like this: k3d cluster create learning \\ --port \"8080:80@loadbalancer\" \\ --port \"3000:80@loadbalancer\" Output: INFO[0000] Prep: Network INFO[0000] Created network 'k3d-learning' INFO[0000] Created image volume k3d-learning-images INFO[0001] Creating node 'k3d-learning-server-0' INFO[0002] Creating LoadBalancer 'k3d-learning-serverlb' INFO[0003] Cluster 'learning' created successfully! Then I deployed two different nginx applications with LoadBalancer services, making sure to give them custom content so I could tell them apart. Let me show you exactly what I did so you can see where it goes wrong. First, create ConfigMaps with different content for each app: kubectl create configmap app1-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;APP 1&lt;/h1&gt;&lt;p&gt;This should only appear on port 8080&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' kubectl create configmap app2-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;APP 2&lt;/h1&gt;&lt;p&gt;This should only appear on port 3000&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' Output: configmap/app1-html created configmap/app2-html created Create the first deployment and service. Create a file called app1-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: app1 spec: replicas: 1 selector: matchLabels: app: app1 template: metadata: labels: app: app1 spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: html # References the volume defined below mountPath: /usr/share/nginx/html # Where nginx looks for HTML files volumes: - name: html # Volume name referenced above configMap: name: app1-html # References the ConfigMap we created --- apiVersion: v1 kind: Service metadata: name: app1-service spec: type: LoadBalancer # Using LoadBalancer type selector: app: app1 # Routes traffic to pods with label app=app1 ports: - port: 80 # Port the service listens on inside the cluster targetPort: 80 # Port on the pod containers Apply it: kubectl apply -f app1-deployment.yaml Output: deployment.apps/app1 created service/app1-service created How ConfigMap keys become files: When you mount a ConfigMap as a volume, Kubernetes automatically converts each key-value pair into a file. The ConfigMap key (index.html) becomes the filename, and the value (the HTML content) becomes the file contents. So when we mount this ConfigMap at /usr/share/nginx/html, Kubernetes will create /usr/share/nginx/html/index.html inside the container with our HTML content. This is why nginx can find and serve the file—it’s automatically created in the directory where nginx looks for HTML files. Create the second deployment and service. Create app2-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: app2 spec: replicas: 1 selector: matchLabels: app: app2 template: metadata: labels: app: app2 spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: html # References the volume defined below mountPath: /usr/share/nginx/html # Where nginx looks for HTML files volumes: - name: html # Volume name referenced above configMap: name: app2-html # References app2's ConfigMap --- apiVersion: v1 kind: Service metadata: name: app2-service spec: type: LoadBalancer # Using LoadBalancer type selector: app: app2 # Routes traffic to pods with label app=app2 ports: - port: 80 # Port the service listens on inside the cluster targetPort: 80 # Port on the pod containers Apply it: kubectl apply -f app2-deployment.yaml Output: deployment.apps/app2 created service/app2-service created Check your services: kubectl get services Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app1-service LoadBalancer 10.43.123.45 &lt;pending&gt; 80:31234/TCP 1m app2-service LoadBalancer 10.43.234.56 &lt;pending&gt; 80:31567/TCP 30s kubernetes ClusterIP 10.43.0.1 &lt;none&gt; 443/TCP 5m Both services are created! I opened my browser and tried: http://localhost:8080 - Shows “404 page not found” http://localhost:3000 - Shows… “404 page not found” Wait, what? I’m getting 404 errors from both ports. The services are running (I can see them in kubectl get services), but I can’t access them. What’s going on? What I Figured Out About LoadBalancer Mapping After some testing and digging, here’s what I learned: k3d uses Traefik as its built-in load balancer. When you map ports to @loadbalancer, those ports forward traffic to Traefik. But here’s the catch—Traefik needs Ingress resources to know how to route traffic to your services. Without Ingress configuration, Traefik has no routing rules and just returns 404. “But wait, didn’t port-forward work in the previous tutorials?” Yes! And that confused me at first too. Here’s the key difference: kubectl port-forward creates a direct tunnel from your machine to the pod through the Kubernetes API server. It completely bypasses all cluster networking, load balancers, and ingress controllers. It’s purely a development tool that works immediately with zero configuration. LoadBalancer with port mapping routes traffic through k3d’s Traefik load balancer, which expects proper Ingress resources with routing rules (hostnames, paths, etc.) to know which service should handle the traffic. So to make LoadBalancer work in k3d, you’d need to: Create Ingress resources for each service Configure host-based routing (like app1.localhost and app2.localhost) Set up DNS or host file entries Possibly configure TLS certificates For local development, that seemed like way too much work just to access my apps. I needed something simpler, and that’s when I discovered NodePort. (We’ll explore Ingress and Traefik in a future post—they’re powerful for production setups, but overkill for basic local development.) What Worked for Me: Using NodePort What ended up working was using NodePort instead of LoadBalancer, and mapping my localhost ports directly to specific NodePorts. This way, each localhost port goes straight to a specific service without going through a shared load balancer. First, let’s clean up the old cluster: k3d cluster delete learning Output: INFO[0000] Deleting cluster 'learning' INFO[0001] Cluster 'learning' deleted successfully! Now create a new cluster with NodePort mappings: k3d cluster create learning \\ --port \"8080:30080@server:0\" \\ --port \"3000:30081@server:0\" Output: INFO[0000] Prep: Network INFO[0000] Created network 'k3d-learning' INFO[0000] Created image volume k3d-learning-images INFO[0001] Creating node 'k3d-learning-server-0' INFO[0002] Creating LoadBalancer 'k3d-learning-serverlb' INFO[0003] Cluster 'learning' created successfully! The port mapping syntax means: Traffic to localhost:8080 → forwards to port 30080 inside the cluster Traffic to localhost:3000 → forwards to port 30081 inside the cluster @server:0 targets the first server node (the control plane) NodePort services listen on ports between 30000-32767 by default, so we’re using 30080 and 30081. Deploy Apps with NodePort Services Now let’s deploy two apps with NodePort services that use specific ports. We’ll also customize the content so we can tell them apart. Step 1: Create App 1 with Custom Content First, create a ConfigMap with custom HTML for app1: kubectl create configmap app1-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 1 on localhost:8080&lt;/h1&gt;&lt;p&gt;NodePort: 30080&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' Output: configmap/app1-html created Create a file called app1-nodeport.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: app1 spec: replicas: 1 selector: matchLabels: app: app1 template: metadata: labels: app: app1 spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: html # References the volume defined below mountPath: /usr/share/nginx/html # Where nginx looks for HTML files volumes: - name: html # Volume name referenced above configMap: name: app1-html # References the ConfigMap we created --- apiVersion: v1 kind: Service metadata: name: app1-service spec: type: NodePort # Using NodePort instead of LoadBalancer selector: app: app1 # Routes traffic to pods with label app=app1 ports: - port: 80 # Port the service listens on inside the cluster targetPort: 80 # Port on the pod containers nodePort: 30080 # Specific NodePort (must match cluster port mapping) The key parts: The ConfigMap contains our custom HTML The volume makes the ConfigMap available to the pod The volumeMount puts that HTML where nginx expects to find it (/usr/share/nginx/html) The NodePort is set to 30080, which matches our cluster port mapping Apply it: kubectl apply -f app1-nodeport.yaml Output: deployment.apps/app1 created service/app1-service created Step 2: Create App 2 with Different Content Create a ConfigMap for app2: kubectl create configmap app2-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 2 on localhost:3000&lt;/h1&gt;&lt;p&gt;NodePort: 30081&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' Output: configmap/app2-html created Create app2-nodeport.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: app2 spec: replicas: 1 selector: matchLabels: app: app2 template: metadata: labels: app: app2 spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: html # References the volume defined below mountPath: /usr/share/nginx/html # Where nginx looks for HTML files volumes: - name: html # Volume name referenced above configMap: name: app2-html # References app2's ConfigMap --- apiVersion: v1 kind: Service metadata: name: app2-service spec: type: NodePort # Using NodePort instead of LoadBalancer selector: app: app2 # Routes traffic to pods with label app=app2 ports: - port: 80 # Port the service listens on inside the cluster targetPort: 80 # Port on the pod containers nodePort: 30081 # Different NodePort for app2 Apply it: kubectl apply -f app2-nodeport.yaml Output: deployment.apps/app2 created service/app2-service created Step 3: Verify the Deployments Check that everything is running: kubectl get all Output: NAME READY STATUS RESTARTS AGE pod/app1-7d8c9f5b4-x9k2p 1/1 Running 0 1m pod/app2-6n4m8r3c2-y7p5q 1/1 Running 0 45s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/app1-service NodePort 10.43.123.45 &lt;none&gt; 80:30080/TCP 1m service/app2-service NodePort 10.43.234.56 &lt;none&gt; 80:30081/TCP 45s service/kubernetes ClusterIP 10.43.0.1 &lt;none&gt; 443/TCP 5m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/app1 1/1 1 1 1m deployment.apps/app2 1/1 1 1 45s Notice the PORT(S) column shows 80:30080/TCP and 80:30081/TCP—these are the specific NodePorts we configured. Step 4: Access Your Applications Now open your browser and visit: http://localhost:8080 You should see: This is App 1 on localhost:8080 NodePort: 30080 http://localhost:3000 You should see: This is App 2 on localhost:3000 NodePort: 30081 Perfect! Each port shows different content. The traffic flow is: Browser hits localhost:8080 k3d forwards to NodePort 30080 on the cluster node NodePort 30080 routes to app1-service Service forwards to an app1 pod Pod serves the custom HTML from the ConfigMap And the same happens for localhost:3000 → NodePort 30081 → app2-service → app2 pod. Why I Prefer This Approach For my local k3d setup, I found NodePort to be more straightforward: Each localhost port maps to a specific NodePort Each NodePort routes to a specific service No shared load balancer to cause confusion Simple and predictable for local development The trade-off is that you need to plan your port mappings when creating the cluster. If you want to add a third app later, you’d need to recreate the cluster with an additional port mapping. But for local development, I found that to be a reasonable compromise. Quick Experiments Add a Third Application Let’s add one more app to see how easy it is once you understand the pattern. First, recreate the cluster with an additional port mapping: k3d cluster delete learning k3d cluster create learning \\ --port \"8080:30080@server:0\" \\ --port \"3000:30081@server:0\" \\ --port \"8081:30082@server:0\" Output: INFO[0000] Deleting cluster 'learning' INFO[0001] Cluster 'learning' deleted successfully! INFO[0000] Prep: Network INFO[0000] Created network 'k3d-learning' INFO[0003] Cluster 'learning' created successfully! Create a ConfigMap for app3: kubectl create configmap app3-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 3 on localhost:8081&lt;/h1&gt;&lt;p&gt;NodePort: 30082&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' Output: configmap/app3-html created Create app3-nodeport.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: app3 spec: replicas: 1 selector: matchLabels: app: app3 template: metadata: labels: app: app3 spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: html mountPath: /usr/share/nginx/html volumes: - name: html configMap: name: app3-html --- apiVersion: v1 kind: Service metadata: name: app3-service spec: type: NodePort selector: app: app3 ports: - port: 80 targetPort: 80 nodePort: 30082 Apply it: kubectl apply -f app3-nodeport.yaml Output: deployment.apps/app3 created service/app3-service created Redeploy app1 and app2 (since we recreated the cluster): kubectl create configmap app1-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 1 on localhost:8080&lt;/h1&gt;&lt;p&gt;NodePort: 30080&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' kubectl create configmap app2-html --from-literal=index.html='&lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 2 on localhost:3000&lt;/h1&gt;&lt;p&gt;NodePort: 30081&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' kubectl apply -f app1-nodeport.yaml kubectl apply -f app2-nodeport.yaml Output: configmap/app1-html created configmap/app2-html created deployment.apps/app1 created service/app1-service created deployment.apps/app2 created service/app2-service created Now you have three apps accessible at: http://localhost:8080 (app1) http://localhost:3000 (app2) http://localhost:8081 (app3) Check Service Details You can see all the NodePort mappings: kubectl get services Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app1-service NodePort 10.43.123.45 &lt;none&gt; 80:30080/TCP 2m app2-service NodePort 10.43.234.56 &lt;none&gt; 80:30081/TCP 2m app3-service NodePort 10.43.111.22 &lt;none&gt; 80:30082/TCP 1m kubernetes ClusterIP 10.43.0.1 &lt;none&gt; 443/TCP 5m You can see the NodePort for each service in the PORT(S) column—80:30080, 80:30081, and 80:30082. Test with curl Instead of using a browser, you can test with curl: curl http://localhost:8080 Output: &lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 1 on localhost:8080&lt;/h1&gt;&lt;p&gt;NodePort: 30080&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; curl http://localhost:3000 Output: &lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 2 on localhost:3000&lt;/h1&gt;&lt;p&gt;NodePort: 30081&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; curl http://localhost:8081 Output: &lt;html&gt;&lt;body&gt;&lt;h1&gt;This is App 3 on localhost:8081&lt;/h1&gt;&lt;p&gt;NodePort: 30082&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; Perfect! Each port serves different content. What Happens If You Skip the Port Mapping? Let’s see what happens if you try to access a NodePort that wasn’t mapped when creating the cluster. Create a service with NodePort 30083 (which we didn’t map): kubectl create deployment app4 --image=nginx:alpine kubectl expose deployment app4 --type=NodePort --port=80 --name=app4-service --target-port=80 Output: deployment.apps/app4 created service/app4-service exposed Check what NodePort it got: kubectl get service app4-service Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app4-service NodePort 10.43.98.77 &lt;none&gt; 80:31847/TCP 10s It got assigned NodePort 31847 (a random port since we didn’t specify one). Now try to access it: curl http://localhost:31847 Output: curl: (7) Failed to connect to localhost port 31847 after 0 ms: Couldn't connect to server It doesn’t work because we didn’t map that port when creating the cluster. You can only access NodePorts that you explicitly mapped during cluster creation. This is why planning your port mappings ahead of time is important with k3d. When to Use LoadBalancer vs NodePort Based on what I’ve learned so far, here’s when I’d use each: I use NodePort when: I’m running k3d locally for development I want multiple services accessible on different localhost ports I want a direct, simple mapping without extra configuration I want something that works immediately without Ingress setup I’d use LoadBalancer when: I’m ready to set up Ingress resources with proper routing rules (we’ll cover this in a future post) I’m deploying to a real cloud environment (AWS, GCP, Azure) where LoadBalancer actually provisions external IPs I want a single entry point with host-based or path-based routing I’m building a production-ready setup with proper domain names For my local k3d development with multiple services, NodePort with port mapping has been working really well. It’s direct, predictable, and doesn’t require any additional configuration beyond the cluster creation. But I’m sure there are other ways to handle this that I haven’t discovered yet. Clean Up Delete all the deployments and services: kubectl delete deployment app1 app2 app3 app4 kubectl delete service app1-service app2-service app3-service app4-service kubectl delete configmap app1-html app2-html app3-html (Note: If you’re cleaning up after the LoadBalancer example earlier, you’ll only have app1, app2, and their ConfigMaps.) Output: deployment.apps \"app1\" deleted deployment.apps \"app2\" deleted deployment.apps \"app3\" deleted deployment.apps \"app4\" deleted service \"app1-service\" deleted service \"app2-service\" deleted service \"app3-service\" deleted service \"app4-service\" deleted configmap \"app1-html\" deleted configmap \"app2-html\" deleted configmap \"app3-html\" deleted Remove the YAML files: rm -f app1-deployment.yaml app2-deployment.yaml app1-nodeport.yaml app2-nodeport.yaml app3-nodeport.yaml If you want to keep the cluster for future experiments, leave it running. Otherwise, delete it: k3d cluster delete learning Output: INFO[0000] Deleting cluster 'learning' INFO[0001] Cluster 'learning' deleted successfully! What You Learned ✅ Port mapping in k3d isn’t automatic—you set it up when creating the cluster ✅ LoadBalancer in k3d requires Ingress resources to route traffic (we’ll cover this in a future post) ✅ kubectl port-forward is different from LoadBalancer—it creates a direct tunnel bypassing all cluster networking ✅ NodePort gives you direct port-to-service mappings without needing Ingress configuration ✅ You need to plan your port mappings ahead of time with k3d ✅ ConfigMaps can serve custom HTML content from volumes ✅ Multiple services can run on different localhost ports with the right setup What I found is that for local k3d development, NodePort with explicit port mappings gave me the control and predictability I needed. I know exactly which localhost port goes to which service, with no surprises and no extra configuration. Your mileage may vary depending on your setup and requirements. What’s Next? In Part 5 of this series, we’ll explore Persistent Storage in Kubernetes. You’ll learn how to: Use volumes to persist data beyond pod lifecycles Work with Persistent Volumes and Persistent Volume Claims Deploy stateful applications that need to keep data Stay tuned! Series Navigation Part 1: Deploy Your First App Part 2: ConfigMaps and Secrets Part 3: Understanding Namespaces Part 4: Understanding Port Mapping in k3d ← You just finished this! Part 5: Persistent Storage (Coming soon) Found a mistake or have questions? Feel free to open an issue here. " }, { "title": "Kubernetes Learning Path: Understanding Namespaces", "url": "/posts/kubernetes-learning-path-namespaces/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3d, namespaces, configmap", "date": "2025-10-20 10:00:00 +0500", "content": "Kubernetes Learning Path: Understanding Namespaces In the previous posts, we’ve deployed apps and managed configuration. But what happens when you want to run multiple environments on the same cluster? Maybe you’re running dev, staging, and prod all together, or different teams need to share the cluster without accidentally deleting each other’s stuff. That’s where namespaces come in. They’re basically Kubernetes’s way of keeping things organized. What Are Namespaces? Think of namespaces like separate folders on your computer. You could put all your files in one big folder, but that gets messy fast. Instead, you organize them into different folders—work stuff here, personal stuff there, projects over there. Namespaces do the same thing for your Kubernetes resources. They give you: Isolation - Resources in one namespace don’t interfere with resources in another Organization - Group related resources together (dev, staging, prod) Access Control - Different teams can have permissions to different namespaces Resource Limits - You can set quotas per namespace But here’s the thing—namespaces aren’t completely isolated. Pods in different namespaces can still talk to each other (unless you set up network policies). Think of namespaces more like organizational boundaries than security walls. See Your Current Namespaces Kubernetes creates a few namespaces by default: kubectl get namespaces Output: NAME STATUS AGE default Active 2d kube-node-lease Active 2d kube-public Active 2d kube-system Active 2d Here’s what they’re for: default - Where your resources go if you don’t specify a namespace kube-system - System components like DNS and dashboard live here kube-public - Public resources readable by everyone (rarely used) kube-node-lease - Heartbeat information from nodes (you won’t need to touch this) When you’ve been running kubectl get pods in the previous tutorials, you’ve actually been looking at the default namespace this whole time. You just didn’t know it. Creating Namespaces Let’s create two namespaces for development and production environments: kubectl create namespace dev kubectl create namespace prod Output: namespace/dev created namespace/prod created Check your namespaces again: kubectl get namespaces Output: NAME STATUS AGE default Active 2d dev Active 5s kube-node-lease Active 2d kube-public Active 2d kube-system Active 2d prod Active 5s Your new namespaces are ready to use. Real Example: Deploy App to Multiple Namespaces Let’s deploy a simple app to both dev and prod namespaces. We’ll use different ConfigMaps for each environment to show how namespaces keep things separate. Step 1: Create ConfigMaps for Each Environment First, create a ConfigMap for dev with development-specific settings: kubectl create configmap app-config \\ --from-literal=ENVIRONMENT=development \\ --from-literal=DEBUG_MODE=true \\ --from-literal=API_URL=http://dev-api.example.com \\ --namespace=dev Output: configmap/app-config created Now create a different ConfigMap for prod with production settings: kubectl create configmap app-config \\ --from-literal=ENVIRONMENT=production \\ --from-literal=DEBUG_MODE=false \\ --from-literal=API_URL=http://api.example.com \\ --namespace=prod Output: configmap/app-config created Notice we used the same name app-config in both namespaces. You might think this would cause a conflict, but it doesn’t—resources with the same name can exist in different namespaces without any issues. They’re completely separate. View the dev ConfigMap: kubectl get configmap app-config -n dev -o yaml Output: apiVersion: v1 data: API_URL: http://dev-api.example.com DEBUG_MODE: \"true\" ENVIRONMENT: development kind: ConfigMap metadata: name: app-config namespace: dev The -n flag is shorthand for --namespace. You’ll use it a lot. Step 2: Create the Application Deployment Create a file called app-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: webapp spec: replicas: 2 selector: matchLabels: app: webapp template: metadata: labels: app: webapp spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 env: # Environment variables pulled from ConfigMap # Each env variable references a key from the ConfigMap in the SAME namespace - name: ENVIRONMENT # Name of the environment variable inside the container valueFrom: configMapKeyRef: name: app-config # ConfigMap name (must exist in the same namespace) key: ENVIRONMENT # Key from the ConfigMap to get the value from - name: DEBUG_MODE # Another environment variable in the container valueFrom: configMapKeyRef: name: app-config # Same ConfigMap as above key: DEBUG_MODE # Different key from the same ConfigMap - name: API_URL # Third environment variable valueFrom: configMapKeyRef: name: app-config # Still the same ConfigMap key: API_URL # Another key from the ConfigMap This is a simple nginx deployment that pulls configuration from a ConfigMap. The cool part? We can use this exact same YAML file in both namespaces, and each deployment will automatically grab its own namespace-specific ConfigMap. Same file, different configs. Deploy to dev: kubectl apply -f app-deployment.yaml --namespace=dev Output: deployment.apps/webapp created Deploy to prod (using the same file): kubectl apply -f app-deployment.yaml --namespace=prod Output: deployment.apps/webapp created Step 3: Verify the Deployments Check pods in the dev namespace: kubectl get pods -n dev Output: NAME READY STATUS RESTARTS AGE webapp-7d8c9f5b4-2x9k7 1/1 Running 0 15s webapp-7d8c9f5b4-5m2p9 1/1 Running 0 15s Check pods in the prod namespace: kubectl get pods -n prod Output: NAME READY STATUS RESTARTS AGE webapp-7d8c9f5b4-6n3q8 1/1 Running 0 10s webapp-7d8c9f5b4-8p4r2 1/1 Running 0 10s Same deployment name, same pod names—but they’re in different namespaces, so there’s no conflict. Step 4: Verify Different Configurations Let’s check that each deployment is actually using its own ConfigMap. Check the dev environment variables: kubectl exec -n dev deployment/webapp -- sh -c 'echo \"ENV: $ENVIRONMENT\" &amp;&amp; echo \"DEBUG: $DEBUG_MODE\" &amp;&amp; echo \"API: $API_URL\"' Output: ENV: development DEBUG: true API: http://dev-api.example.com Now check prod: kubectl exec -n prod deployment/webapp -- sh -c 'echo \"ENV: $ENVIRONMENT\" &amp;&amp; echo \"DEBUG: $DEBUG_MODE\" &amp;&amp; echo \"API: $API_URL\"' Output: ENV: production DEBUG: false API: http://api.example.com There we go! Each deployment is using its own ConfigMap values. The dev environment has debug mode enabled and points to a dev API, while prod has debug mode disabled and points to the production API. Same deployment file, totally different behavior. Working with Namespaces View Resources in a Specific Namespace # Get all pods in dev namespace kubectl get pods -n dev # Get all resources in prod namespace kubectl get all -n prod Output: NAME READY STATUS RESTARTS AGE pod/webapp-7d8c9f5b4-6n3q8 1/1 Running 0 5m pod/webapp-7d8c9f5b4-8p4r2 1/1 Running 0 5m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/webapp 2/2 2 2 5m View Resources Across All Namespaces kubectl get pods --all-namespaces Or use the shorthand: kubectl get pods -A Output: NAMESPACE NAME READY STATUS RESTARTS AGE dev webapp-7d8c9f5b4-2x9k7 1/1 Running 0 10m dev webapp-7d8c9f5b4-5m2p9 1/1 Running 0 10m prod webapp-7d8c9f5b4-6n3q8 1/1 Running 0 8m prod webapp-7d8c9f5b4-8p4r2 1/1 Running 0 8m kube-system coredns-59b4f5bbd5-8xk2p 1/1 Running 0 2d Set a Default Namespace Typing -n dev or -n prod every single time gets annoying real quick. You can set a default namespace for your current context so you don’t have to: kubectl config set-context --current --namespace=dev Output: Context \"k3d-learning\" modified. Now all your commands will default to the dev namespace: kubectl get pods Output (without needing -n dev): NAME READY STATUS RESTARTS AGE webapp-7d8c9f5b4-2x9k7 1/1 Running 0 15m webapp-7d8c9f5b4-5m2p9 1/1 Running 0 15m Switch back to default when you’re done: kubectl config set-context --current --namespace=default Describe a Namespace kubectl describe namespace dev Output: Name: dev Labels: kubernetes.io/metadata.name=dev Annotations: &lt;none&gt; Status: Active No resource quota. No LimitRange resource. This shows you any resource quotas or limits applied to the namespace (none in our case). Namespace Communication Here’s something that caught me by surprise—pods in different namespaces can still talk to each other by default. Namespaces don’t block network traffic, they just change how DNS works. Let’s say you have a service called api-service in the prod namespace. Pods can reach it using: api-service - Only works from within the same namespace api-service.prod - Works from any namespace api-service.prod.svc.cluster.local - Full DNS name (also works from anywhere) So namespaces aren’t security walls—they’re more like organizational folders. If you need actual network isolation, you’d have to set up Network Policies, but that’s beyond the scope of this post. Practical Tips Specify Namespace in YAML Files Instead of using -n flags all the time, you can specify the namespace in your YAML files: apiVersion: v1 kind: ConfigMap metadata: name: app-config namespace: dev # &lt;-- Specify namespace here data: ENVIRONMENT: development This is clearer and less error-prone, especially when managing multiple resources. Use Namespaces for Environments A common pattern is to use namespaces for different environments on the same cluster: dev namespace for development staging namespace for staging prod namespace for production But remember—if you have the resources, separate clusters for production are often better. Namespaces provide logical separation, not physical isolation. Don’t Overdo It I’ve seen people create a namespace for every little thing, and honestly, it just makes life harder. Start simple: One or two namespaces for different environments Maybe separate namespaces for completely different applications Don’t create namespaces just because you can If you find yourself constantly switching between 10+ namespaces, you’ve probably gone too far. Keep it simple. Clean Up Delete the deployments from both namespaces: kubectl delete deployment webapp -n dev kubectl delete deployment webapp -n prod Output: deployment.apps \"webapp\" deleted deployment.apps \"webapp\" deleted Delete the ConfigMaps: kubectl delete configmap app-config -n dev kubectl delete configmap app-config -n prod Output: configmap \"app-config\" deleted configmap \"app-config\" deleted Delete the namespaces themselves (this also deletes everything inside them): kubectl delete namespace dev kubectl delete namespace prod Output: namespace \"dev\" deleted namespace \"prod\" deleted When you delete a namespace, Kubernetes automatically deletes everything inside it—all pods, deployments, services, everything. Super handy for cleanup, but also scary if you accidentally delete the wrong namespace. Double-check before you hit enter on that delete command. What You Learned ✅ What namespaces are and why they’re useful ✅ How to create and manage namespaces ✅ How to deploy the same application to different namespaces ✅ How to use namespace-specific ConfigMaps ✅ How to work with resources across namespaces That’s pretty much it for namespaces. They’re not complicated—just think of them as folders for organizing your cluster. Once you start using them, you’ll wonder how you managed without them. What’s Next? In Part 4 of this series, we’ll explore Persistent Storage in Kubernetes. You’ll learn how to: Use volumes to persist data beyond pod lifecycles Work with Persistent Volumes and Persistent Volume Claims Deploy stateful applications like databases Stay tuned! Series Navigation Part 1: Deploy Your First App Part 2: ConfigMaps and Secrets Part 3: Understanding Namespaces ← You just finished this! Part 4: Persistent Storage (Coming soon) Part 5: Ingress and Load Balancing (Coming soon) Found a mistake or have questions? Feel free to open an issue here. " }, { "title": "Kubernetes Learning Path: ConfigMaps and Secrets", "url": "/posts/kubernetes-learning-path-configmaps-and-secrets/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3d, configmap, secrets", "date": "2025-10-19 10:00:00 +0500", "content": "Kubernetes Learning Path: ConfigMaps and Secrets In the previous post, we deployed our first application to Kubernetes. But here’s the thing—real applications need configuration. Database URLs, API keys, feature flags, and other settings that change between environments. You definitely don’t want to hardcode these values into your container images. That would mean rebuilding your entire image just to change a database URL or update an API key. That’s where ConfigMaps and Secrets come in. What Are ConfigMaps and Secrets? Think of ConfigMaps and Secrets like configuration files that live inside Kubernetes. Instead of baking configuration into your container images, you store it separately and inject it into your pods at runtime. ConfigMaps are for non-sensitive configuration data—things like application settings, feature flags, or database names. Secrets are for sensitive data—like passwords, API keys, or certificates. They’re similar to ConfigMaps but have some extra protection (though they’re still just base64-encoded, not encrypted by default). Let’s see how to use them with some practical examples. Working with ConfigMaps Create a ConfigMap from Literal Values The simplest way to create a ConfigMap is from literal values on the command line: kubectl create configmap app-config \\ --from-literal=APP_ENV=production \\ --from-literal=LOG_LEVEL=info Output: configmap/app-config created Check what you just created: kubectl get configmap app-config -o yaml Output: apiVersion: v1 data: APP_ENV: production LOG_LEVEL: info kind: ConfigMap metadata: name: app-config namespace: default See how your key-value pairs are stored? Now let’s create one from a file. Create a ConfigMap from a File First, create a simple config file: cat &gt; app.properties &lt;&lt; EOF database.name=myapp database.pool.size=20 cache.enabled=true EOF Now create a ConfigMap from this file: kubectl create configmap app-config-file --from-file=app.properties Output: configmap/app-config-file created View it: kubectl describe configmap app-config-file Output: Name: app-config-file Namespace: default Labels: &lt;none&gt; Annotations: &lt;none&gt; Data ==== app.properties: ---- database.name=myapp database.pool.size=20 cache.enabled=true The entire file content is stored as a single key called app.properties. Use ConfigMap as Environment Variables Now let’s use our ConfigMap in a pod. Create a file called pod-with-configmap.yaml: apiVersion: v1 kind: Pod metadata: name: config-demo-pod spec: containers: - name: demo image: alpine:latest command: [\"sh\", \"-c\", \"echo APP_ENV=$APP_ENV &amp;&amp; echo LOG_LEVEL=$LOG_LEVEL &amp;&amp; sleep 3600\"] env: - name: APP_ENV # Environment variable name inside the container valueFrom: configMapKeyRef: name: app-config # References the ConfigMap we created earlier key: APP_ENV # Pulls the value of \"APP_ENV\" key from that ConfigMap - name: LOG_LEVEL valueFrom: configMapKeyRef: name: app-config # Same ConfigMap as above key: LOG_LEVEL # Pulls the value of \"LOG_LEVEL\" key Apply it: kubectl apply -f pod-with-configmap.yaml Output: pod/config-demo-pod created Check the logs to see if the environment variables were injected: kubectl logs config-demo-pod Output: APP_ENV=production LOG_LEVEL=info Great! The values from your ConfigMap are now available as environment variables inside the container. Use ConfigMap as a Volume Sometimes you want configuration files, not just environment variables. You can mount a ConfigMap as a volume. Create pod-with-config-volume.yaml: apiVersion: v1 kind: Pod metadata: name: config-volume-pod spec: containers: - name: demo image: alpine:latest command: [\"sh\", \"-c\", \"cat /config/app.properties &amp;&amp; sleep 3600\"] volumeMounts: - name: config-volume # References the volume defined below mountPath: /config # Where to mount the files inside the container volumes: - name: config-volume # Volume name (must match volumeMounts above) configMap: name: app-config-file # References the ConfigMap created from app.properties file Apply it: kubectl apply -f pod-with-config-volume.yaml Output: pod/config-volume-pod created Check the logs: kubectl logs config-volume-pod Output: database.name=myapp database.pool.size=20 cache.enabled=true The file from your ConfigMap is now mounted inside the container at /config/app.properties. This is handy when your app expects actual config files instead of environment variables. Working with Secrets Secrets work almost the same way as ConfigMaps, but they’re meant for sensitive data. Create a Secret from Literal Values kubectl create secret generic db-secret \\ --from-literal=username=admin \\ --from-literal=password=super-secret-password Output: secret/db-secret created View it: kubectl get secret db-secret -o yaml Output: apiVersion: v1 data: password: c3VwZXItc2VjcmV0LXBhc3N3b3Jk username: YWRtaW4= kind: Secret metadata: name: db-secret namespace: default type: Opaque Notice the values are base64-encoded. That’s not encryption—it’s just encoding. Anyone with access to your cluster can decode them: echo \"YWRtaW4=\" | base64 -d Output: admin So Secrets aren’t super secure by default, but they’re better than ConfigMaps because Kubernetes handles them differently—they’re not written to disk on nodes unless needed, and you can enable encryption at rest if you want. Create a Secret from a File Create a file with sensitive data: echo \"my-super-secret-api-key\" &gt; api-key.txt Create a Secret from it: kubectl create secret generic api-secret --from-file=api-key.txt Output: secret/api-secret created Use Secret as Environment Variables Create pod-with-secret.yaml: apiVersion: v1 kind: Pod metadata: name: secret-demo-pod spec: containers: - name: demo image: alpine:latest command: [\"sh\", \"-c\", \"echo DB_USER=$DB_USER &amp;&amp; echo DB_PASS=$DB_PASS &amp;&amp; sleep 3600\"] env: - name: DB_USER # Environment variable name inside the container valueFrom: secretKeyRef: name: db-secret # References the Secret we created earlier key: username # Pulls the value of \"username\" key from that Secret - name: DB_PASS valueFrom: secretKeyRef: name: db-secret # Same Secret as above key: password # Pulls the value of \"password\" key Apply it: kubectl apply -f pod-with-secret.yaml Output: pod/secret-demo-pod created Check the logs: kubectl logs secret-demo-pod Output: DB_USER=admin DB_PASS=super-secret-password The secret values are automatically decoded and injected as plain text into your environment variables. Use Secret as a Volume Just like ConfigMaps, you can mount Secrets as volumes. Create pod-with-secret-volume.yaml: apiVersion: v1 kind: Pod metadata: name: secret-volume-pod spec: containers: - name: demo image: alpine:latest command: [\"sh\", \"-c\", \"cat /secrets/api-key.txt &amp;&amp; sleep 3600\"] volumeMounts: - name: secret-volume # References the volume defined below mountPath: /secrets # Where to mount the secret files inside the container readOnly: true # Good security practice for secrets volumes: - name: secret-volume # Volume name (must match volumeMounts above) secret: secretName: api-secret # References the Secret created from api-key.txt file Apply it: kubectl apply -f pod-with-secret-volume.yaml Output: pod/secret-volume-pod created Check the logs: kubectl logs secret-volume-pod Output: my-super-secret-api-key The secret file is mounted and readable inside the container. Notice we set readOnly: true—that’s a good practice for security. Practical Example: nginx with Custom Config Let’s put it all together. We’ll deploy nginx with a custom config file from a ConfigMap, and also inject both ConfigMap and Secret values as environment variables. This shows how you can use multiple ConfigMaps and Secrets in a single deployment. First, create a simple custom nginx config file: cat &gt; default.conf &lt;&lt; EOF server { listen 80; location / { return 200 'Hello from Kubernetes!\\nThis config came from a ConfigMap.\\n'; add_header Content-Type text/plain; } } EOF Create a ConfigMap from it: kubectl create configmap nginx-config --from-file=default.conf Output: configmap/nginx-config created Now create a deployment that uses both ConfigMap and Secret. Create nginx-deployment-with-config.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-with-config spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 env: # Environment variable from ConfigMap - name: APP_ENV # Variable name in the container valueFrom: configMapKeyRef: name: app-config # References ConfigMap created with --from-literal key: APP_ENV # Gets the \"APP_ENV\" key value # Environment variable from Secret - name: API_KEY # Variable name in the container valueFrom: secretKeyRef: name: api-secret # References Secret created from api-key.txt key: api-key.txt # Gets the file content as the value volumeMounts: - name: nginx-config # Must match volume name below mountPath: /etc/nginx/conf.d/default.conf # Exact file path in container subPath: default.conf # Mounts only this file, not entire directory volumes: - name: nginx-config # Volume name referenced above configMap: name: nginx-config # References ConfigMap created from default.conf Notice the subPath: default.conf—this mounts just the single file instead of the entire directory, which prevents nginx from having issues with its default config. Apply it: kubectl apply -f nginx-deployment-with-config.yaml Output: deployment.apps/nginx-with-config created Wait for the pod to be ready: kubectl get pods -l app=nginx Output: NAME READY STATUS RESTARTS AGE nginx-with-config-7d8c9f5b4-x9k2p 1/1 Running 0 15s Test it with port-forwarding: kubectl port-forward deployment/nginx-with-config 8080:80 In another terminal: curl http://localhost:8080 Output: Hello from Kubernetes! This config came from a ConfigMap. Perfect! Now let’s verify the environment variables are available inside the container: kubectl exec deployment/nginx-with-config -- sh -c 'echo \"APP_ENV=$APP_ENV\" &amp;&amp; echo \"API_KEY=$API_KEY\"' Output: APP_ENV=production API_KEY=my-super-secret-api-key Your nginx is now running with configuration from a ConfigMap and has access to both the ConfigMap and Secret values through environment variables. Quick Experiments Update a ConfigMap You can update a ConfigMap and see how it affects running pods. Note that environment variables won’t update automatically—you need to restart the pod. But volume-mounted configs can update automatically (though it might take a minute). kubectl edit configmap app-config This opens an editor where you can change the values. Change APP_ENV from production to staging and save. For the changes to take effect in pods using environment variables, you need to restart them: kubectl rollout restart deployment/nginx-with-config Output: deployment.apps/nginx-with-config restarted View Secret Values You already saw that secrets are just base64-encoded. Let’s decode one: kubectl get secret db-secret -o jsonpath='{.data.password}' | base64 -d Output: super-secret-password This is why you still need to protect access to your Kubernetes cluster—secrets aren’t truly encrypted without additional setup. Clean Up Delete all the resources we created: kubectl delete pod config-demo-pod config-volume-pod secret-demo-pod secret-volume-pod kubectl delete deployment nginx-with-config kubectl delete configmap app-config app-config-file nginx-config kubectl delete secret db-secret api-secret rm -f app.properties api-key.txt default.conf Output: pod \"config-demo-pod\" deleted pod \"config-volume-pod\" deleted pod \"secret-demo-pod\" deleted pod \"secret-volume-pod\" deleted deployment.apps \"nginx-with-config\" deleted configmap \"app-config\" deleted configmap \"app-config-file\" deleted configmap \"nginx-config\" deleted secret \"db-secret\" deleted secret \"api-secret\" deleted What You Learned ✅ How to create ConfigMaps from literal values and files ✅ How to inject ConfigMaps as environment variables and volumes ✅ How to create and use Secrets for sensitive data ✅ How to mount Secrets in pods ✅ The difference between ConfigMaps and Secrets That’s it! You can now keep configuration separate from your container images, which makes your apps way more flexible when moving between environments. What’s Next? In Part 3 of this series, we’ll explore Understanding Namespaces in Kubernetes. You’ll learn how to: Organize resources using namespaces Deploy apps to different environments Work with namespace-specific configurations Stay tuned! Series Navigation Part 1: Deploy Your First App Part 2: ConfigMaps and Secrets ← You just finished this! Part 3: Understanding Namespaces (Coming soon) Part 4: Persistent Storage (Coming soon) Part 5: Ingress and Load Balancing (Coming soon) Found a mistake or have questions? Feel free to open an issue here. " }, { "title": "Kubernetes Learning Path: Deploy Your First App", "url": "/posts/kubernetes-learning-path-deploy-your-first-app/", "categories": "Kubernetes, Tutorial", "tags": "kubernetes, k3d, deployment, docker", "date": "2025-10-17 16:00:00 +0500", "content": "Kubernetes Learning Path: Deploy Your First App I have been wanting to learn Kubernetes for a while now, and I came across a tool called k3d. It is a great tool that makes it easy to run Kubernetes clusters locally inside Docker containers, making it perfect for learning and development without needing complex infrastructure setup. In this post, I’ll walk you through deploying your first application to Kubernetes using k3d. We’ll start from scratch and work our way up to a running nginx server. Quick Setup: Install k3d First, make sure you have Docker running on your machine. Then install k3d: # macOS (using Homebrew) brew install k3d # Or using curl curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash Verify the installation: k3d version Create Your First Cluster A cluster is a group of machines (nodes) working together to run your applications—think of it like a team of servers that can share the workload. Creating a cluster is simple: # Create a cluster named \"learning\" k3d cluster create learning Output: INFO[0000] Prep: Network INFO[0000] Created network 'k3d-learning' INFO[0000] Created image volume k3d-learning-images INFO[0000] Starting new tools node... INFO[0001] Creating node 'k3d-learning-server-0' INFO[0001] Pulling image 'ghcr.io/k3d-io/k3d-tools:5.6.0' INFO[0002] Creating LoadBalancer 'k3d-learning-serverlb' INFO[0003] Cluster 'learning' created successfully! INFO[0003] You can now use it like this: kubectl cluster-info This takes just a few seconds! Now check that your cluster is running: k3d cluster list Output: NAME SERVERS AGENTS LOADBALANCER learning 1/1 0/0 true Check the cluster nodes: kubectl get nodes Output: NAME STATUS ROLES AGE VERSION k3d-learning-server-0 Ready control-plane,master 30s v1.27.4+k3s1 You should see one node in “Ready” state. Now you’re ready to deploy. Deploy Your First App So you’ve got a local Kubernetes cluster running with k3d? Great! Let’s deploy something to it. We’ll use nginx as our first application—it’s simple, reliable, and perfect for understanding how Kubernetes deployments work. I know that in real life you’ll be deploying much more complex applications than just a static nginx server, but we all have to start somewhere. Once you understand the basics with nginx, the same patterns apply to any containerized application. We’ll work our way up to more complex deployments in future posts. What You’ll Deploy We’re going to create two things: A Deployment that runs 2 nginx containers A Service that makes nginx accessible from your browser Step 1: Create the Deployment Create a file called nginx-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-demo labels: app: nginx # &lt;--- Label for the Deployment itself (for organizing Deployments) spec: replicas: 2 selector: matchLabels: app: nginx # &lt;--- Deployment manages pods with this label template: metadata: labels: app: nginx # &lt;--- These labels will be stamped on each pod spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 What does this do? Creates 2 identical nginx pods (containers running nginx) Uses the lightweight nginx:alpine image Exposes port 80 in each container Understanding the Three Label Sections: When I first saw three different places with app: nginx, I was confused about why we needed so many labels. Here’s what each one does: metadata.labels - Labels for the Deployment resource itself (helps you organize Deployments with commands like kubectl get deployments -l app=nginx) spec.selector.matchLabels - Tells the Deployment which pods it should manage template.metadata.labels - Labels that get stamped on each pod created by this Deployment The selector and template labels must match, otherwise the Deployment won’t know which pods to manage. We’ll use these pod labels to connect the Service in Step 2. Deploy it: kubectl apply -f nginx-deployment.yaml # Watch your pods start kubectl get pods -w First, you’ll see your 2 pods being created: NAME READY STATUS RESTARTS AGE nginx-demo-7d4c9d8c9b-4xk2p 0/1 ContainerCreating 0 2s nginx-demo-7d4c9d8c9b-7m8qz 0/1 ContainerCreating 0 2s The 0/1 means 0 out of 1 containers are ready yet—they’re still starting up. Then, after a few seconds, both pods will be running: NAME READY STATUS RESTARTS AGE nginx-demo-7d4c9d8c9b-4xk2p 1/1 Running 0 5s nginx-demo-7d4c9d8c9b-7m8qz 1/1 Running 0 5s Now 1/1 shows both pods are fully ready and running. The -w flag watches for changes in real-time (similar to tail -f), so press Ctrl+C to stop watching and return to your command prompt. Step 2: Expose with a Service Your pods are running, but you can’t access them yet. So why do we need a Service anyway? Why Services are Required Think of it like a hotel receptionist. Guests don’t need to know which room each staff member is in—they just ask at the front desk, and the receptionist routes them to whoever can help. In Kubernetes: Each pod gets its own IP address that changes when it restarts We have 2 nginx pods running, and more could be added or removed The Service gives you one stable address and automatically routes traffic to healthy pods When pods restart or scale up/down, the Service updates its routing automatically Without a Service, you’d have to track pod IPs manually every time something changes. Create nginx-service.yaml: apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: LoadBalancer selector: app: nginx # &lt;--- This selector matches pods with label \"app: nginx\" ports: - port: 80 targetPort: 80 How the Service finds your Pods See the selector: app: nginx? The Service looks for all pods with the label app: nginx (the same labels we set in Step 1) and automatically sends traffic to them. When you add more pods with this label, the Service finds them. When you remove pods, the Service stops sending traffic to them. It’s all based on matching labels. Apply it: kubectl apply -f nginx-service.yaml # Check the service kubectl get service nginx-service You should see: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-service LoadBalancer 10.43.123.456 &lt;pending&gt; 80:32000/TCP 5s The service is now routing traffic to your pods. The EXTERNAL-IP shows &lt;pending&gt; in k3d—don’t worry about it, we’ll use port-forwarding to access the app. Step 3: Access Your Application Use port-forwarding to access nginx: kubectl port-forward service/nginx-service 8080:80 Open your browser and visit http://localhost:8080. You should see the nginx welcome page. Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. Your application is now running on Kubernetes and accessible from your browser. Quick Experiments Scale Your Application # Scale to 5 pods kubectl scale deployment nginx-demo --replicas=5 Output: deployment.apps/nginx-demo scaled Now watch the new pods appear: kubectl get pods -w You’ll see 3 additional pods being created: NAME READY STATUS RESTARTS AGE nginx-demo-7d4c9d8c9b-4xk2p 1/1 Running 0 5m nginx-demo-7d4c9d8c9b-7m8qz 1/1 Running 0 5m nginx-demo-7d4c9d8c9b-9n2kx 0/1 ContainerCreating 0 2s nginx-demo-7d4c9d8c9b-6p4mw 0/1 ContainerCreating 0 2s nginx-demo-7d4c9d8c9b-8r5tn 0/1 ContainerCreating 0 2s Scale back to 2: kubectl scale deployment nginx-demo --replicas=2 Kubernetes will automatically terminate the extra pods. Check the Logs # Get pod name kubectl get pods Output: NAME READY STATUS RESTARTS AGE nginx-demo-7d4c9d8c9b-4xk2p 1/1 Running 0 10m nginx-demo-7d4c9d8c9b-7m8qz 1/1 Running 0 10m Now view logs from one of the pods: kubectl logs nginx-demo-7d4c9d8c9b-4xk2p Output (nginx access logs): /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Configuration complete; ready for start up 127.0.0.1 - - [17/Oct/2025:10:30:15 +0000] \"GET / HTTP/1.1\" 200 615 \"-\" \"Mozilla/5.0\" These are the nginx startup messages and any HTTP requests. Your logs might be different. Execute Commands # Check nginx version (replace with your actual pod name) kubectl exec nginx-demo-7d4c9d8c9b-4xk2p -- nginx -v Output: nginx version: nginx/1.25.3 Open a shell inside the pod: kubectl exec -it nginx-demo-7d4c9d8c9b-4xk2p -- /bin/sh You’ll get an interactive shell prompt: / # pwd / / # ls bin dev etc home lib media mnt opt proc root run sbin srv sys tmp usr var / # exit You can run any commands inside the container. Type exit to leave the shell. Clean Up When you’re done experimenting, it’s good practice to clean up the resources. This also helps you practice the delete commands: kubectl delete -f nginx-service.yaml kubectl delete -f nginx-deployment.yaml Output: service \"nginx-service\" deleted deployment.apps \"nginx-demo\" deleted Verify everything is gone: kubectl get all Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.43.0.1 &lt;none&gt; 443/TCP 30m Your nginx resources are gone! The only thing remaining is the default kubernetes service, which is a system service that’s always present. What You Learned ✅ How to create a Kubernetes Deployment ✅ How to expose an app with a Service ✅ How to scale applications up and down ✅ How to inspect pods (check logs and execute commands) You’ve successfully deployed your first application to Kubernetes. The same pattern works for any containerized application—just swap the nginx image for your own. What’s Next? In Part 2 of this series, we’ll explore ConfigMaps and Secrets to manage configuration and sensitive data in your Kubernetes applications. You’ll learn how to: Store configuration separately from your code Manage environment variables Handle sensitive information securely Stay tuned! Series Navigation Part 1: Deploy Your First App ← You just finished this! Part 2: ConfigMaps and Secrets Part 3: Understanding Namespaces (Coming soon) Part 4: Persistent Storage (Coming soon) Part 5: Ingress and Load Balancing (Coming soon) Found a mistake or have questions? Feel free to open an issue here. " }, { "title": "Deploying Ruby on Rails on Ubuntu", "url": "/posts/deploy-ruby-on-rails-on-ubuntu/", "categories": "Ubuntu, Deployment", "tags": "ruby-on-rails", "date": "2022-07-12 13:54:50 +0500", "content": "Deploying Ruby on Rails on Ubuntu I always wanted to learn how the deployment and server provisioning is done. While setting up a homelab using Raspbery Pi 4, which uses an ARM based architecture and many deployment tools do not support that yet so I thought this is perfect oppertuity to finally learn. This is my first ever home lab setup: I use 4, Raspbery Pi 4 with 4Gig of RAM. The case I am using is from UCTRONICS. For operating system I am using Ubuntu server 22.04. While configuring the Raspberry Pi to host my Ruby on Rails sites, these are the steps I had to go through. Secure the remote server with SSH Install the copy utility Make sure ssh-copy-id is installed on Mac, it does not come in mac by default. brew install ssh-copy-id Copy command The following command will prompt for password ssh-copy-id -i ~/.ssh/id_rsa.pub user@remote-host-ip You should now be able to ssh into remote server without needing to provide your password. Disable password based login Inside the sshd_config file set the PasswordAuthentication to no sudo vim /etc/ssh/sshd_config Restart the SSH service sudo systemctl restart ssh Installing Ruby &amp; Rails dependencies Node.js Node version manager sudo apt install curl curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | bash Run the following command so nvm command is available source ~/.profile Install node version Install the LTS version nvm install --lts Install Yarn sudo apt install yarn Install Redis server sudo apt install redis-server sudo apt install redis-tools ### Make sure it is up and running sudo systemctl status redis-server Dependencies for compiiling Ruby along with Node.js and Yarn sudo apt install git-core curl zlib1g-dev build-essential libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 libxml2-dev libxslt1-dev libcurl4-openssl-dev software-properties-common libffi-dev dirmngr gnupg apt-transport-https ca-certificates Install Ruby rbenv Run each command separately git clone https://github.com/rbenv/rbenv.git ~/.rbenv echo 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' &gt;&gt; ~/.bashrc echo 'eval \"$(rbenv init -)\"' &gt;&gt; ~/.bashrc git clone https://github.com/rbenv/ruby-build.git ~/.rbenv/plugins/ruby-build echo 'export PATH=\"$HOME/.rbenv/plugins/ruby-build/bin:$PATH\"' &gt;&gt; ~/.bashrc git clone https://github.com/rbenv/rbenv-vars.git ~/.rbenv/plugins/rbenv-vars exec $SHELL Ruby ubuntu@ubuntu:~$ rbenv install 3.1.2 ubuntu@ubuntu:~$ rbenv global 3.1.2 ubuntu@ubuntu:~$ ruby -v Bundler gem install bundler Puma sudo apt -y install puma Nginx sudo apt -y install nginx Nginx site Remove defailt site sudo rm /etc/nginx/sites-enabled/default Enable your site sudo vim /etc/nginx/sites-enabled/app-name Add site configuration upstream puma_your_app_name { server unix:///home/ubuntu/freeapi/shared/tmp/sockets/puma.sock; } server { listen 80 default_server deferred; server_name freeapi.com www.freeapi.com; # Don't forget to update these, too root /home/ubuntu/your_app_directory/current/public; access_log /home/ubuntu/your_app_directory/current/log/nginx.access.log; error_log /home/ubuntu/your_app_directory/current/log/nginx.error.log info; location ^~ /assets/ { gzip_static on; expires max; add_header Cache-Control public; } try_files $uri/index.html $uri @puma_your_app_name; location @puma_your_app_name { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_redirect off; proxy_pass http://puma_your_app_name; } error_page 500 502 503 504 /500.html; client_max_body_size 10M; keepalive_timeout 10; } Restart Nginx service sudo service nginx restart If the service fails to start check the logs to see what the error is. The status command will also provide helpful information systemctl status nginx.service. At this stage the error might be related to nginx log files not being available, thats because we did not deploy the app yet. If you want you can comment the access_log and error_log line in the above confirutaion, restart the nginx service and then check if it is running successfully. Install PostgreSQL sudo apt install postgresql postgresql-contrib libpq-dev If you see a lock error then simply reboot and run the above command again. Create the PostgreSQL user and database sudo su - postgres createuser --pwprompt ubuntu createdb -O ubuntu your-app-db exit Deployment Capistrano Add the following gems to the Gemfile and run the command bundle install gem 'capistrano', require: false gem 'capistrano-rails', require: false gem 'capistrano-puma', require: false gem 'capistrano-rbenv', require: false gem 'capistrano-bundler', require: false Generate the deployment files cap install STAGES=production This will generate the following files Capfile config/deploy.rb config/deploy/production.rb and the following directory lib/capistrano/tasks Add the following in Capfile require \"capistrano/rails\" require \"capistrano/rbenv\" require \"capistrano/bundler\" require \"capistrano/rails/assets\" require \"capistrano/rails/migrations\" require \"capistrano/puma\" install_plugin Capistrano::Puma install_plugin Capistrano::Puma::Systemd set :rbenv_type, :user set :rbenv_ruby, '3.1.2' Add the following inside config/deploy.rb set :application, \"your-app-name\" set :repo_url, \"git@github.com:username/myapp.git\" set :deploy_to, \"/home/ubuntu/#{fetch :application}\" append :linked_dirs, 'log', 'tmp/pids', 'tmp/cache', 'tmp/sockets', 'vendor/bundle', '.bundle', 'public/system', 'public/uploads' Add the following inside config/deploy/production set :branch, \"main\" server \"your-ip-server\", user: \"ubuntu\", roles: %w[web app db] set :ssh_options, { keys: %w(/Users/your-user/.ssh/id_ed25519.pub), forward_agent: true, auth_methods: %w(publickey) } Make sure the branch you want to deploy is already pushed on GitHub. Setup env vars SSH into the remote server mkdir /home/ubuntu/your-app nano /home/ubuntu/your-app/.rbenv-vars Add the DB and Master key env vars # For Postgres DATABASE_URL=postgresql://user:PASSWORD@127.0.0.1/myapp RAILS_MASTER_KEY=xyz123 RACK_ENV=production RAILS_ENV=production Upload puma config cap production puma:config Upload puma service cap production puma:systemd:config puma:systemd:enable Finally deploy the app using the following command cap production deploy --trace Troubleshooting Removing the repository from Ubuntu This guide is for Ubuntu 22.04, if you end up adding the wrong repo which is not compatiable for example sudo add-apt-repository ppa:chris-lea/redis-server just remove it by using the --remove flag sudo add-apt-repository --remove ppa:chris-lea/redis-server Puma If you see the following error on deployment NameError: uninitialized constant Capistrano::Puma install_plugin Capistrano::Puma ^^^^^^ /Users/shairyar/Sites/fakeapi/Capfile:44:in `&lt;top (required)&gt;' Make sure you are using the gem capistrano3-puma and not capistrano-puma. GitHub permission issue Just make sure the new secure ssh key id_ed25519.pub is added to remote server and inside config/deploy/production key forward_agent is set to true set :ssh_options, { keys: %w(/Users/shairyar/.ssh/id_ed25519.pub), forward_agent: true, auth_methods: %w(publickey) } Yarn install issue Running the following command shold fix the problem sudo apt-get remove cmdinstall;sudo apt update;sudo apt-get install yarn Contribute Found a mistake? Or if you have any questions, please feel free to open an issue here. Found something that can be improved? Feel free to create a PR here. " } ]
